[
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_containers_and_orchestration/",
	"title": "00 - Containers and Orchestration",
	"tags": [],
	"description": "",
	"content": "  Containers Microservices Container Orchestration  Container Orchestrators   Containers Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications.\nMicroservices Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.\nContainer Orchestration Container orchestrators are tools which group systems together to form clusters where containers\u0026rsquo; deployment and management is automated at scale while meeting following requirements.\n Fault-tolerance On-demand scalability Optimal resource usage Auto-discovery to automatically discover and communicate with each other Accessibility from the outside world Seamless updates/rollbacks without any downtime.  Container Orchestrators  Amazon Elastic Container Service Azure Container Instances Azure Service Fabric Kubernetes Marathon Nomad Docker Swarm  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/introduction/",
	"title": "00 - Terraform Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction Terraform is the infrastructure as code offering from HashiCorp. It is a tool for building, changing, and managing infrastructure in a safe, repeatable way.\nUsing HCL as High Level Language, Terraform support Infrastrucre as Code by automating creation of those resources. Terraform provides support for desired resources on almost any provider (AWS, GCP, GitHub, Docker etc)\nFeature of Terraform Infrastructue as Code  Idempotent Uses High Level Language (HCL) Code Reusability using Modules  Execution Plan  Show the intent of the deploy Can help ensure everything in the development is intentional  Resource Graph  Illustrates all changes and dependencies  Use Cases for Terraform  Hybrid Cloud Multi-tier Architecture Software Defined Networking  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/01-compute/",
	"title": "01 - AWS Compute Services",
	"tags": [],
	"description": "",
	"content": " Elastic Cloud Compute (EC2)  EC2 is a Cloud Computing Service. It is responsible for providing long-running compute as service. Configures your EC2 by choosing your OS, Storage, Memory, Network Throughput EC2 comes in variety Instance types of specialization for different roles [EC2 Families]  General Purpose - Balance of Compute, Memory and Networking resources Compute Optimized - Ideal for compute based application that benefit from high performance processor Memory Optimized - Ideal for fast performance workloads processing large data sets in memory Storage Optimized - Ideal for high sequential, read and write access to very large dataset on local storage Accelerated Optimized - Hardware accelerators, or, co-processors  Instance sizes include Nano, Small, Medium, Large, X.Large, 2X.Large and Larger  Instance Profile  Instance Profiles is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. An instance profile is either created automatically when using the console UI or manually when using the CLI. EC2 instance roles are IAM roles that are \u0026ldquo;assumed\u0026rdquo; by EC2 using intermediary called an Instance Profile.  Instance Metadata  Instance metadata can be used to access information about current instance from the instance. It allows applications running within EC2 to have visibility into their environment. E.g, curl http://169.254.169.254/latest/meta-data provides infromation about intance type, current ip etc.  Placement Groups  Let you choose the logical placement of your instances to optimize for network, performance or durability Placement groups are free of cost.  Cluster Placement Group (CPG)  Places instances physically near other in a single availability zone It works with Enhanced Networking for deliverying maximum performance  Partition Placement Group (PPG)  Instances deployed into a partition placement group are separated into partitions, each occupying isolated rack in AZs It minimizes failure in partition and provides visibility on placement  Spread Placement Group (SPG)  Designed for max of seven (7) instances per AZ Each instance occupies a partition and has an isolated fault domain  User Data  When you launch an instance in Amazon EC2, you have the option of passing User Data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives  EC2 Pricing EC2 has 5 pricing models.\nOn-Demand  Pay for EC2 instances you use (Flexible) Pay per hour or second(Minimum of 60 sec)] Ideal when work-load cannot be interrupted The use of On-Demand instances frees you from the costs and complexities of planning, purchasing, and maintaining hardware Use Case - Short Term Instances, Spiky Traffic, First Time Applications (Unknown Demand)  Reserved Instances  You can save upto 75% off compared to on-demand instances Reserved instance lock in a reduced rate for one or three years Your commitment incurs costs even if instance aren’t launched Use Case - Long-running, Critical , Known / Understood, and Consistent workload systems  Spot Instances  You can save upto 90% compared to on-demand instances Allow consumption of spare AWS capacity at really reduced rate. Instances are provided to you as long as your bid price is above the spot price, and you only ever pay the spot price. If your bid is exceeded, instances are terminated with two-minute-warning Use Case - Non critical workoads, burst workloads or consistent non-critical workloads that can tolerate interruptions. Not good options for long running jobs that cannot tolerate interruptions.  Savings Plans  Flexible pricing model that offer low price on EC2 or Fargate usage, in exchange for a commitment to a consistent amoutn of usage (measured in $/hour) for a 1 or 3 year term  Dedicated Hosts  Dedicated host give you complete control over physical instance placement and dedicated hardware free from other customer interactions. Dedicated hosts are generally used when software is licensed per core/CPU and not compatibile with running within a shared cloud environment. Can be purchased On-Demand (Hourly)  Amazon Machine Image (AMI)  AMIs are used to build instances. They store snapshot of EBS volume, launch permission and block device mapping that specify the volumes to attach to the instance when it\u0026rsquo;s launched. AMIs are regional service You can create an AMI from existing EC2 instnace that are in either running or stopped state Community AMIs are AMIs managed by public (community). These AMI come from AWS users, and are not verified by AWS AWS Marketplace - AMIs verified by AWS. AWS Marketplace consists of both Free as well as paid version of AMIs Two types of AMI  Instance Store Back AMIs - Root Volume doesn\u0026rsquo;t use EBS EBS Backed AMIs - Root Volume uses EBS  AMIs are faster but compared to UserData, AMIs doesn\u0026rsquo;t support dynamic configuration  Amazon Elastic Block Storage (EBS)  Storage service that creates and manages volumes EBS Volumes are durable block level storage device that you can attach to a single EC2 istance Volumes are persistent, can be attached and removed from EC2 instances, and are replicated within a single AZ EBS Snapshots are a point-in-time backup of an EBS volume stored in S3 Snapshots are incremental - The initial snapshot is a full copy of the volume. Future snapshots only store the data changed since the last snapshot Snapshots can be used to create new volumes and are a great way to move or copy instances between AZs. When creating a snapshot of the root volume of an instance of busy volume, it’s recommended the instance is powered off, or disks are “flushed” Snapshots can be copied between regions, shared and automated using Data Lifecycle Manager (DLM) By defualt, root volumes are deleted on termination Volume encryption uses EC2 host hardware to encrypt data at rest and In-Transit between EBS and EC2 instances  EBS Volume Types  General Purpose SSD (gp2)  Default for most workloads  Provisioned IOPS SSD (io1)  Used for applications that required sustained IOPS performance Ideal for large database workloads  Throughput Optimized HHD (st1)  Low cost volume storage Used for frequently accessed, throughput-intensive workoads (Streaming, BigData) Cannot be root volume  Cold HDD (sc1)  Cheapest volume solution Infrequent accessed data Cannot be root volume   AWS Lambda  Serverless Compute service offering Lambda\u0026rsquo;s are serverless functions. You don\u0026rsquo;t need to manage or provision servers Lambda functions are stateless You choose the amount of memory you want to allocate to your functions and AWS Lambda allocates propotional CPU, Network, Bandwidth and Disk There are 7 runtime languages supported:  Ruby Python Java NodeJS C# Powershell Go  Pricing - Pay per invocation (The duration and the amount of memory used) rounded up to the nearest 100 ms. First 1M requests per month are free You can adjust the duration timeout upto 15 minutes and memory upto 300 MB By default, AWS executes your Lambda function code securely within a VPC. Alternatively you can enable your Lambda function to access resource inside your private VPC by providing additional VPC specific configuration Labda is HA and scalable by design. Lambda can scale to 1000 os concurrent functions in seconds Lambdas have Cold Starts (delayed Initial Start), if a function has not been recently been executed.  Amazon Elastic Container Service (ECS)  ECS is a managed container service It allows docker containers to be deployed and managed within AWS environments ECS can use infrastructure cluster based on EC2 or Fargate  With EC2 launch type, you own EC2 instances AWS Fargate is a managed service, so tasks are auto placed  Amazon ECS is a regional service Components  Cluster - A logical collection of ECS resources - either ECS EC2 instances or a logical representation of managed Fargate infrastructure Task Definition - Defines your application. Similar to Dockerfile but for running containers in ECS. Task definition can contain multiple containers Container Definition - Inside a task definition, a container definition defines the individual containers a task uses. It controls the CPU and memory each container has, in addition to port mappings for the container Task - A single running copy of any containers defined by a task definition. One working copy of an application Service - Allows task definitions to be scaled by adding additional tasks. Define minimum and maximum values Registry - Storage for container images. (eg. ECS Container Registry or Dockerhub). Used to download image to create containers   Auto Scaling Groups (ASG)  An ASG is a collection of EC2 instances grouped for scaling and management Metrics such as CPU utilization or network transfer can be used either to scale out or scale in using scaling policies Size of an ASG is based on a Min, Max and Desired Capacity (How many EC2 instances you want to ideally run) Auto Scaling Groups are often paired with ELB ASG uses launch configuration or launch template and allow automatic scale-out or scale-in based on configuration metrics Scaling can be Manual, Scheduled or Dynamic Scaling policy can be:  Simple Scaling Policy - Policy triggers a scaling when an alarm is breached Scaling Policy with Steps - New version of Simple Scaling policy and allows you to create steps based on alarm values Target Scaling Policy - Based on when a target value for a metrics is breached. e.g, Average CPU Utilization exceeds 90%  Health checks determine the current state of an instance in the ASG Health checks can be run against either an ELB or the EC2 instances  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/01-architecture-101/",
	"title": "01 - Architecture 101",
	"tags": [],
	"description": "",
	"content": "  Access Management Basics Shared Reponsibility Model Service Models Availability and Fault Tolerance Scalling Tiered Application Design Encryption  Access Management Basics Principal - A person or application that can make an authenticated or anonymous request to perform an action on a system.\nAuthentication - The process of authenticating a principal against an identity. This could be via username and passsword or API Keys.\nIdentity - Objects that require authentication and are authorized to access resources.\nAuthorization - The process of checking and allowing or denying access to a resoruce for an identity.\nShared Reponsibility Model  AWS is responsible for the \u0026ldquo;Security of the Cloud\u0026rdquo;, This infrastructure is composed of the hardware, software, networking, and facilities that run AWS Cloud services.\n Customer is responsible for the \u0026ldquo;Security in the Cloud\u0026rdquo;, Customers are responsible for managing their data (including encryption options), classifying their assets, and using IAM tools to apply the appropriate permissions.\n  Service Models Service models define how a service or product is delivered, How you pay and what you receive. They also define which part of the product you manage and accept the risk for, as well as which part the vendor is responsible for.\nCloud models come in three types: SaaS (Software as a Service), IaaS (Infrastructure as a Service) and PaaS (Platform as a Service). Each of the cloud models has their own set of benefits that could serve the needs of various businesses.\nAvailability and Fault Tolerance High Availability - Hardware, Software and configuration allowing a system to recover quickly in the event of a failure.\nFault Tolerance - System designed to operate though a failure with no use impact. Fault Tolenace is more expensive and complex to achieve.\nRecory Point Objective (RPO) - How much a business can tolerate to lose, expressed in time. The maximum time between a failure and the last successful backup.\nRecovery Time Objective (RTO) - The maximum amount of time a system can be down. How long a solution takes to recover.\nScalling  Vertial Scalling (Down/Up) is achieved by adding additional resources in from of CPU or memory to existing machine. By doing so, the machine is able to service additional customers or perform compute tasks quicker. Eventually, maximum machine size will constraint your ability to scale - either technically or from a cost perspective.\n Horizontal Scalling(Right/Left) is achieved by adding additional machines into a pool of resource, each of which provide the same service. Horizontal scalling suffers none of the size limitations of vertical scalling and can scale to nearly infinite levels but requires application support to scale effectively.\n  Tiered Application Design Architectually, applications consist of three tiers: Presentation Tier, which intereacts with the customer of the application, the logic tier, which delivers the application functionality; and the data tier, which controls interaction with a database.\nEncryption Encryption is the process of taking plaintext and converting it into ciphertext, and converting ciphertext into plaintext. Plaintext and cipheredtext can be text, images or other data.\nTypes of Enryption\n Symmetrical - Same key is used for both encryption and decryption Asymmetrical - Different keys - called public and private keys are used   Cost efficient or Cost Effective  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/hcl_basics/",
	"title": "01 - HCL Basics",
	"tags": [],
	"description": "",
	"content": "  Terraform CLI Commands Terraform Syntax Resources Console and Outputs Variables  Passing Variable  DataSources Terraform Workspaces NullResources and Local-exec  Terraform CLI Commands    Command Description     init Initializes a new or existing Terraform configuration   validate Validates the Terraform files   plan Generates ans shows an execution plan   apply Builds or Change Infrastructure   output Reads an output from a state file   show Inspects Terraform State or Plan   providers Print a tree of the providers using the configuration   destory Destorys Terraform-managed infrastructure    Terraform Syntax  Single line comments start with # Multi line comments are wrapped with /* and */ Values are assigned with the syntax key=value Strings are double quoted Strings can interpolate other values using the syntax ${}  Resources Resosurces are the Objects manged by Terraform such as VM or S3 Buckets. Declaring a Resource tells Terraform that it should CREATE and MANAGE the resource described. If the resource already exists it must be imported into Terraform\u0026rsquo;s State.\nSyntax\nresource \u0026lt;Resource_Type\u0026gt; \u0026lt;Resource_Name\u0026gt; { // Meta Parameters }  Example\n# Example # Download the latest Ghost image resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;ghost:latest\u0026quot; } # Start the Container resource \u0026quot;docker_container\u0026quot; \u0026quot;container_id\u0026quot; { name = \u0026quot;ghost_blog\u0026quot; image = \u0026quot;${docker_image.image_id.latest}\u0026quot; ports { internal = \u0026quot;2368\u0026quot; external = \u0026quot;80\u0026quot; } }  Console and Outputs Outputs are printed by the CLI after apply. These can reveal calculated values.\n#Output the IP Address of the Container output \u0026quot;ip_address\u0026quot; { value = \u0026quot;${docker_container.container_id.ip_address}\u0026quot; description = \u0026quot;The IP for the container.\u0026quot; } #Output the Name of the Container output \u0026quot;container_name\u0026quot; { value = \u0026quot;${docker_container.container_id.name}\u0026quot; description = \u0026quot;The name of the container.\u0026quot; }  Variables Input variables serve as parameters for a Terraform file. A variable block configures a single input variable for a Terraform module. Each block declares a single variable.\nSyntax\nvariable [NAME] { [OPTION] = \u0026quot;[VALUE]\u0026quot; }  Variables can be used using interpolation syntax \u0026quot;${var.name}\u0026quot;\nExample\nvariable \u0026quot;image_name\u0026quot; { description = \u0026quot;Image for container.\u0026quot; default = \u0026quot;ghost:latest\u0026quot; } resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;${var.image_name}\u0026quot; }  Passing Variable Variables can be specified on the command line with -var bucket_name=my-bucket or in file using terraform.tfvars\nterraform apply -var 'foo=bar' terraform apply -var 'container_name=ghost_blog' -var 'ext_port=8080'  We can set the variabl using Environment Variables\nexport TF_VAR_container_name=ghost_blog export TF_VAR_ext_port=8080  DataSources Data sources allow data to be fetched or computed for use elsewhere in Terraform configuration. Use of data sources allows a Terraform configuration to make use of information defined outside of Terraform, or defined by another separate Terraform configuration.\nEach provider may offer data sources alongside its set of resource types.\nTerraform Workspaces Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as Terraform State are stored. Terraform supports multiple workspace to isolate different terraform plans and their respective state files.\n# Creates Dev Workspace terraform workspace new dev # List Workspaces terraform workspace list # Switch to default Workspace terraform workspace select default  NullResources and Local-exec # Download the latest Ghost Image resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;${lookup(var.image_name, var.env)}\u0026quot; } # Start the Container resource \u0026quot;docker_container\u0026quot; \u0026quot;container_id\u0026quot; { name = \u0026quot;${lookup(var.container_name, var.env)}\u0026quot; image = \u0026quot;${docker_image.image_id.latest}\u0026quot; ports { internal = \u0026quot;${var.int_port}\u0026quot; external = \u0026quot;${lookup(var.ext_port, var.env)}\u0026quot; } } # Using Null Resource and Local Exec to Write Container Name and Container Address to TextFile resource \u0026quot;null_resource\u0026quot; \u0026quot;null_id\u0026quot; { provisioner \u0026quot;local-exec\u0026quot; { command = \u0026quot;echo ${docker_container.container_id.name}:${docker_container.container_id.ip_address} \u0026gt;\u0026gt; container.txt\u0026quot; } }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/01_kubernetes-architecture/",
	"title": "01 - Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "  Master Node  Master Node Components Master Node Components: API Server Master Node Components: Scheduler Master Node Componets: Controller Managers Master Node Components: etcd  Worker Node  Worker Node Components Worker Node Component: Container Runtime Worker Node Components: kubelet Worker Node Components: kube-proxy Worker Node Components: Addons  Networking Challenges  Container to Container Communication inside Pods Pod-to-Pod Communication Across Nodes Pod-to-External World Communication   At a very high level, Kubernetes has the following main components\n One ore more master nodes One or more worker nodes Distributed key-value store, such as etcd  Master Node The master node provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster\u0026rsquo;s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API).\nMaster Node Components  API Server Scheduler Controller managers etcd  Master Node Components: API Server All the administrative tasks are coordinated by the kube-apiserver, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster\u0026rsquo;s current state from the etcd, and after a call\u0026rsquo;s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster\u0026rsquo;s data store.\nThe API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules.\nMaster Node Components: Scheduler The role of the kube-scheduler is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object\u0026rsquo;s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object\u0026rsquo;s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with disk==ssd key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc.\nMaster Node Componets: Controller Managers The controller managers are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster\u0026rsquo;s desired state (provided by objects\u0026rsquo; configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state.\nMaster Node Components: etcd etcd is a distributed key-value data store used to persist a Kubernetes cluster\u0026rsquo;s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store.\nOut of all the control plane components, only the API server is able to communicate with the etcd data store.\nWorker Node A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together.\nWorker Node Components A worker node has the following components:\n Container runtime kubelet kube-proxy Addons for DNS, Dashboard, cluster-level monitoring and logging.  Worker Node Component: Container Runtime Although Kubernetes is described as \u0026ldquo;container orchestration engine\u0026rdquo;, it does not have the capability to directly handle containers. In order to run and manage a container\u0026rsquo;s lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Docker is the most widely used container runtime with Kubernetes.\nWorker Node Components: kubelet The kubelet is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod\u0026rsquo;s running containers.\nThe kubelet connects to the container runtime using Container Runtime Interface (CRI). CRI consists of protocol buffers, gRPC API and libraries.\nWorker Node Components: kube-proxy The kube-proxy is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods.\nWorker Node Components: Addons Addons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.\n DNS - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources Dashboard - a general purposed web-based user interface for cluster management Monitoring - collects cluster-level container metrics and saves them to a central data store Logging - collects cluster-level container logs and saves them to a central log store for analysis.  Networking Challenges Container to Container Communication inside Pods Making use of the underlying host operating system\u0026rsquo;s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A network namespace is shared across containers, or with the host operating system.\nWhen a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost.\nPod-to-Pod Communication Across Nodes Kubernetes uses \u0026ldquo;IP-per-Pod\u0026rdquo; model to ensure Pod-to-Pod communication, just as VM are able to communicate with each other. Containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI)\nPod-to-External World Communication Kubernetes enables external accessibility through services, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with kube-proxy, applications become accessible from outside the cluster over a virtual IP.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/01_setting_up_k8_cluster/",
	"title": "01 - Setting up Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": " Using Ubuntu Distribution (Ubuntu Xenial LTS 16.04) as Base Image for the Virtual Machine. We will be building a Kubernetes Cluster\nSetup Docker and Kubernetes Repositories curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF  Install Docker, Kubelet, KubeAdm and KubeCtl sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.17.0-00 kubeadm=1.17.0-00 kubectl=1.17.0-00 sudo apt-mark hold docker-ce kubelet kubeadm kubectl  Enable IPTable Bridge Call echo \u0026quot;net.bridge.bridge-nf-call-iptables=1\u0026quot; | sudo tee -a /etc/sysctl.conf sudo sysctl -p  Initailize Cluster on Kube Master # Initialize Kube Cluster sudo kubeadm init --pod-network-cidr=10.244.0.0/16 # Setup Local KubeConfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  Install Flannel Networking kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml  Join Master Node sudo kubeadm join 172.31.47.84:6443 --token c06upw.nhz4qqqifxmqdzaf --discovery-token-ca-cert-hash sha256:970ac46eb8847181b2ff5cf911836ef73c3ac103508739f3679e4933a1795775  You can use kubeadm token create --print-join-command to get the join command.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/02-aws-fundamentals/",
	"title": "02 - AWS Fundamentals",
	"tags": [],
	"description": "",
	"content": "  AWS Accounts  Authentication Authorization Billing  AWS Physical and Networking Layer Well-Architected Framework  Pillars of Well Architected Framework Security Reliability Performance Efficiency Operational Excellence Cost Optimization  Elasticity Introduction to S3 - (Simple Storage Service) Introduction to Cloud Formation  AWS Accounts AWS accounts are more than just a way to log in and access AWS services — they are a crucial AWS feature that AWS solutions architects can use to implement secure and high-performance systems.\nAWS Account is used to perform following functionalities:\n Authentication Authorization Billing  Authentication AWS account are isolated. they are created initially with a single root user. This user, via it\u0026rsquo;s username / password / API Keys, is the only identity that can use the account. If account credentials are leaked, the impact is limited to that account.\nAuthorization Authorization is controlled on a per-account basis. The root user starts with full control of the account and its resources. Additional identities can be created or external identities can be granted access. Unless defined otherwise, no identity except the account root user has access to resources.\nBilling Every AWS account has it\u0026rsquo;s own isolation billing information. This is initially in the form of an attached credit card, but established acccounts can be converted to user traditional, term-based invoicing. By default, you are only billed for resoure in your account. Billing or security exploits are limited to a single account.\nAWS Physical and Networking Layer Regions contain multiple Availability Zones (AZs), which are separated and isolated networks. AZs in the same region are connected with reduandant high-speed, low-latency network connections.\nA failure in one AZ generally won\u0026rsquo;t impact another AZ.\n Most AWS services run within AZs. Some series operate from one AZ, while other replicate between AZs. Some services allow you to choose the AZ to use, and some don\u0026rsquo;t.\nEdge locations are small pockets of AWS compute, storage, and networking close to major populations and are generally used for edge computing and content delivery.\n Well-Architected Framework Well-Architected Framework is a set of Best Practices, Principles, Concepts to help you build effective AWS solutions. It introduces general design principles to help you build efficient systems.\nPillars of Well Architected Framework Security The security pillar includes the ability to protect information, system and assets while delivering business value through risk accessment and mitigation strategies\nDesign Principles\n Implement a strong identity founcation Enable tracebility Apply security at all layers Automate security best practices Protect data in transit and data at rest Prepare for security events  Reliability The reliability pillar includes the ability of a system to recover from infrastructure or service distruptions, dynamically acquire computing resources to meet demand, and mitigate distruptions, such as misconfigurations or transient network issues.\nDesign Principles\n Test recovery procedures Automatically recover from failure Scale horizontally to increase aggregate system availability Stop guessing capacity Manage change in automation  Performance Efficiency The performance efficiency pillar includes the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve.\nDesign Principles\n Democratize advanced technologies Go global in minutes Use serverless architecture Experiment more often Mechanical sympathy  Operational Excellence The operational excellence pillar includes the ability to run and monitor systems to deliver business value and to continually improve supporting processesses and procedures.\nDesign Principles\n Perform operations as code Annotate documentation Make frequent, small, reversible changes Refine operations procedures frequently Anticipate failure Learn from all operational failure  Cost Optimization The cost optimization pillar includes the ability to avoid or elimiate unneeded cost of suboptimal resources.\nAWS Well-Architected\nAWS-Well Architected Framework - PDF\nElasticity Traditional legacy system use vertial scalling. An attempt is made to forecast demand and purchase servers ideally before the demand passes current capacity. Purchase too early and capacity is wasted. Purchase too late and performance is impacted.\nWhen horizontal scalling is used (more smaller servers), capacity can be maintained closer to demand. There is less waste because servers are smaller and there\u0026rsquo;s less risk of performance impact as each increase is less expensive, so it generally require less approval.\nElasticity, or Elastic Scalling is where automation and horizontal scalling are used in conjunction to match capacity with demand. Demand is rarely so linear - it can increase or decrease, often in a rapid and sudden way. An efficient platform should scale OUT and IN, matching demand on that system.\nIntroduction to S3 - (Simple Storage Service) Simple Storage Service (S3) is a global object storage platfrom that can be used to store objects in the form of text files, photos, audio, movies, large binaries or other object types.\nIntroduction to Cloud Formation CloudFormation is an Infrasturcture as a Code (IaC) product, you can use to create, manage and remove infrastructure using JSON or YAML.\nCloudFormation is effective if you frequently deploy the same infrastructure or you require guarantted consistent configuration\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/02_core_concepts/",
	"title": "02 - Core Concepts",
	"tags": [],
	"description": "",
	"content": "  Kubernetes API Primitives Pods Namespaces Basic Container Configuration  Kubernetes API Primitives Kubernetes API Primitives are also called Kubernetes Objects. These are data objects that represent the state of the cluster. Example of Kubernetes Objects:\n Pod Node Service Service Account  The kubectl api-resource command will list the object types currently available to the cluster.\nEvery object has a spec and status:\n Spec - You provide the spec. This defines the desired state of the object Status - This is provided by the Kubernetes cluster and contains information about the current state of the object.  Kubernetes objects are often represented in the yaml format, so you can create an object by providing the cluster with yaml, defining the object and it\u0026rsquo;s spec.\nYou can get information about an object\u0026rsquo;s spec and status using the kubectl describe command:\nkubectl describe $object_type $object_name  Pods Pods are the basic building blocks of any application running in Kubernetes.\nA Pod consists of one or more containers and a set of resources shared by those containers. All containers managed by Kubenernetes cluster are part of a pod.\nA basic pod in yaml format:\napiVersion: v1 kind: Pod metadata: name: my-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello K8s! \u0026amp;\u0026amp; sleep 3600']  Save the pod\u0026rsquo;s yaml definition to a file my-pod.yml, then create the pod using\nkubectl create -f my-pod.yml  Similarly, you can change existing objects using:\nkubectl apply -f my-pod.yaml # Or kubectl edit pod my-pod  # Delete Pod kubectl delete pod my-pod  Namespaces Namespaces provide a way to keep your objects organized within the cluster. Every object belongs to a namespace. When no namespace is specified, the cluster will assume the default namespace.\nWhen creating an object, you can assign it to a namespace by specifying a namespace in the metadata:\nmypod-ckad-ns.yml\napiVersion: v1 kind: Pod metadata: name: my-pod namespace: my-ckad labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 60s']  When working with objects using kubectl use the -n flag to specify the namespace. For example,\nkubectl get pods -n my-ckad  Basic Container Configuration You can specify the command that will be used to run a container in the Pod spec. Thiw will override any built-in default command specified by the container image.\napiVersion: v1 kind: Pod metadata: name: my-command-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['echo'] args: ['Hello World'] restartPolicy: Never  Ports are another important part of container configuration. If you need a poart that the container is listening on to be exposed to the cluster, you can specify a containerPort\nmy-containerport-pod.yml\napiVersion: v1 kind: Pod metadata: name: my-containerport-pod labels: app: myapp spec: containers: - name: myapp-container image: nginx ports: - containerPort: 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/02_installing_kubernetes/",
	"title": "02 - Installing Kubernetes",
	"tags": [],
	"description": "",
	"content": "  Local Installation On-Premise Installation Cloud Installation  All-in-One Single-Node Installation In this setup, all the master and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, and it should not be used in production. Minikube is one such example, and we are going to explore it in future chapters.\nSingle-Node etcd, Single-Master and Multi-Worker Installation In this setup, we have a single-master node, which also runs a single-node etcd instance. Multiple worker nodes are connected to the master node.\nSingle-Node etcd, Multi-Master and Multi-Worker Installation In this setup, we have multiple-master nodes configured in HA mode, but we have a single-node etcd instance. Multiple worker nodes are connected to the master nodes.\nMulti-Node etcd, Multi-Master and Multi-Worker Installation In this mode, etcd is configured in clustered HA mode, the master nodes are all configured in HA mode, connecting to multiple worker nodes. This is the most advanced and recommended production setup.\nLocal Installation  Minikube - single-node local Kubernetes cluster Docker Desktop - single-node local Kubernetes cluster for Windows and Mac CDK on LXD - multi-node local cluster with LXD containers.  On-Premise Installation  On-Premise VMs Kubernetes can be installed on VMs created via Vagrant, VMware vSphere, KVM, or another Configuration Management (CM) tool in conjunction with a hypervisor software. There are different tools available to automate the installation, such as Ansible or kubeadm.\n On-Premise Bare Metal Kubernetes can be installed on on-premise bare metal, on top of different operating systems, like RHEL, CoreOS, CentOS, Fedora, Ubuntu, etc. Most of the tools used to install Kubernetes on VMs can be used with bare metal installations as well.\n  Cloud Installation  Hosted Solutions  Google Kubernetes Engine (GKE) Azure Kubernetes Service (AKS) Amazon Elastic Container Service for Kubernetes (EKS) DigitalOcean Kubernetes OpenShift Dedicated   Kubernetes The Hard Way\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/modules/",
	"title": "02 - Terraform Modules",
	"tags": [],
	"description": "",
	"content": " Module is a container for multiple resources that are going to be used together.\nMain goal of module is logical grouping of resources to it\u0026rsquo;s cohesive unit that can be reused and shared across different systems. Modules can also be shared across multiple teams or via public registry such as GitHub or Terraform Cloud registry.\nUsing Terraform Modules # Download the image module \u0026quot;image\u0026quot; { source = \u0026quot;./image\u0026quot; image_name = \u0026quot;${var.image_name}\u0026quot; } # Start the container module \u0026quot;container\u0026quot; { source = \u0026quot;./container\u0026quot; image = \u0026quot;${module.image.image_out}\u0026quot; container_name = \u0026quot;${var.container_name}\u0026quot; int_port = \u0026quot;${var.int_port}\u0026quot; ext_port = \u0026quot;${var.ext_port}\u0026quot; }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/02-networking/",
	"title": "02 - VPC Networking",
	"tags": [],
	"description": "",
	"content": " Virtual Private Cloud (VPC)  A private network within AWS. It lets you provision a logically isolated section of AWS cloud where you can launch AWS resources in a virtual network you define. Can be configured to be public/private or a mixture Regional Service (can’t span regions), highly available, and can be connected to your data center and corporate networks Isolated from other VPCs by default VPC and subnet: Max /16 (65,536 IPs) and minimum /28 (16 IPs) VPC subnet cannot span AZs (1:1 Mapping) Certain IPs are reserved in subnets By default you can create up to 5 VPC per region Default VPC  Required for some AWS services Pre-configured with all required network / security configurations A /20 Public subnet in each AZ, allocating a public P by default Attached internet gateway with a \u0026ldquo;main\u0026rdquo; route table sending all IPv4 traffic to the internet gateway using a 0.0.0.0/0 route   VPC Routing  Every VPC has a virtual routing device called the VPC Router Router interconnects subnet and directs traffic entering and leaving the VPC and it\u0026rsquo;s subnets Router Table is a collection of routes that are used when traffic from a subnet arrives at the VPC Router Every route table has a local route, which matches the CIDR of the VPC and lets traffic be routed between subnets A route contains a destination and a target. Traffic is forwarded to the target if its destination matches the route destination If multiple routes apply, the most specific is chosen. A/32 is chosen before a /24, before a /16 A subnet is a public subnet if it is  (1) configured to allocate public IPs (2) if the VPC has an associated internet gateway (3) if that subnet has a default route to that internet gateway.   Subnets  Public Subnet - If a subnet traffic is routed to Internet Gateway, the subnet is known as a Public Subnet Private Subnet - If the subnet doesn\u0026rsquo;t have a route to Internet Gateway, then the subnet is Private Subnet VPN only Subnet - If the subnet doesn\u0026rsquo;t have route to Internet Gateway, but has it\u0026rsquo;s traffic routed to virtual private gateway for a VPN Subnet map 1 on 1 to AZ\u0026rsquo;s and cannot span AZ  NAT Gateway  NAT (Network Address Translation) is a process where the source or destination address of an IP packets are changed Static NAT is process of 1:1 translation where an internet gateway converts a private address to public IP Address Dynamic NAT is a variation that allows many private IP to get outgoing internet access using smaller number of public IP (generally one) Dynamic NAT is provided within AWS using NAT gateway that allows private subnet in AWS VPC to access the internet NAT Gateway is used to enable instances ina private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances NAT Gateway must be crated within a Public Subnet NAT Gateway is not HA by design. For multi AZ redundancy, create NAT gateway in each AZ with routes for private subnet to use local gateway NAT instances are simply EC2 instances with specially configured routing table  Internet Gateway (IGW)  IG is horizontally scaled, redundant and highly available VPC component that allows communication between instance in your VPC and internet IG serves as:  To provide target in your VPC route table for internet routable traffic To perform NAT translation for instance that have been assigned public Ipv4 address  You cannot have multiple IG for a single VPC  VPC Endpoint  VPC Endpoints are gateway objects created within a VPC. They can be used to connect to AWS public servers without the need for the VPC to have an attached internet gateway and be public. VPC Endpoint Types:  Gateway endpoints: Can be used for DynamoDB and S3 Interface endpoints: Can be used for everything else (e.g. SNS, SQS)  Gateway endpoints are free whereas Interface endpoint cost money Gateway endpoints are HA across AZs in a region Interface endpoint uses and Elastic Network Interface (ENI) with private IP Interface endpoints are interfaces in a specific subnet. For HA, you need to add multiple interfaces - one per AZ  Security Group (SG)  Security group acts like a firewall at the instance level Unless allowed specifically, all inbound traffic is blocked by default All outbound traffic from the instance is allowed by default SGs are Stateful, which means if traffic is allowed inbound it is also allowed outbound EC2 instances can belong to multiple SG Using SG, you cannot block specific IP, SG only supports allow You can have up to 10,000 SG per region You can have 60 inbound and 60 outbound rules per Security Group You can have 16 SG associated to an ENI  Network Access Control List (NACL)  NACLs are collection of rules that explicitly allow or deny traffic based on its protocol, port range, and source/destination (Unlike SG, which can only allow) NACL operate at Layer 4 of the OSI Model (TCP/UDP and below) Each subnet within a VPC must be associated with a NACL NACLs only impact traffic crossing the boundary of a subnet. (If communication occurs within a subnet, no NACLs are involved) Rules are processed in number order, lowest first. When a match is found, that action is taken and processing stops NACLs are stateless NACL can be used to block a single IP (SG cannot perform implicitly deny)  AWS Managed VPN  Virtual Private Network (VPN) provides a software based secure connection between a VPC and On-premise networks Components of VPN  Customer Gateway (CGW) - Configuration for On-Premise Router Virtual Private Gateway attached to VPC VPN Connection   VPC Peering  Allows direct communication between VPCs enabling you to route traffic privately using private IPv4 address or IPv6 address Services can communicate using private IPs from VPC to VPC VPC peers can span AWS accounts and even regions (with some limitations) Data is encrypted and transits via the AWS global backbone VPC peers are used to link two VPCs at layer 3 Ideal use cases for VPC peering - company mergers, shared services, company and vendor, auditing During VPC peering, VPC CIDR blocks cannot overlap Routing across VPC is not transitive NACL and SGs can be used to control access on the VPC peering  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/03-storage/",
	"title": "03 - AWS Storage",
	"tags": [],
	"description": "",
	"content": " Amazon S3  Simple Storage Service (S3) is object based Storage Service. Stores unlimited amount of data without worrying about underlying infrastructure S3 replicates data across at least 3 AZ\u0026rsquo;s to ensure 99.99% Availability and 99.99% (11 9\u0026rsquo;s Durability) Objects can be from 0 Bytes up to 5 TB (Multipart Upload). Single PUT upload support up to 5 GB For filesize greater than 100 MB, Multipart Uploads are recommended S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. A Presigned URL is a temporary URL that allows users to see assigned S3 objects using the creator’s credentials. Presigned Urls are commonly used to access private objects.  Buckets  S3 stores data as objects within buckets. Buckets can also contain folders which can in turn contain objects Bucket name must be a unique DNS-complaint name  Bucket names are unique across all AWS accounts After you create the bucket you cannot change the name All new buckets are \u0026ldquo;private\u0026rdquo; by default.  Access to S3 buckets are configured using Bucket Policy and Access Control List (ACL). ACLs are legacy mechanism to handle access control  Versioning  Once versioning is turned On, Objects are given a VersionID When a new objects are uploaded the old objects are kept. You can access any object version With versioning enabled, an AWS account is billed for all versions of all objects Object deletions by default do not delete an object - instead a delete marker is added to indicate the object is deleted MFA Delete enforce DELETE operation to require MFA token in order to delete an object. To enable MFA delete, versioning should be turned on Once a bucket is version-enabled, it can never be fully switched off - only suspended.  Amazon S3 Security  As Data in Motion, all the data transfer coming in and out from S3 are encrypted with SSL Data at Rest can be configured per object basis using  AWS Managed Server Side Encryption - (SSE-KMS) - Envelope encryption via AWS KMS and you mange the keys S3 Managed Server Side Encryption - (SSE-AES) - S3 handles the key, uses AES-256 algorithm Client Side Encryption - Client is responsible for managing both encryption / decryption and it\u0026rsquo;s keys   Storage Classes S3 Standard  Default, all purpose storage or when usage is unknown 99.99 (11 nines) Durability and four nines Availability No minimum object size No retrieval fee  S3 Standard Infrequent Access (Standard-IA)  For long-lived, but less frequently accessed data Stores the object data redundantly across multiple geographically separated AZs. (Replicated 3+ AZ) 30 Days and 128 KB minimum charge and object retrieval fee  S3 One Zone Infrequent Access (OneZone-IA)  Less expensive than Standard-IA Data stored in only 1 AZ (99.5% Availability) Ideal for non mission critical and/or reproducible objects 30 Days and 128 KB minimum charge and object retrieval fee  Amazon S3 Intelligent Tiering  Storage class designed for customer who want to optimize storage costs automatically when data access patterns change, without performance impact or operational overhead Automatically move data between Frequent access and infrequent access tiers S3 Intelligent-Tiering monitors access patterns and move objects that have not been accessed for 30 consecutive days to the infrequent access tier.  Glacier  For long-term archival storage (warm and cloud backups) Archived objects are not available for real-time access. You must first restore the objects before you can access them 3+ AZ replication, 90 Days and 40KB minimum charge and retrieval fee  Glacier Deep Archive  Long-term retention of data that is accessed rarely in a year Lowest cost storage 180 Days and 40KB minimum charge and retrieval fee\n Storage Classes can be controlled via Lifecycle Rules, which allow for the automated transition of object between storage classes, or in certain cases allow for the expiration of objects that are no longer required.\n  S3 Cross Region Replication (CRR)  Feature that can be enabled on S3 buckets allowing one-way replication of data from a source bucket to a destination bucket in another region You must have versioning enabled in the source and destination bucket to enable CRR You can have CRR replicate to bucket in another AWS Account Replication requires an IAM role with permission to replicate objects, With the replication configuration, it is possible to override the storage class and object permission as they are written to the destination.  Amazon Elastic File System (EFS)  EFS provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services, and on-premises resource It is a popular shared storage system that can be natively mounted as a file system within Linux instances With EFS, File system can be created and mounted on multiple Linux instances at the same time Amazon EFS offers two storage classes: The Standard storage class, Infrequent Access Storage Class EFS is designed for large scale parallel access of data. Ideal use cases include, Shared media, Logging solutions where various clients need to access Shared Data Amazon EFS is Region Resilient. Security groups are used to control access to NFS mount targets.  AWS Storage Gateway  Hybrid storage service that allows you to migrate data into AWS, extending your on-premises storage capacity using AWS There are three main types of Storage Gateway:  File gateway Volume gateway Tape gateway  A File gateway supports a file interface into AWS S3 and combines a server and a virtual software appliance. Using File gateway, you can store and retrieve objects in Amazon S3 using NFS and SMB Volume Gateway provides cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI). The volume gateway is deployed into your on-premises environment as a VM running on VMWare ESXI, KVM or Microsoft Hyper-V hypervisor A tape gateway provides cloud-backed virtual tape storage. The tape gateway is deployed into your on-premises environment as a VM running on VMware ESXi, KVM or Microsoft Hyper-V hypervisor  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/03_configuration/",
	"title": "03 - Configuration",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps SecurityContexts Resource Requirements Secrets Service Accounts   ConfigMaps A ConfigMap is a Kubernetes Object that stores configuration data in a key-value format. This configuration data can then be used to configure software running in a container, by referencing the ConfigMap in the Pod spec.\nmyconfigmap.yml\napiVersion: v1 kind: ConfigMap metadata: name: my-config-map data: myKey: myValue anotherKey: anotherValue  Passing ConfigMap data to a pod\u0026rsquo;s container as an environment variable:\nmyconfigmap-pod.yml\napiVersion: v1 kind: Pod metadata: name: my-configmap-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo $(MY_VAR) \u0026amp;\u0026amp; SLEEP 3600\u0026quot;] env: - name: MY_VAR valueFrom: configMapKeyRef: name: my-config-map key: myKey  It\u0026rsquo;s also possible to pass ConfigMap data to container, in the form of file using a mount volume.\napiVersion: v1 kind: Pod metadata: name: my-configmap-volume-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo $(cat /etc/config/myKey) \u0026amp;\u0026amp; sleep 3600\u0026quot;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: my-config-map  SecurityContexts A Pod\u0026rsquo;s Security Context defines privilege and access control settings for a pod. If a container needs special operating system-level permissions, we can provide them using the securityContext.\nThe securityContext is defined as part of a Pod\u0026rsquo;s spec.\nThis spec will cause the container to run as the OS user with an ID of 1000, and the containers will run as the group with and ID of 2000.\napiVersion: v1 kind: Pod metadata: name: my-securitycontext-pod spec: securityContext: runAsUser: 2000 fsGroup: 3000 containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;cat /message/message.txt \u0026amp;\u0026amp; sleep 3600s\u0026quot;]  Resource Requirements Kubernetes allows us to specify the resource requirements of a container in the pod spec. A container\u0026rsquo;s memory and CPU requirements are defined in terms of resource requests and limits.\nResoruce request: The amount of resource necessary to run a container. A pod will only be a run on a node that has enough available resources to run the pod\u0026rsquo;s containeers.\nResource limit: A maximum value for the resource usage of a container\nResource requests and limits are defined as part of a pod\u0026rsquo;s spec.\napiVersion: v1 kind: Pod metadata: name: my-resource-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot;  Memory is measured in bytes. 128Mi means 128 Mebibytes CPU is measured in \u0026ldquo;cores\u0026rdquo;. 500m means 500 milliCPU or 0.5 CPU cores\nSecrets Secrets are pieces of sensitive information store in the Kubernetes cluster, such as passwords, tokens and keys. If a container needs a sensitive piece of information, such as password, it is more secure to store it as a secret than storing it in a pod spec or in the container itself.\nmy-secret.yml\napiVersion: v1 kind: Secret metadata: name: my-secret stringData: myKey: myPassword  apiVersion: v1 kind: Pod metadata: name: my-secret-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! $(MY_PASSWORD) \u0026amp;\u0026amp; sleep 3600\u0026quot;] env: - name: MY_PASSWORD valueFrom: secretKeyRef: name: my-secret key: myKey  Service Accounts ServiceAccounts allow containers running in pods to access the Kubernetes API. Some applications may need to interact with the cluster itself, and service accounts provide a way to let them to do it securely, with properly limited permissions.\nYou can determine the ServiceAccount that a pod will use by specifying a serviceAccountName in the pod spec:\n# Createing Service Account kubectl create serviceaccount my-serviceaccount  apiVersion: v1 kind: Pod metadata: name: my-serviceaccount-pod spec: serviceAccountName: my-serviceaccount containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/03-identity_access_control/",
	"title": "03 - Identity and Access Control",
	"tags": [],
	"description": "",
	"content": "  Identity and Access Control  IAM Essentials IAM Policies IAM Users IAM Groups IAM Access Keys  Multi-Account Management and Organizations  AWS Organizations Role Switching Between Accounts   Identity and Access Control IAM Essentials Identity and Access Management, known as IAM, is one of the key services within AWS. It controls access to the AWS API endpoints that are used by the console UI, command line tools, and any applications wanting to utilize AWS. Identity and Access Management (IAM) is the primary service that handles authentication and authorization within AWS environments.\nIAM controls access to AWS service via policies that can be attached to users, groups and roles. Users are given long-term credentials to access AWS resource (username and password or access keys).\nRoles allow for short-term access to resources when assumed, using temporary access credentials.\nIAM Policies IAM policies are JSON documents that either allow or deny access to combinations of actions and resources. An IAM policy (policy document) is known as an identity policy when attached to an identity or a resource policy when attached to a resource. They have no effect until they are attached to something.\nA policy document is a list of statements.\nEach statement matches a request to AWS. Requests are matched based on their Action(or actions), which are the API calls or operations being attempted and the Resource (or resources) the request is against. A given statement results in an Allow or Deny for the request.\nIAM Policy - Exam Tips\n If a request isn\u0026rsquo;t explicitly allowed, it\u0026rsquo;s implicity (default) denied. If a request is explicitly denied, it overrides everything else. If a request is explicitly allowed, it\u0026rsquo;s allowed unless denied by an explicit deny. Remember: DENY -\u0026gt; ALLOW -\u0026gt; DENY Only attached policies have any impact When evaluating policies, all applicable policies are merged:  All identity (user, group, role) and any resource policies  Managed policies allow the same policy to impact many identities. Inline policies allow exceptions to be applied to identities. AWS-managed policies are low overhead but lack flexibility. Customer-managed policies are flexible but require administration Inline and managed policies can apply to users, groups and roles.  Use Managed Policies to control the base level permissions and for customization use in-line permissions as needed.\n IAM Users IAM users are a type of IAM identity suitable for long-term access for a known entity (human, service, application)\nPrincipals authenticate to IAM users either with a username and password or using access keys.\nExam Facts and Figures:\n Hard limit of 5,000 IAM users per account 10 group membership per IAM user Default maximum of 10 managed policies per user No inline limit, but you cannot exceed 2048 characters for all inline policies on an IAM user 1 MFA per user 2 Access Keys per user  IAM Groups IAM groups allow for large-scale management of IAM users. This way, policies can be applied to groups and impact collections of similar users.\nExam Facts and Figures:\n Groups are an admin feature to group IAM users. Groups can contain many IAM users, and users can be in many groups. IAM inline policies can be added to IAM groups - and these flow onto IAM users who are memebers Managed IAM policies can be attached and flow on to IAM users who are members Groups are not true identities, and they can\u0026rsquo;t be referenced from resource policies Groups have no credentials  IAM Access Keys Access keys consist of access key IDs and secret access keys. The access key ID is the public part of the key and is stored by AWS once generated. The secret access key is the sensitive and private part of the access key available only once when the access key is initally generated. It is stored only the owner of the key and should never be revealed.\nAccess keys are the long-term credentials used to authenticate to AWS for anything but the console UI.\n Multi-Account Management and Organizations AWS Organizations AWS Organizations is useful for businesses that need to manage multiple accounts. It provides the following features:\n Consolidated billing Service control policies (SCPs) Account creation Simplified role switching  Role Switching Between Accounts "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/03_minikube/",
	"title": "03 - Minikube",
	"tags": [],
	"description": "",
	"content": "  Installing Minikube  Command Line Interface (CLI) tools and scripts Web-based User Interface (Web UI) from a web browser APIs from CLI or programmatically Kubectl Proxy   Installing Minikube # Install Minikube brew install minikube # Starting Minikube minikube start minikube start --vm-driver=xhyve minikube start --vm-driver=hyperkit minikube status minikube stop  Any healthy running Kubernetes cluster can be accessed via any one of the following methods:\nCommand Line Interface (CLI) tools and scripts kubectl is the Kubernetes Command Line Interface (CLI) client to manage cluster resources and applications. It can be used standalone, or part of scripts and automation tools. Once all required credentials and cluster access points have been configured for kubectl it can be used remotely from anywhere to access a cluster.\nWeb-based User Interface (Web UI) from a web browser APIs from CLI or programmatically # Open Minikube Dashboard minikube dashboard # Serving on different Port kubectl proxy #Dashboard URL http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy/#!/overview?namespace=default  Kubectl Proxy When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates.\nA Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.\nGetting Token\nTOKEN=$(kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\\t' | tr -d \u0026quot; \u0026quot;)  Getting API Server Endpoint\nAPISERVER=$(kubectl config view | grep https | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)  Confirm that the APISERVER stored the same IP as the Kubernetes master IP by issuing the following 2 commands and comparing their outputs:\n$ echo $APISERVER https://192.168.99.100:8443 $ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 ...  Access the API server using the curl command, as shown below:\ncurl $APISERVER --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure  By using the kubectl proxy we are bypassing the authentication for each and every request to the Kubernetes API.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_1-compute_server_based/",
	"title": "04 (A) - Compute Services - Server Based",
	"tags": [],
	"description": "",
	"content": "  Elastic Cloud Compute (EC2) Elastic Block Storage (EBS)  Exam Facts General Purpose (gp2): (SSD) Provisioned IOPS SSD (io1): (SSD) Throughput Optimized(st1): (HHD) Cold HDD (sc1): HDD  EBS Snapshots Security Groups Instance Metadata AMI Bootstrap Private Instance and Public Instance Advanced Topics  EC2 Instance Roles EBS Volume and Snapshot Encryption EBS Optimized, Enhanced Networking, and Placement Group EBS optimization Enhanced networking  Cluster, partition, and spread placement groups  EC2 Billing Models - Spot Instances EC2 Billing Models - Reserved Instances Dedicated Hosts   Elastic Cloud Compute (EC2) EC2 is one of the most widely used services within AWS. As an Infrastructure as a Service (IaaS) product, it\u0026rsquo;s responsible for providing long-running compute as a service.\nEC2 instances are grouped into families, which are designed for a specific broad type workload. The type determines a certain set of features, and size decide the level of workload they can cope with.\nThe current EC2 families are:\n General Purpose Compute Optimized Memory Optimized Storage Optimized Accelerated Computing  Instance Types include:\n T2 and T3: Low cost instance types that provide burst capability M5: for general Workoads C4: Provides more capable CPU X1 and R4: Optimize large amounts of Fast Memory I3: Delivers fast IO P2, G3 and F1: Deliver GPU and FPGA  Instance sizes include Nano, Small, Medium, Large, X.Large, 2X.Large and Larger\nSpecial Cases\n \u0026ldquo;a\u0026rdquo;: Use AMD CPU \u0026ldquo;A\u0026rdquo;: Arm based \u0026ldquo;n\u0026rdquo;: Higher speed networking \u0026ldquo;d\u0026rdquo;: NVMe storage  Elastic Block Storage (EBS) Elastic Block Storage is a storage service that creates and manages volumes based on four underlying storage types. Volumes are persistent, can be attached and removed from EC2 instances, and are replicated within a single AZ.\nTo protect against AZ failure, EBS snapshots (to S3) can be used. Data is replicated across AZs in the region and (optionally) internationally.\n Exam Facts  EBS supports a maximum per-instance throughput of 1750 MiB/s and 80,000 IOPS.  General Purpose (gp2): (SSD)  Default for most workloads 3 IOPS/GB (100 IOPS - 16,000 IOPS) Burst up to 3,000 IOPS (credit based) Volume Size of 1 GiB - 16 TB size, max throughput p/vol of 250 MB/s  Provisioned IOPS SSD (io1): (SSD)  Used for application that require sustained IOPS performance Large database workloads Volume size of 4 GB - 16TB, max throughput of 1000 MB/s  Throughput Optimized(st1): (HHD)  Low storage cost Used for frequently accessed, throughput-intensive workloads (streaming, bigdata) Cannot be a boot volume Volume size of 500GB - 16TB Max throughput - 500MB/s and IOPS 500  Cold HDD (sc1): HDD  Lowest cost Infrequent accessed data Cannot bee a boot volume Volume size of 500 GB - 16 TB Max throughput - 250MB/s and IOPS 250  EBS Snapshots EBS Snapshots are a point-in-time backup of an EBS volume stored in S3. The initial snapshot is a full copy of the volume. Future snapshots only store the data changed since the last snapshot.\nSnapshots can be used to create new volumes and are a great way to move or copy instances between AZs. When creating a snapshot of the root/boot volume of an instance of busy volume, it\u0026rsquo;s recommended the instance is powered off, or disks are \u0026ldquo;flushed\u0026rdquo;\nSnapshots can be copied between regions, shared and automated using Data Lifecycle Manager (DLM).\nSecurity Groups Security Groups are software firewalls that can be attached to network interface and (by association) products in AWS. Security groups each have inbound rules and outbound rules. A rule allows traffic to and from a source (IP, network, named AWS entity) and protocol. Security Group belongs to a VPC\nEach Elastic Network Interface (ENI) can have upto 5 security groups.\n Security group have a hidden implicit/default deny rule but cannot explicitly deny traffic.\n They are stateful - meaning for any traffic allowed in/out, the return traffic is automatically allowed. Security groups can reference AWS resource, other security groups, and even themselves.\nInstance Metadata Instance metadata can be used to access information about an instance from the instance. It allows applications running within EC2 to have visibility into their environment. Instance metadata is data relating to the instance that can be accessed from within the instance itself using a utility capable of accessing HTTP and using the URL\nhttp://169.254.169.254/latest/meta-data  Instance metadata is a way that scripts and application running on EC2 can get visibility of data they would normally need API calls for.\nThe metadata can provide the current external IPv4 address for the instance, which isn\u0026rsquo;t configured on the instance itself but provided by the internet gateway in the VPC. It provides the Availability Zone the instance was lanched in and the security group applied to the instance. IN the case of spot instances, it also provides the approximate time the instance will terminate.\nRemember the IP address to access metadata: 169.254.169.254\n AMI AMIs (Amazon Machine Images) are used to build instance. They store snapshots of EBS volumes, permissions, and a block device mapping, which configures how the instance OS see the attached volumes. AMIs can be shared, free or paid and can be copied to other AWS regions.\nTypes of AMIs\n Instance Store Back AMIs - Root volume doesn\u0026rsquo;t use EBS EBS Backed AMIs - Root volume uses EBS  Process of Creating AMI\n Configure Instance - Source instance and attached EBS volumes are configured with any required software and configuration.\n Create Image - Snapshots are created from volumes. AMI references snapshots, permission, and block device mapping.\n Launch Instance - With approriate launch permissions, instances can be created from an AMI. EBS volumes are created using snapshots as the source, and an EC2 instance is created uinsg the block device mapping to reference its new volumes.\n  AMI\u0026rsquo;s can be used to build servers with complex configuration.\nDownside of using AMI is you can\u0026rsquo;t do dynamic configuration\n Bootstrap Bootstrapping is a process where instructions are executed on an instance during its launch process. Bootstraping is used to configure the instance, perform software installation, and add application configuraiton.\nIn EC2, user data can be used to run shell scripts or run cloud-init directives.\nBootstrap vs AMI: With AMI\u0026rsquo;s you can reduce time it takes to install and configure versus with bootstrap you can use shell commands to perform dynamic configuration.\nPrivate Instance and Public Instance  Private Instance - Private IP allocated when launching instances. Unchanged during stop/start. Released when terminated. Public Instance - Same private address as private instance. A public IPv4 address is allocated when the machine starts and deallocated when it stops. By default all public IPv4 addresses are dynamic Elastic IP - Elastic IPs are static. When allocated, they replace the normal public IP, which is deallocated.  Advanced Topics EC2 Instance Roles EC2 instance roles are IAM roles that can be \u0026ldquo;assumed\u0026rdquo; by EC2 using an itermediary called an instance profile. An instance profile is either created automatically when using the console UI or manually when using the CLI. It\u0026rsquo;s a container for the role that is associated with an EC2 instance.\nThe instance profile allows application on the EC2 instance to access the credentials from the role using the instance metadata.\nEBS Volume and Snapshot Encryption Volume encryption uses EC2 host hardware to encrypt data at rest and in transit between EBS and EC2 instances. Encryption generates a data encryption key (DEK) from a customer master key (CMK) in each region. A unique DEK encrypts each volume. Snapshots of that volume are encrypted with the same DEK, as are any volumes created from that snapshot.Encrypted DEKs stored with volume are decrypted by KMS using a CMK and given to the EC2 host.\nPlaintext DEKs stored in EC2 memory and used to encrypt and decrypt data. The EC2 instance and OS see plaintext data as normal - no performance impact.\nEBS encryption are configured at per volume basis but can also be specified on per-account basis.\nIf you need to manage the Keys and you need encryption on OS Level, EBS encryption won\u0026rsquo;t fullfill the need, you would need OS level encryption.\n EBS Optimized, Enhanced Networking, and Placement Group EBS optimization EBS-optimized mode, which was historically optional is now the default, adds optimizations and dedicated communcation paths for storage and traditional data networking. This allows consistent utilization of both - and is one required feature to support higher performance storage.\nRestoring from EBS Snapshot - Not all data is copied immediately to the new volume. Data will be copied over time in the background. This preliminary action takes time and can cause a significant increase in the latency of I/O operations the first time each block is accessed which will temporarily adversely affect performance. Volume maximum performance is achieved after all blocks have been downloaded and written to the volume.\nEnhanced networking Traditionally, virtual networking meant a virtual host (EC2 host) arranging access for n virtual machines to access one physical network card - this multitasking is done in software and is typically slow.\nEnhanced networking uses SR-IOV, which allows a single physical network card to appear as multiple physical devices. Each instance can be given one of these (fake) physical devices. This results in faster transfer rate, lower CPU usage, and lower consistent latency. EC2 delivers this via the Elastic Network Adapter (ENA) or Intel 82599 Virtual Function (VF) interface.\nCluster, partition, and spread placement groups  Cluster Placement Group Cluster placement groups place instances physically near each other in a single AZ. Every instance can talk to each other instance at the same time at full speed. Works with enhanced networking for peak performance.  Use Cluster Placement Group for maximum performance\n  Partition Placement Group Instances deployed into a partition placement group (PPG) are separated into partitions, each occupying isolated rack in AZs. PPG can span multiple AZs in a region. PPGs minimize failure to a partiion and give you visibility on placement.  Use Partition Placement Group if you have large infrastructe platform and provides visibility on placement.\n  Spread Placement Group Spread placement groups (SPGs) are designed for a max of seven instances per AZ that need to be separated. Each instance occupies a partition and has an isolated fault domain.  Use Spread Placement Group for maximum availability.\n EC2 Billing Models - Spot Instances Default Billing Model of EC2 is per-second charge for compute instance.\n Spot instance allow consumption of spare AWS capacity for a given instance type and size in a specific AZ. Instances are provided for as long as your bid price is above the spot price, and you ony ever pay the spot price. If your bid is exceeded, instances are terminated with a two-minute warning.\nSpot fleets are a container for \u0026ldquo;capacity needs\u0026rdquo;. You can specify pools of instances of certain types/sizes aiming for a given \u0026ldquo;capacity\u0026rdquo;. A minimum percentage of on-demand can be set to ensure the fleet is always active.\nSpot instances are perfect for non-critical workloads, burst workloads or consistent non-critical jobs that can tolerate interruptions without impacting functionality. Spot is not suitable for long-running workloads that require stability and cannot tolerate interruptions.\nSpot instances are not suitable for long-running workoads that require stability and cannot tolerate interruptions.\n Spot Pricing can save you more than 90% than on-demand application.\n EC2 Billing Models - Reserved Instances Reserved instance lock in a reduced rate for one or three years. Zonal reserved instance include a capacity reservation. Your commitment incurs costs even if instance aren\u0026rsquo;t launched. Reserved purchases are used for long-running, understood, and consistent workloads.\nWhen to Use Reserved Purchases\n Base / Consistent Load Known and Understood Growth Critical Systems / Components  When to Use Spot Instances / Fleets\n Burst-y workloads Cost-critial, when can cope with interruptions  When to Use On-Demand\n Default or unknown demand Anything in between reserved/spot Short-term workloads that cannot tolerate interruptions  Dedicated Hosts EC2 dedicated hosts are a feature of EC2, giving you complete control over physical instance placement and dedicated hardware free from other customer interaction. Dedicated hosts are EC2 hosts for a given typ and size that can be dedicated to you. The number of instances that can run on the host is fixed - depending on the type and size. An on-demand or reserved fee is charged for the dedicated host - there are no charges for instance running on the host. Dedicated hosts are generally used when software is licensed per core/CPU and not compatibile with running within a shared cloud environment.\nFor dedicated host, you are paying for the hourly charge.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_2-compute_serverless/",
	"title": "04 (B) - Compute Services - Serverless",
	"tags": [],
	"description": "",
	"content": "  Serverless Compute  Lambda Essentials API Gateway Step Functions  Container Based Compute  ECS   Serverless Compute Serverless architecture consists of two main principles, including BaaS (Backend as a Service), which means using third party services where possible rather than running your own. Example include Auth0 or Cognito for authentication and Firebase or DynamoDb for data storage. Servless architecture uses event driven architecture using FaaS (Function as a Service) products to provide application logic. These functions are only actively invoked when they are needed.\nLambda Essentials Lambda is a FaaS product. Function are code, which run in a runtime. Functions are invoked by events, perform actions for upto 15 minutes and terminate. Functions are also stateless - each run is clean.\nAPI Gateway API Gateway is a managed API endpoint service. It can be used to create, publish, monitor and secure APIs \u0026ldquo;As a Service\u0026rdquo;. API Gateway can use other AWS services for compute (FaaS/IaaS) as well as to store and recall data.\nAccess to AWS Service is provided by the function\u0026rsquo;s execution role. This role is assumed by Lambda, and temporary security credentials are available to the function via STS.\n Pricing is based on the number of API calls, the data transferred and any caching required to improve performance.\n API Gateway can access some AWS services directly using proxy mode.\n Step Functions Step Functions is a serverless visual workflow servcie that provides state machines. A state machine can orchestrate other AWS services with simple logic, branching, and parallel execution, and it maintains a state. Workflow steps are known as states, and they can perform work via tasks. Step Functions allows for long-running serverless workflows. A state machine can be defined by using Amazon State Language (ASL).\nWithout Step Functions, Lambda functions could only run for 15 minutes. Lambda functions are stateless. State machines maintain state and allow longer running processess.\nContainer Based Compute ECS ECS is a managed container engine. It allows Docker containers to be deployed and managed within AWS environments. ECS can use infrastructure clsutes based on EC2 or Fargate where AWS manages the backing infrastructure.\nWith EC2 launch type utilizes your own EC2 instances. AWS Fargate is a managed service, so tasks are auto placed.\nCluster - A logical collection of ECS resources - either ECS EC2 instances or a logical representation of managed Fargate infrastructure\nTask Definition - Defines your application. Similar to Dockerfile but for running containers in ECS. Task definition can contain multiple containers.\nContainer Definition - Inside a task definition, a container definition defines the individual containers a task uses. It controls the CPU and memory each container has, in addition to port mappings for the container\nTask - A single running copy of any containers defined by a task definition. One working copy of an application\nServices - Allows task definitions to be scaled by adding additional tasks. Define minimum and maximum values.\nRegistry - Storage for container images. (eg. ECS Container Registry or Dockerhub). Used to download image to create containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/04_kubernetes_building_blocks/",
	"title": "04 - Kubernetes Building Blocks",
	"tags": [],
	"description": "",
	"content": "  Kubernetes Object Model Pods  Labels  Replication Controller Replica Set Deployments Namespaces  Kubernetes Object Model With each object, we declare our intent or the desired state under the spec section. When creating an object, the object\u0026rsquo;s configuration data section from below the spec field has to be submitted to the Kubernetes API server.\nExample of Deployment object configuration in YAML format.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  Pods A pod is the basic unit that Kubernetes deals with. Containers themselves are not assigned to hosts. Instead, closely related containers are grouped together in a pod. A pod generally represents one or more containers that should be controlled as a single “application”.\nA Pod is the smallest and simplest Kubernetes object. It is the unit of deployment in Kubernetes, which represents a single instance of the application. A Pod is a logical collection of one or more containers, which:\n Are scheduled together on the same host with the Pod Share the same network namespace Have access to mount the same external storage (volumes).  Below is an example of a Pod object\u0026rsquo;s configuration in YAML format:\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  # List all the Pods kubectl get pods # Displays details of Pod kubectl describe pod webserver-74d8bd488f-dwbzz  Labels Labels are key-value pairs attached to Kubernetes objects (e.g. Pods, ReplicaSets). Labels are used to organize and select a subset of objects, based on the requirements in place. Many objects can have the same Label(s). Labels do not provide uniqueness to objects. Controllers use Labels to logically group together decoupled objects, rather than using objects\u0026rsquo; names or IDs.\n# Lists Pods with labels kubectl get pods -L k8s-app,label2 # Select the Pods with a given Label kubectl get pods -l k8s-app=webserver  Replication Controller Although no longer a recommended method, a ReplicationController is a controller that ensures a specified number of replicas of a Pod is running at any given time. If there are more Pods than the desired count, a replication controller would terminate the extra Pods, and, if there are fewer Pods, then the replication controller would create more Pods to match the desired count. Generally, we don\u0026rsquo;t deploy a Pod independently, as it would not be able to re-start itself if terminated in error. The recommended method is to use some type of replication controllers to create and manage Pods.\nThe default controller is a Deployment which configures a ReplicaSet to manage Pods\u0026rsquo; lifecycle.\nReplica Set A ReplicaSet is the next-generation ReplicationController. ReplicaSets support both equality- and set-based selectors, whereas ReplicationControllers only support equality-based Selectors. Currently, this is the only difference.\nWith the help of the ReplicaSet, we can scale the number of Pods running a specific container application image. Scaling can be accomplished manually or through the use of an autoscaler.\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\n # List Replica Sets kubectl get replicasets  Deployments Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node\u0026rsquo;s controller manager, and it ensures that the current state always matches the desired state. It allows for seamless application updates and downgrades through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling.\n# Lists all the Deployments in a given namespace kubectl get deployments # Deleting Deployment (Along with ReplicaSet and Pods) kubectl delete deployments webserver  Deleting a Deployment also deletes the ReplicaSet and the Pods it created.\n Namespaces If multiple users and teams use the same Kubernetes cluster we can partition the cluster into virtual sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/04_multi_container_pods/",
	"title": "04 - Multi Container Pods",
	"tags": [],
	"description": "",
	"content": " Multi-container pods are simply pods with more than one container that all work together as a single unit.\nIt is often a good idea to keep containers separate by keeping them in their own separate pods, but there are several cases where multi-container pods can be beneficial.\nYou can create multi-container pods by listing multiple containers in the pod definition.\napiVersion: v1 kind: Pod metadata: name: multi-container-pod spec: containers: - name: nginx image: nginx:1.15.8 ports: - containerPort: 80 - name: busybox-sidecar image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;while true; do echo \u0026quot;Hello From SideCar\u0026quot;; sleep 3600s; done;\u0026quot;]  How can containers interact with one another in a pod ?\n Shared Network - All listening ports are accessible to other containers in the pod, even if they are not exposed outside the pod. Shared Storage Volumes - Containers can intereact with each other by reading and modifying files in a shared storage volume that is mounted with both containers. Shared Process Namespace - With process namespace sharing enabled, containers in the same pod can interact with and signal one another\u0026rsquo;s process.  Design Patterns for Multi-Container Pods  Sidecar Pod - The sidecar pattern uses a sidecar container that enhances or adds functionality of the main container in some way. This could be something like, Eg, a sidecar that syncs files from a Git repository to the file system in a web server container. Ambassador Pod - The ambassador pattern uses an ambassador container to accept network traffic and pass it on to the main controller. One example might be an ambassador that listens on a custom port, and forwards traffic to the main container on its hard-coded port. Adapter Pod - The adapter pattern uses an adapter container to change the output of the main container in some way. An example could be an adapter that formats and decorates log output from the main container.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/04-network-content-delivery/",
	"title": "04 - Network and Content Delivery",
	"tags": [],
	"description": "",
	"content": " Amazon CloudFront  CloudFront is a CDN (Content Delivery Network). It makes website load faster by serving cached content Benefits of using CloudFront includes  Lower Latency Higher Transfer Speed Reduced load on Content Server  Origin is the address of where the original copies of your files reside eg. S3, EC2, ELB Distribution defines a collection of Edge locations and behaviors on how it should handle your cached content Distribution has 2 types: Web Distribution (static website content) and RTMP (streaming media) Edge Locations are local infrastructure that hosts cache of data Origin Identity Access (OAI) is used to access private S3 buckets, restricting S3 bucket access only via Cloud Front Access to cached content can by protected via Signed Urls or Signed Cookies Lambda@Edge allows you to pass each request through a Lambda to change the behavior of the response  Amazon Route 53  Route53 is a DNS provider, register and manage domains, create record sets and health check of resources Simple Routing (Default) - A simple routing policy is a single record within a hosted zone that contains one or more values. When queried a simple routing policy record returns all the values in a randomized order. Weighted Routing - Weighted routing policy can be used to control the amount of traffic that reaches specific resources, based on different \u0026lsquo;weights\u0026rsquo; assigned (Percentages). It can be useful when testing new software or when resources are being added or removed from. Latency Based Routing - Directs traffic based on region, for lowest possible latency for users Failover Routing - Failover routing allows you to create two records with same name. One is designed as the primary and another as secondary. Queries will resolve to the primary - unless it is unhealthy, in which the Route 53 will respond with the secondary. Geolocation Routing - Route traffic based on the geographic location of a requests origin Traffic Flow - Visual editor, for chaining routing policies, can version policy records for easy rollback AWS Alias Record - AWS\u0026rsquo;s smart DNS record, detects changed IPs for AWS resources and adjusts automatically Route53 Resolver - Lets you regionally route DNS queries between your VPC and your on-premise network  API Gateway  Enabled developers to Create, Publish, Maintain, Monitor and secure APIs Api gateway can use other AWS Services With Lambda, API Gateway forms the front facing part of AWS serverless infrastructure Stages allow you to have multiple published version of your API. Eg, staging, QA, prod CORS issues are common with API Gateway, CORS can be enabled on all or individual endpoints With API Gateway, you can use setup cache with customizable keys and TTL for your API data API Gateway is integrated with CloudWatch, so you get backend performance metrics such as API calls, latency, and error rates API Gateway can also log API execution errors to CloudWatch Logs.  AWS Direct Connect  A Direct Connect (DX) is a physical connection between your network and AWS either directly via a cross-connect and customer router at a DX location or DX partner Ideal used for Higher throughput network traffic with low latency  AWS Elastic Load Balancers (ELB)  ELB is a service that provides a set of highly available and scalable load balancers in one of three versions: Classic (CLB), Application (ALB) and Network (NLB) ELBs can be paired with Auto Scaling groups to enhance high availability and fault tolerance - Automating scaling / Elasticity An elastic load balancer has a DNS record, which allows access to the external side ELBs cannot go cross-region. You must create one per region  Classic Load Balancers  CLB use Listeners and EC2 instances are directly registered as targets to CLB Support L3 and L4 (TCP and SSL) and some HTTP/S features Supports 1 SSL certificate per CLB - can get expensive for complex projects Sticky sessions can be enabled for CLB  Application Load Balancers  Operates on L7 of the OSI model ALB has Listeners, Rules and Target Groups to route traffic ALBs are now recommend as the default LB for VPCs. They perform better than CLBs and are most always cheaper. Content rules can direct certain traffic to specific target groups.  Host-based rules: Route traffic based on the host used Path-based rules: Route traffic based on URL path  ALBs support EC2, ECS, EKS, Lambda, HTTPS, HTTP/2 and WebSockets, and they can be integrated with AWS Web Application Firewall (WAF) Sticky sessions can be enabled for ALB   Network Load Balancers  NLB user Listeners and Target Groups to route traffic NLB is for high network throughput applications  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/05-database/",
	"title": "05 - AWS Database Services",
	"tags": [],
	"description": "",
	"content": " Amazon RDS (Relational Database Service)  RDS is a Database as a Service (DBaaS) product. It can be used to provision a fully functional database without the admin overhead traditionally associated with DB platforms RDS supports a number of database engines - MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, Aurora RDS can be deployed in single AZ or Multi-AZ mode (for resilience) and supports General purpose and Memory Optimized instances RDS instances are managed by AWS, you cannot SSH into the VM instances Primary use case for RDS are Relational, Transactional databases. Best for relational datastore requirements (OLTP) By default, Customer are allowed to have up to 40 RDS databases Pricing - RDS instances are charged based on  Instance Size Provisioned storage (Not Used) IOPS (io1) Data Transfer Out Any backups/snapshots beyond the 100% total database storage for a region  RDS Supports encryption  Encryption can be configured when creating DB instances Encryption can be added by taking a snapshot, making an encrypted snapshot, and creating a new encrypted instance from that encrypted snapshot Once encrypted, encryption cannot be removed  Automated backups to S3 occur daily and can be retained from 0 to 35 days. (0 means disabled) Manual snapshot are taken manually and exist until deleted, and point-in-time log-based backups are also stored on S3  RDS Multi-AZ  RDS can be provisioned in Single or Multi-AZ mode. Multi-AZ makes an exact copy of your data in another AZ Multi-AZ provisions a primary instance and a standby instance in a different AZ of the same region With MultiAZ you do not have access to secondary AZ Multi-AZ has Automatic Failover protection if one AZ goes down, failover will occur and the standby will be promoted to primary  RDS Read Replica  RDS Read Replicas are read-only copies of an RDS instance that can be created in the same region or a different region from the primary instance Read Replicas can be addressed independently (each having their own DNS name) and used for read workloads, allowing you to scale reads 5 Read Replicas can be created from a RDS instance, allowing a 5x increase in reads Unlike Multi-AZs, read replicas can be used across regions. They also can be within the same AZ or even across AZs The primary instance needs to have automatic backups enabled to use Read Replicas With read replicas, AWS takes care of the networking aspects needed for asynchronous syncing between the primary and secondary regions Read replica use cases  When needing a faster recovery time than restoration from a snapshot When most of your DB usage is reading rather than writing, you can scale out your datbase instances for read-only purpose. (Horizontal Scaling) Have a Global Resilience   Amazon Aurora  Amazon Aurora is a fully-managed, MySQL-compatible, relational database engine that combines the speed and availability of high-end commercial databases that needs to scale, with Automatic backups, high availability, and fault tolerance Aurora MySQL is 5x faster over regular MySQL and 3x faster over regular Postgres Aurora is 1/10th the cost over its competitors with similar performance and availability options Aurora uses a base configuration of a cluster, which consists of one more db instances and a cluster volume that spans multiple AZs, with each AZ having a copy of the db cluster data A cluster contains a single primary instance and zero or more replicas Aurora cluster volume automatically scale as the amount of data in your database increases, up to max of 64 TB AWS Aurora only bills for the consumed data, and it\u0026rsquo;s constantly backed up to S3 Aurora replicates 6 copies of your database across 3 availability zones (2 Copies of data are kept in each AZ with minimum of 3 AZ) Aurora Serverless  Aurora Endpoints  Cluster endpoint - Connects to the primary db instance for the db cluster. This endpoint is the one that can perform write operation Reader endpoint - Connects to the one of the available Aurora replicas for the database cluster Custom endpoint - Represents a set of database instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instance in the group to handle the connection. Instance endpoint - Connects to a specific database instance within an Aurora cluster. The instance endpoint provides direct control over connections to the db cluster.  Aurora Serverless  On-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions) The database will automatically start up, shut down, and scale capacity up or down based on your application\u0026rsquo;s needs It\u0026rsquo;s a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads Enabled you to run database in cloud without managing any database instances Pricing is based on per-second for the database capacity you use when the database is active  DynamoDB  DynamoDB is a fully managed NoSQL key/value and document database that provides fast and precedable performance with seamless scalability It’s a regional service, partitioned regionally and allows the creation of tables, hence tables names in DynamoDB have to be regionally unique DynamoDB can be set to have Eventual Consistent Reads (default) and Strongly Consistent Reads Eventual consistent reads data is returned immediately but data can be inconsistent. Strongly Consistent Reads will wait until data is consistent DynamoDb stores 3 copies of data on SSD drive across 3 regions Key Components  A Table is a collection of items that share the same partition key (PK) or partition key and sort key(SK) together with other configuration and performance settings An Item is a collection of attributes (up to 400 KB in size) inside a table that shares the same key structure as every other items in the table Attribute is a fundamental data element which consists of key and value Primary Key is a unique identifier for the items in the table Partition Key and Sort Key is composed of two of more attributes Secondary Indexes allows you to query the data in the table using the alternate key   DynamoDB Indexes  Indexes provide an alternative representation of data in a table, which is useful for application with varying query demands Indexes come in two forms: Local Secondary Indexes (LSI) and Global Secondary Indexes (GSI) Indexes are interacted with as though they are table, but they are just an alternate representation of data in an existing table There are two types of Secondary Indexes  Global Secondary Index (GSI) - An index with a partition key and sort key that can be different from that on the table Local Secondary Index (LSI) - An index that has the same partition key as the table, but a different sort key  LSIs must be created at the time of creation of table. GSI can be created at any point after the table is created. You can define up to 20 GSI and 5 LSI per table  DynamoDB Streams  When enabled, streams provide an ordered list of changes that occur to items within a DynamoDB table A stream is a rolling 24-hour window of changes Streams are enabled per table and only contain data from the point of being enabled. Streams can be configured with one of four view types:  KEYS_ONLY - Whenever an item is added, updated or deleted, the key(s) of that item are added to the stream. NEW_IMAGE - The entire item is added to the stream “post-change” OLD_IMAGE - The entire item is added to the stream “pre-change” NEW_AND_OLD_IMAGES - Both the new and old versions of the items are added to the stream.  Streams can be integrated with AWS Lambda, invoking a function whenever items are changed in a DynamoDB table (a DB trigger)  DynamoDb Performance and Billing  DynamoDB has two read/write capacity modes: Provisioned throughput (default) and On-Demand mode When using On-Demand mode, DynamoDB automatically scales to handle performance demands and bills a per-request charge When using Provisioned throughput mode, each table is configured with read capacity units (RCU) and write capacity units (WCU)  Database Migration Service (DMS)  DMS is a service to migrate relational database, To and From from any location with network connectivity to AWS DMS is compatible with a broad range of DB Sources, including Oracle, MS SQL, MySQL, MariaDB, PostgreSQL, MongoDB, Aurora, and SAP Data can be synced to most of the above engines, as well as Redshift, S# and DynamoDB DMS is useful in a number of common scenarios:  Scaling database resources up and down without downtime Migrating databases from on-premises to AWS, from AWS to on-premises or to/from other cloud platforms Moving data between different DB engines, including schema conversion Partial / subset data migration Migration with little to no admin overhead, as a service   "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/05_authorization_access_control/",
	"title": "05 - Authentication, Authorization, and Admission Control",
	"tags": [],
	"description": "",
	"content": "  Authentication Authorization  Types of RoleBindings Admission Control  Demo - Authentication and Authorization  To access and manage any Kubernetes resource or object in the cluster, we need to access a specific API endpoint on the API server. Each access request goes through the following three stages:\n Authentication - Logs in a user Authorization - Authorizes the API requests added by the logged-in user. Admission Control - Software modules that can modify or reject the requests based on some additional checks, like a pre-set Quota.  Authentication Kubernetes does not have an object called user, nor does it store usernames or other related details in its object store. However, even without that, Kubernetes can use usernames for access control and request logging.\nKubernetes has two kinds of users:\n Normal Users They are managed outside of the Kubernetes cluster via independent services like User/Client Certificates, a file listing usernames/passwords, Google accounts, etc.\n Service Accounts With Service Account users, in-cluster processes communicate with the API server to perform different operations. Most of the Service Account users are created automatically via the API server, but they can also be created manually. The Service Account users are tied to a given Namespace and mount the respective credentials to communicate with the API server as Secrets.\n  For authentication, Kubernetes uses different authentication modules:\n Client Certificates To enable client certificate authentication, we need to reference a file containing one or more certificate authorities by passing the --client-ca-file=SOMEFILE option to the API server. The certificate authorities mentioned in the file would validate the client certificates presented to the API server. A demonstration video covering this topic is also available at the end of this chapter.\n Static Token File We can pass a file containing pre-defined bearer tokens with the --token-auth-file=SOMEFILE option to the API server. Currently, these tokens would last indefinitely, and they cannot be changed without restarting the API server.\n Bootstrap Tokens This feature is currently in beta status and is mostly used for bootstrapping a new Kubernetes cluster.\n Static Password File It is similar to Static Token File. We can pass a file containing basic authentication details with the --basic-auth-file=SOMEFILE option. These credentials would last indefinitely, and passwords cannot be changed without restarting the API server.\n Service Account Tokens This is an automatically enabled authenticator that uses signed bearer tokens to verify the requests. These tokens get attached to Pods using the ServiceAccount Admission Controller, which allows in-cluster processes to talk to the API server.\n OpenID Connect Tokens OpenID Connect helps us connect with OAuth2 providers, such as Azure Active Directory, Salesforce, Google, etc., to offload the authentication to external services.\n Webhook Token Authentication With Webhook-based authentication, verification of bearer tokens can be offloaded to a remote service.\n Authenticating Proxy If we want to program additional authentication logic, we can use an authenticating proxy.\n  Authorization After a successful authentication, users can send the API requests to perform different operations. Then, those API requests get authorized by Kubernetes using various authorization modules.\n Node Authorizer Attribute-Based Access Control (ABAC) Authorizer Webhook Authorizer Role-Based Access Control (RBAC) Authorizer  Role - With Role, we can grant access to resources within a specific Namespace.\nClusterRole - The ClusterRole can be used to grant the same permissions as Role does, but its scope is cluster-wide.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: lfs158 name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]  Types of RoleBindings RoleBinding\nIt allows us to bind users to the same namespace as a Role. We could also refer a ClusterRole in RoleBinding, which would grant permissions to Namespace resources defined in the ClusterRole within the RoleBinding’s Namespace.\nClusterRoleBinding\nIt allows us to grant access to resources at a cluster-level and to all Namespaces.\nTo enable the RBAC authorizer, we would need to start the API server with the \u0026ndash;authorization-mode=RBAC option\n Admission Control Admission control is used to specify granular access control policies, which include allowing privileged containers, checking on resource quota, etc. To use admission controls, we must start the Kubernetes API server with the --enable-admission-plugins, which takes a comma-delimited, ordered list of controller names:\n--enable-admission-plugins=NamespaceLifecycle,ResourceQuota,PodSecurityPolicy,DefaultStorageClass  Demo - Authentication and Authorization  Configuring User by assigning key and certificate Create Context with newly created User Add RBAC Role and rolebindings to namespace  minikube start kubectl config view # Create New Namespace for Demo kubectl create namespace lfs158 mkdir rbac \u0026amp;\u0026amp; cd rbac # Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool openssl genrsa -out student.key 2048 openssl req -new -key student.key -out student.csr -subj \u0026quot;/CN=student/O=learner\u0026quot; cat student.csr | base64 | tr -d '\\n' touch signing-request.yaml apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: \u0026lt;assign encoded value from cat command\u0026gt; usages: - digital signature - key encipherment - client auth kubectl create -f signing-request.yaml kubectl get csr kubectl certificate approve student-csr kubectl get csr # Generating User Certificate kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode \u0026gt; student.crt cat student.crt # Configuring Student User by assigning the key and certificate kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key # Create Student Context with Selected User kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student kubectl config view # Creating a Deployment with Nginx Image kubectl -n lfs158 create deployment nginx --image=nginx:alpine  From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context:\nkubectl --context=student-context get pods  Error from server (Forbidden): pods is forbidden: User \u0026ldquo;student\u0026rdquo; cannot list resource \u0026ldquo;pods\u0026rdquo; in API group \u0026ldquo;\u0026rdquo; in the namespace \u0026ldquo;lfs158\u0026rdquo;\n# Create RBAC Role to allow only get, watch, list actions in lfs158 namespace ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] ~/rbac$ kubectl create -f role.yaml ~/rbac$ kubectl -n lfs158 get roles NAME AGE pod-reader 57s  Create a YAML configuration file for a rolebinding object, which assigns the permissions of the pod-reader role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace:\n# Create RBAC Role Binding to the User ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME AGE pod-read-access 23s  Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context.\n~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-77595c695-f2xmd 1/1 Running 0 7m41s  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/05-networking/",
	"title": "05 - Networking",
	"tags": [],
	"description": "",
	"content": "  Networking Fundamentals  OSI Model IP Addressing Subnetting Firewall Proxy Servers  Virtual Private Cloud (VPC)  Virtual Private Cloud (VPC): VPC Routing Routes Bastion Hosts (Jump Boxes)  * Multifactor authentication, ID federation and/or IP blocks.  NAT Gateway Network ACLs VPC Peering VPC Endpoints  Global DNS (Route 53)  Hosted Zone Public Zones Private Zones Routing Policy Simple Routing Failover Routing Weighted Routing Policy Latency Based Routing GeoLocation based Routing   Networking Fundamentals OSI Model The Open Syste Interconnection (OSI) Model is a standard used by networking manufacturers globally. It was created and published in 1984; it splits all network communications into seven layers. Each layer servers the layer that\u0026rsquo;s above it plus the layer beneath it which adds additional capabilities. Data between two devices travels down the stack on the device\u0026rsquo;s A-side (wrapped at each layer) and gets transmitted before moving up the stack at the device B-side (where the wrapping gets stripped aways at every stage). The data wrapping process is called encapsulation.\n At Layer 1 (Physical), networks use a shared medium where devices can transmit signals and listen. Layer 1 showcases how data gets received and transmitted while taking into consideration the medium, voltages, and RF details.\n Layer 2 (Data Link) adds MAC addresses that can be used for named communication between two devices on a local network. Aditionally, layer 2 adds control over the media, avoiding cross-talk, this allows a back-off time and retransmission. L2 communications use L1 to broadcast and listen. L2 runs on top of L1.\n The Network Layer (L3) allows for unique device-to-device communcation over interconnected networks. L3 devices can pass packets over tens or even hundreds of L2 networks. The packets remain largely unchanged during this journey - travelling within different L2 frames as they pass over various networks.\n L4 (Transport) adds TCP and UDP. TCP is designated for reliable transport, and UDP is aimed at speed. TCP uses segments to ensure data is received in the consistent order and adds error checking and \u0026ldquo;ports\u0026rdquo; allowing different stream of communications to the same host.\n L5 (Session) adds the concept of sessions, so that request and reply communication streams are viewed as a single \u0026ldquo;session\u0026rdquo; of communication between client and server.\n L6 (Presentation) adds data converstion, encryption, compression,and standard that L7 can use. L7(Application) is where protocols (such as HTTP, SSH, and FTP) are added.\n  IP Addressing IPv4 addresses are how two devices can communicate at layer 4 and above of the OSI seven-layer model. IP address (IPs) are actually 32-bit binary values but are represented in dotted-decimal notation to make them easier for humans to read and understand.\nIPs are split into a network part and host part. The netmask (eg. 255.255.255.0) or prefix (e.g. /24) shows where this split occurs.\n   IP 10 0 0 0     Binary 10000000 00000000 00000000 00000000   Subnet Mask 255 255 255 0   Prefix /24 11111111 11111111 11111111 X    Subnetting Subnetting is a process of breaking a network down into smaller subnetworks. You might be allocated a public range for your business or decide on a private range for a VPC. Subnetting allows you to break it into smaller allocations for use in smaller network.\nIf you pick 10.0.0.0/16 for your VPC, it\u0026rsquo;s a single network from 10.0.0.0 to 10.0.255.255 and offers 65,536 addresses. With a certain size of VPC, increasing the prefix creates two smaller networks. Increasing agian creates four even smaller networks. Increasing again creates eight smaller and so on.\nFirewall A firewall is a device that historically sits at the border between different networks and monitors traffic flow between them. A firewall is capable of reading packet data and either allowing or denying traffic based on that data.\nFirewall establish a barrier between networks of different security levels and historically have been the first line of defense against perimeter attacks.\nProxy Servers A proxy server acts as a gateway between you and the internet. It\u0026rsquo;s an intermediary server separating end users from the websites they browse. Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy.\nA proxy server is a type of gateway that sits between a private and public network. Proxy servers can also choose to pass on traffic or not based on network layer appliances eg, username or element of corporate identity. It inspects outbound requests from an on-premise or private network client\nVirtual Private Cloud (VPC) Virtual Private Cloud (VPC):  A private network within AWS. It\u0026rsquo;s your private data center inside the AWS platform. Can be configured to be public/private or a mixture Regional (can\u0026rsquo;t span regions), highly available, and can be connected to your data center and corporate networks Isolated from other VPCs by default VPC and subnet: Max /16 (65,536 IPs) and minimum /28 (16 IPs) VPC subnet cannot span AZs (1:1 Mapping) Certain IPs are reserved in subnets  Regional Default VPC:\n Required for some services, used as a default for most Pre-configured with all required network/security Configured using /16 CIDR Block (172.31.0.0/16) A /20 Public subnet in each AZ, allocating a public P by default Attached internet gateway with a \u0026ldquo;main\u0026rdquo; route table sending all IPv4 traffic to the internet gateway using a 0.0.0.0/0 route A default DHCP option set attached SG: Default - all from itself, all outbound NACL: Default - allow all inbound and outbound  Custom VPC:\n Can be designed and configured in any valid way You need to allocate IP ranges, create subnets, and provision gateways and networking, as well as design and implement security When you need multiple tiers of a more complex set of networking Best practise is to not use default for most production things  VPC Routing  Every VPC has a virtual routing device called the VPC Router. It has an interface in any VPC subnet known as the \u0026ldquo;subnet+1\u0026rdquo; address for 10.0.1.0/24, this would be 10.0.1.1 / 32 The router is highly available, scalable, and controls data entring and leaving the VPC and its subnets. Each VPC has a \u0026ldquo;main\u0026rdquo; route table, which is allocated to all subnets in the VPC by default. A subnet must have one route table. Addditional \u0026ldquo;custom\u0026rdquo; route tables can be created and associated with subnets - but only one route table (RT) per subnet. A route table controls what the VPC router does with traffic leaving a subnet. An internet gateway is created and attached to a VPC (1:1). It can route traffic for public IPs to and from internet.  Routes  A Route Table is a collection of routes that are used when traffic from a subnet arrives at the VPC router. Every route table has a local route, which matches the CIDR of the VPC and lets traffic be routed between subnets. A route contains a destination and a target. Traffic is forwarded to the target if its destination matches the route destination. If multiple routes apply, the most specific is chosen. A/32 is choen before a /24, before a /16. Default routes (0.0.0.0/0 v4 and ::/0 v6) can be added that match any traffic not already matched. Targets can be IPs or AWS networking gateway objects. A subnet is a public subnet if it is (1) configured to allocate public IPs, (2) if the VPC has an associated internet gateway, and (3) if that subnet has a default route to that internet gateway.  An Internet gateway can be attached to only a single VPC; A VPC can have a single Internet Gateway. Internet Gateway is Highly Available by Design.\n Bastion Hosts (Jump Boxes)  A host that sits at the perimeter of a VPC It functions as an entry point to the VPC for trusted admins. Allows for update or configuration tweaks remotely while allowing the VPC to stay private and protected. Generally connected to via a SSH (Linux) or RDP (Windows) Bastion hosts must be kept updated, and security hardened and audited regularly. Multifactor authentication, ID federation and/or IP blocks. \u0026mdash;  NAT Gateway NAT (Network Address Translation) is a process where the source or destination attribute of an IP packets are changed. Static NAT is process of 1:1 translation where an internet gateway converts a private address to public IP Address. Dynamic NAT is a variation that allows many private IP to get outgoing internet access using smaller number of public IP (generally one). Dynamic NAT is provided within AWS using NAT gateway that allows private subnet in AWS VPC to acess the internet.\nNAT Gateway is used to enable instances ina private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances.\nNAT Gateway are not HA by Design. It is inside single Available Zone. It scales well with load.\n Network ACLs  Network ACLs operate at Layer 4 of the OSI Model (TCP/UDP and below) A subnet has to be associated with a NACL - either the VPC default or a custom NACL NACLs only impact traffic crossing the boundary of a subnet. (If communication occurs within a subnet, no NACLs are involved) NACLs are collection of rules that explicitly allow or deny traffic based on its protocol, port range, and source/destination. Rules are processed in number order, lowest first. When a match is found, that action is taken and processing stops. NACLs have two set of rules: inbound and outbound.  With NACL, you cannot reference logical AWS resources because it operates at Layer 4 or below of OSI Model. But with NACL, you can explicitly deny routes.\n VPC Peering  Allows direct communication between VPCs Services can communicate using private IPs from VPC to VPC VPC peers can span AWS accounts and even regions (with some limitations) Data is encrypted and transits via the AWS global backbone. VPC peers are used to link two VPCs at layer 3: company mergers, shared services, company and vendor, auditing.  Important Limits and Considerations:\n VPC CIDR blocks cannot overlap VPC peers connect two VPCs - routing is not transitive Routes are required at both sites (remote CIDR -\u0026gt; peer connection) NACLs and SGs can be used to control access SGs can be referenced but not cross-region IPv6 support is not available cross-region DNS reoslution to private IPs can be enabled, but it\u0026rsquo;s a setting needed at both sides.  Steps to Enable VPC Peering\n Create VPC Peering Object Add Route FROM VPC Subnet and TO VPC Peering Object. Update Security Group to Unblock the Inbound and Outbound Rules Ensure there aren\u0026rsquo;t any connection Blocking Rules in Network ACLs  VPC Pairing doesn\u0026rsquo;t support Transitive Routing\n VPC Endpoints VPC Endpoints are gateway objects created within a VPC. They can be used to connect to AWS public servers without the need for the VPC to have an attached internet gateway and be public.\nVPC Endpoint Types:\n Gateway endpoints: Can be used for DynamoDB and S3 Interface endpoints: Can be used for everything else (e.g. SNS, SQS)  When to Use a VPC Endpoint:\n If the entire VPC is private with no IGW If a specific instance has no public IP/NATGW and needs to access public services To access resources restricted to specific VPCs or endpoints (private S3 bucket)  Limitations and Considerations:\n Gateway endpoints are used via route table entries - they are gateway devices. Prefix lists for a service are used in the destintation field with the gateway as the target. Gateway endpoints can be restricted via policies Gateway endpoints are HA across AZs in a region Interface endpoints are interfaces in a specific subnet. For HA, you need to add multiple interfaces - one per AZ Interface endpoints are controlled via SGs on that interface. NACLs also impact traffic. Interface endpoints add or replace the DNS for the service - no route table updates are required. Code changes to use the endpoint DNS, or enable private DNS to override the default service DNS.  Egress-only internet gateways provide IPv6 instances with outgoing access to the public internet using IPv6 but prevent the instances from being accessed from the internet.\nNAT isn\u0026rsquo;t required with IPv6, and so NATGW\u0026rsquo;s are compatible with IPv6. Egress-only gateways provide the outgoing-only access of a NATGW but do so without adjusting any IP addresses.\n Global DNS (Route 53) Hosted Zone A zone or hosted zone is a container for DNS records relating to a particular domain (e.g, google.com). Route 53 supports public hosted zones, which influce the domain that is viable from the internet and VPCs. Private hosted zones are similar but accessible only from the VPCs they are associated with.\nPublic Zones  A public hosted zone is created when you register a domain with Route 53, when you transfer a domain into Route 53, or if you create one manually. A hosted zone will have \u0026ldquo;name servers\u0026rdquo; - these are the IP addressess you can give to a domain operator, so Route 53 becomes \u0026ldquo;authoritative\u0026rdquo; for a domain  Private Zones  Private Zone are created manually and associated with one or more VPCs Private zones need enableDnsHostnames and enableDnsSupport enabled on VPC  Routing Policy Simple Routing A simple routing policy is a single record within a hosted zone that contains one or more values. When queried a simple routing policy record returns all the values in a randomized order.\nPros: Simple, the default, even spread of requests\nCons: No performance control, no granual health checks, for alias type\nFailover Routing Failover routing allows you to create two records with same name. One is designed as the primary and another as secondary. Queries will resolve to the primary - unless it is unhealthy, in which the Route 53 will respond with the secondary.\n(Single Primary Record Type and Single Secondary Record Type)\nWeighted Routing Policy Weighted routing can be used to control the amount of traffic that reaches specific resources. It can be useful when testing new software or when resources are being added or removed from.\nLatency Based Routing Checks with Latency Database with latency based host in DNS\nGeoLocation based Routing Geolocation routing lets you choose the resource that server your traffic based on the geographic region from which queries originate.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/05_observability/",
	"title": "05 - Observability",
	"tags": [],
	"description": "",
	"content": "  Liveness and Readiness Probes  Liveness probe Readiness Probe  Container Logging Installing Metrics Server Monitoring Applications  Liveness and Readiness Probes Probes - Allow you to customize how Kubernetes determines the status of your containers\nLiveness probe Indicates whether the container is running properly, and governs whether the cluster will automatically stop or restart the container. Liveness probes can be created by including them in the container spec. This probe runs a command to test the container\u0026rsquo;s liveness.\napiVersion: v1 kind: Pod metadata: name: my-liveness-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026quot;] livenessProbe: exec: command: - echo - testing initialDelaySeconds: 5 periodSeconds: 5  Readiness Probe Indicates whether the container is ready to service requests, and governs whether request will be forwarded to the pod.\nLiveness and readiness probes can determine container status by doing things like running a command or making an http request\napiVersion: v1 kind: Pod metadata: name: my-readiness-pod spec: containers: - name: myapp-container image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5  Container Logging A container\u0026rsquo;s normal console output goes into the container log. You can access container logs using the kubectl logs command. For example, create a pod that generates some output every second:\napiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: [/bin/sh, -c, 'i=0; while true; do echo \u0026quot;$i: $(date)\u0026quot;; i=$((i+1)); sleep 1; done']  If a pod has more than one container, you must specify which container to get logs from using the -c flag.\nkubectl logs my-pod -c my-container  Installing Metrics Server The Kubernetes metrics server provides an API which allows you to access data about your pods, and nodes, such as CPU and memory usage.\ncd ~/ git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/  Verify\nkubectl get --raw /apis/metrics.k8s.io/  Monitoring Applications With a working metrics server, you can use the kubectl top to gather information about resource usage within the cluster.\nkubectl top pods kubectl top pod resource-consumer-big kubectl top pods -n kube-system kubectl top nodes  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/06-analytics/",
	"title": "06 - Analytics",
	"tags": [],
	"description": "",
	"content": " Amazon Athena  Amazon Athena is an interactive query service that utilizes Schema-On-Read, allowing you to run ad-hoc SQL like queries on data from a range of sources Athena is used to query large dataset (structured, semi-structured, and unstructured) stored in S3 with infrequent access pattern You are charged for compute time only. You don’t need to maintain separate dataset for Athena, it can directly access S3 bucket  Amazon EMR  EMR is tool for large-scale parallel processing of big data and other large data workloads It is based on the Apache Hadoop framework and is delivered as a managed cluster using EC2 instances It is used for huge-scale log analysis, indexing, machine learning, financial analysis, simulations, bio-informatics and many other large-scale applications EMR cluster have zero or more core nodes, which are managed by the master node. They run tasks and manage data for HDFS Data can be input from and output to S3. Intermediate data can be stored using HDFS in the cluster or EMRFS using S3.  Amazon Kinesis  Streaming service designed to ingest large amounts of data from hundreds, thousands or even millions of producers Scalable and Resilient Consumers can access a rolling window of that data, or it can be stored in persistent storage of database products  Kinesis Data Stream  A Kinesis data stream can be used to collect, process, and analyze a large amount of incoming data Storage for all incoming data within a 24 hour default window, which can be increased to seven days for an additional charge Kinesis Data records (The basis entity written to and read from Kinesis stream, a data record can be up to 1 MB in size) are added by producers and read by consumers  Kinesis Data Firehose  Reliably load streaming data into data lakes, data stores and analytics tools It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk It enables near real-time analytics with existing business intelligence tools and dashboards you’re already using today Kinesis Data Streams can be used as the source(s) to Kinesis Data Firehose Pay for only the data ingested   Kinesis Data Analytics  Process and analyze real-time, streaming data Can use standard SQL queries to process Kinesis data streams A Kinesis Data Analytics application consists of three components:  Input – the streaming source for your application Application code – a series of SQL statements that process input and produce output Output – one or more in-application streams to hold intermediate results   Kinesis Video Analytics  Securely ingests and stores video and audio encoded data to consumers such as SageMaker, Rekognition or other services to apply Machine Learning and Video processing \u0026mdash;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/07-databases/",
	"title": "06 - Databases",
	"tags": [],
	"description": "",
	"content": "  SQL - RDS  RDS Backups and Restore RDS Resilency RDS Multi-AZ RDS Read Replica  When would you use RDS Read Replica Deployment   SQL - Aurora  Aurora Serverless  Notes   NoSQL - DynamoDB  Query and Scan Operation Performance and Billing Streams and Triggers DynamoDB Indexes  In-Memory Caching  DynamoDB Accelerator (DAX) Use Case Elastic Cache   SQL - RDS RDS is a Database as a Service (DBaaS) product. It can be used to provision a fully functional database without the admin overhead traditionally associated with DB platforms. It can perform at scale, be made publicly accessible, and can be configured for demanding availability and durability scenarios.\nRDS supports a number of database engines:\n MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server Aurora: An in-house developed engine with substantial features and performance enhancements.  RDS can be deployed in single AZ or Multi-AZ mode (for resilience) and supports the following instance types:\n General purpose (DB.M4 and DB.M5) Memory optimized (DB.R4 and DB.R5, and DB.X1e and DB.X1 for Oracle)  Types of storage are supported:\n General Purpose SSD (gp2) Provisioned IOPS SSD  RDS instances are charged based on:\n Instance size Provisioned storage (Not Used) IOPS if using io1 Data transfer out Any backups/snapshots beyond the 100% that is free with each DB instance.  RDS supports encryption with the following limits/restrictions/conditions:\n Encryption can be configured when creating DB instances. Encryption can be added by taking a sanpshot, making an encrypted snapshot, and creating a new encrypted instance from that encrypted snapshot. Encryption cannot be removed.  Network access to an RDS instance is controlled by a security group (SG) associated with the RDS instance.\nSince RDS can only have one master instance at a time, increasing its resources will help with its writing capacity.\nRDS Backups and Restore RDS is capable of a number of different types of backups. Automated backsups to S3 occur daily and can be retained from 0 to 35 days. (0 means disabled). Manual snapshot are taken manually and exist until deleted, and point-in-time log-based backups are also stored on S3.\n When you restore a snapshot, it will create a brand new instance of RDS with new CNAME. When restoring from a backup, a new database will be created, meaning it will not override the existing database.  KMS is a regional service, so if the database is restored to another region, it will require a new master key.\n RDS Resilency RDS Multi-AZ  RDS can be provisioned in single or Multi-AZ mode. Multi-AZ provisions a primary instance and a standby instance in a different AZ of the same region. Only the primary can be accessed using the instance CNAME. There is no performance benefit, but it provides a better (Recovery Time Objective) RTO than restoring a snapshot.  With MultiAZ you donot have access to secondary AZ. For RDS only point of contact to the database is via CNAME. For the maintenance, changes happens first on the secondary and then later promoted to primary.\nRDS Read Replica RDS Read Replicas are read-only copies of an RDS instance that can be created in the same region or a different region from the primary instance. Read Replicas can be addressed independently (each having their own DNS name) and used for read workloads, allowing you to scale reads. 5 Read Replicas can be created from a RDS instance, allowing a 5x increase in reads.\n Unlike Multi-AZs, read replicas can be used across regions. They also can be within the same AZ or even across AZs. AWS takes care of the networking aspects needed for asynchronous syncing between the primary and secondary regions. Unlike Multi-AZ, Read Replicas perform asynchronous replication from the primary to the secondary region. The master instance needs to have backups enabled for read replication to occur.  When would you use RDS Read Replica Deployment  When needing a faster recovery time than restoration from a snapshot When most of your DB usage is reading rather than writing, you can scale out your datbase instances for read-only purpose. (Horizontal Scalling) Have a Global Resilence   SQL - Aurora Amazon Aurora is a fully-managed, MySQL-compatible, relational database engine that combines the speed and availability of high-end commercial databases with the simplicity and cost-effectiveness of open source database.\n Aurora uses a base configuration of a cluster - shared storage A cluster contains a single primary instance and zero of more replicas. All instances (primary and replicas) use the same shared storage - the cluster volumes. Aurora cluster volumes automatically scale as the amount of data in your database increases, up to a maximum of 64 tebibytes (TiB) AWS Aurora only bills for consumed data, and it\u0026rsquo;s constantly backed up to S3.  Aurora can have maximum of 15 replicas.\n  Automatic Backup Snapshot Process Backtrack - There is an outage, whenever we do a backtrack. Maximum window of 72 Hours. Parallel Queries - The parallel query option allows for optimal performance by allowing query execution across all nodes, at the same time, in a cluster.  Aurora Serverless Aurora Serverless is a fully managed on-demand, auto-scalling, high-availabile relational database that only charge when it\u0026rsquo;s in use. Aurora Serverless is based on the same database engine as Aurora, but instead of provisioning certain resouce allocation. Aurora Serverless handles this as a service. You simply specify a minimum and maximum number of Aurora capacity units (ACUs).\nAbstracts away the capacity planning for database. You simply pay based on the resource you use (per second). You start with specifying minimum and capacity unit with ACUs.\nCommon Use case for Aurora Serveless are -\n When an application uses a database and has random surges of traffic When you want to remove the complexity of managing database instances When you want automatically scaling database instances When deploying an application and have unpredictable database usage patterns Test environments when you need to Shutdown database when not in use  Notes  Aurora Serverless exists only in single Availability Zone. Aurora Serverless DB clusters can only be accessed from within a VPC. With Aurora Serverless, the database endpoint connects to a proxy fleet that routes the workload to a fleet of resources that are automatically scaled. Data API allows you to query database using traditional apis. Using Query Editor, a web-based tool you can log into the Aurora Serverless cluster and execute queries.  NoSQL - DynamoDB Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It is a NoSQL database service. It\u0026rsquo;s a regional service, partioned regionally and allows the creation of tables, hence tables names in DynamoDB have to be regionally unique. .\n A Table is a collection of items that share the same partition key (PK) or partition key and sort key(SK) together with other configuration and performance settings. An Item is a collection of attributes (upto 400 KB in size) inside a table that shares the same key structure as every other items in the table. An attribute is a key and value Dynamo DB doesn\u0026rsquo;t enforce strict schema for items in the collection Using Identity Policy, we can provide access to DynamoDB. Unlike S3, you cannot apply resource level policy. From resilency perspective, DynamoDB is resilent on regional basis. DynamoDB stores atleast 3 copies of data across different availability zone. DynamoDB handles networking and replication.  user_fav_movies\n{ \u0026quot;name\u0026quot;: \u0026quot;Bob Smith\u0026quot;, \u0026quot;createdAtTS\u0026quot;: 1590268741 \u0026quot;email\u0026quot;: \u0026quot;bob.smith@gmail.com\u0026quot;, \u0026quot;fav_movies\u0026quot;: [ \u0026quot;Titanic\u0026quot;, \u0026quot;50 First Dates\u0026quot; ] } { \u0026quot;name\u0026quot;: \u0026quot;John Doe\u0026quot;, \u0026quot;createdAtTS\u0026quot;: 1590268931 \u0026quot;email\u0026quot;: \u0026quot;john.doe@gmail.com\u0026quot;, \u0026quot;fav_movies\u0026quot;: [ \u0026quot;Beautiful Mind\u0026quot;, \u0026quot;Shutter Island\u0026quot; ] }  Query and Scan Operation  Scan - Scan operation has to read every items on the table and apply filter to filter out items that doesn\u0026rsquo;t match the filter. It is super flexible but it consumers lots of capacity units so it\u0026rsquo;s so efficent to perform scan query on large tables. Query - Query operation allows to lookup on the table without perform entire table scan, but you need to specify primary key and filter based on sort key. It is more efficent than Scan operation.\n DynamoDB maintains continuous backups of your table for the last 35 day\n Manual Backup\n Encryption comes default with DynamoDB\n Global Tables - Table needs to be empty and Enabled Streams Enable\n DynamoDB has full integration with CloudWatch\n  Performance and Billing DynamoDB has two read/write capacity modes: Provisioned throughput (default) and on-demand mode. When using on-demand mode, DynamoDB automatically scales to handle performance demands and bills a per-request charge. When using provisioned throughput mode, each table is configured with read capacity units (RCU) and write capacity units (WCU).\nDynamoDB is highly resilent and replicates data across multiple AZs in a region. When you receive a HTTP 200, a write has been completed and is durable. This doesn\u0026rsquo;t mean it\u0026rsquo;s been written to all AZs - this generally occurs within a second.\nAn eventually consistent read will request data, preferring speed. It\u0026rsquo;s possible the data received may not relect a recent write. Eventual consistency is the default for read operations in DDB.\nA strong consistency read ensures DynamoDB returns the most up-to-date copy of data - it takes longe rbut is sometimes required for application that require consistency.\nStreams and Triggers When enabled, streams provide an ordered list of changes that occur to items within a DynamoDB table. A stream is a rolling 24-hour window of changes. Streams are enabled per table and only contain data from the point of being enabled.\nEvery stream has an ARN that identifies it globally across all tables, accounts, and regions.\nStreams can be configured with one of four view types:\n KEYS_ONLY - Whenever an item is added, updated or deleted, the key(s) of that item are added to the stream. NEW_IMAGE - The entire item is added to the stream \u0026ldquo;post-change\u0026rdquo; OLD_IMAGE - The entire item is added to the stream \u0026ldquo;pre-change\u0026rdquo; NEW_AND_OLD_IMAGES - Both the new and old versions of the items are added to the stream.  Streams can be integrated with AWS Lambda, invoking a function whenever items are changed in a DynamoDB table (a DB trigger)\nDynamoDB Indexes Indexes provide an alternative representation of data in a table, which is useful for application with varying query demands. Indexes come in two forms: Local Secondary Indexes (LSI) and Global Secondary Indexes (GSI). Indexes are intracted with as though they are table, but they are just an alternate representaiton of data in an existing table.\nLocal secondary indexes must be created at the same time as creating a table. They use the same partition key but an altneative sort key. They share the RCU and WCU values for the main table. With using LSI efficient queries can be performed based on alternative sort key. You can create upto 5 LSI per table.\nGlobal secondary indexes can be created at any point after the table is created. They can use different partition and sort keys. They have their own RCU and WCU values. You can create upto 20 GSI per table.\nIndexes can have certain projected attributes.\nIn-Memory Caching DynamoDB Accelerator (DAX) DynamoDB Accelerator (DAX) is an in-memory cache designed specifically for DynamoDB. Results delivered from the DAX are available in microseconds rather than in the single-digit millisecond available from DynamoDB.\nDAX maintains two distinct caches: the item cache and the query cache. The item cache is populated with results from the GetItem and BatchGetItems and has a five minute default TTL. The query cache stores results of Query and Scan operations and caches based on the parameters specified.\nUse Case  Read Intensive Applications Applications that are latency sensitive (Real-time bidding, Online Stores)  Elastic Cache ElasticCache is a managed in-memory data store supporting the Redis or Memcached engines.\nElastiCache is used for two common use cases: * Offloading database reads by caching responses, improving application speed and reducing costs * Storing user sessions state, allowing for stateless compute instances (used for fault-tolerant architectures)\nGenerally, ElastiCache is used with key value databases or to store simple sessions data but it can be used with SQL datbase engines.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/06_pod-design/",
	"title": "06 - Pod Design",
	"tags": [],
	"description": "",
	"content": "  Labels, Selectors, and Annotations Deployments Rolling Updates and Rollbacks  Rolling Updates and Rollbacks  Jobs and CronJobs  Labels, Selectors, and Annotations Labels are key-value pairs attached to Kubernetes objects. They are used for identifying various attributes of objects which can in turn be used to select and group various subsets of those objects.\nWe can attach labels to objects by listing them in the metadata.labels section of an object descriptor.\napiVersion: v1 kind: Pod metadata: name: my-production-label-pod labels: app: my-app environment: production spec: containers: - name: nginx image: nginx  kubectl get pods --show-labels  Selectors are used for identifying and selecting a specific group of objects using their labels. One way to use selectors is to use them with kubectl get to retrieve a specific list of objects. We can specify a selector using the -l flag.\n# Equality Based Selector kubectl get pods -l app=my-app # InEquality Based Selector kubectl get pods -l env!=prod # Set Based Selector kubectl get pods -l 'env in (dev, qa)' # Chain Multiple Selectors kubectl get pods -l app=myapp, env=prod  Annotations are similar to labels in that they can be used to store custom metadata about objects.\nHowever, unline labels, annotations cannot be used to select or group objects in Kubernetes. External tools can read, write and interact with anootations.\n We can attach annotations to objects using the metadata.annotations section of the object descriptors:\napiVersion: v1 kind: Pod metadata: name: my-annotation-pod annotations: owner: bob@homeoffice.com git-commit: bd7833kjgll20xj3 spec: containers: - name: nginx image: nginx  Like labels, existing annotations can also be viewed using kubectl describe\nDeployments Deployments provide a way to declaratively manage a dynamic set of replica pods. They provide powerful functionality such as scalling and rolling updates.\nA deployment define a desired state for the replica pods. The cluster will constantly work to maintain the desired state, creating, removing, and modifying the replica pods accordingly.\nA deployment is a Kubernetes object that can be created using a descriptor:\nnginx-deployment.yml\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  Note the following: * spec.replicas - The number of replica pods * spec.template - A template pod descriptor which defines the pods which will be created * spec.selector - The deployment will manage all pods whose label match this selector\nkubectl get deployments kubectl get deployment \u0026lt;deployment_name\u0026gt; kubectl describe deployment \u0026lt;deployment_name\u0026gt; kubectl edit deployment \u0026lt;deployment_name\u0026gt; kubectl delete deployment \u0026lt;deployment_name\u0026gt;  Rolling Updates and Rollbacks Rolling updates provide a way to update a deployment to a new container version by gradually updating replicas so that there is no downtime.\nrolling-deployment-nginx.yml\napiVersion: apps/v1 kind: Deployment metadata: name: rolling-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.1 ports: - containerPort: 80  # Set New Image for the Deployment kubectl set image deployment/rolling-deployment nginx=nginx:1.7.9 --record # Perform Rollout kubectl rollout history deployment/rolling-deployment  The \u0026ndash;record flag records information about the update so that it can be rolled back later.\n Rolling Updates and Rollbacks Rollbacks allow us to revert to a previous state. For example, if a rolling update breaks something, we can quickly recover by using a rollback.\n# Get Rollout History for Deployment kubectl rollout history deployment/rolling-deployment  The \u0026ndash;revision flag will give more information on a specific revision number\n # Perform Rollback to previous revision kubectl rollout undo deployment/rolling-deployment kubectl rollout undo deployment/rolling-deployment --to-revision=1  You can also control how rolling update are performed by setting maxSurge and maxUnavailable in the deployment spec:\napiVersion: apps/v1 kind: Deployment metadata: name: rolling-deployment spec: strategy: rollingUpdate: maxSurge: 3 maxUnavailable: 2 replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.1 ports: - containerPort: 80  Jobs and CronJobs Jobs can be used to reliably execute a workload until it completes. The job will create one or more pods. When the job is finished, the container(s) will exit and pods(s) will enter the Completed state.\nbatch-job.yml\napiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\u0026quot;perl\u0026quot;, \u0026quot;-Mbignum=bpi\u0026quot;, \u0026quot;-wle\u0026quot;, \u0026quot;print bpi(2000)\u0026quot;] restartPolicy: Never backoffLimit: 4  hello-cron-job.yml\napiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \u0026quot;*/1 * * * *\u0026quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/06_services/",
	"title": "06 - Services",
	"tags": [],
	"description": "",
	"content": "  Services Service Object Example kube-proxy Service Discovery Servie Type  Cluster IP NodePort LoadBalancer ExternalIP ExternalName   Services  An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don’t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.\n Using the selectors app==frontend and app==db, we group Pods into two logical sets: one with 3 Pods, and one with a single Pod.\nWe assign a name to the logical grouping, referred to as a Service. In our example, we create two Services, frontend-svc, and db-svc, and they have the app==frontend and the app==db Selectors, respectively.\nServices can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets.\nService Object Example kind: Service apiVersion: v1 metadata: name: frontend-svc spec: selector: app: frontend ports: - protocol: TCP port: 80 targetPort: 5000  The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.\nWhile the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic.\n kube-proxy All worker nodes run a daemon called kube-proxy, which watches the API server on the master node for the addition and removal of Services and endpoints.\nService Discovery As Services are the primary mode of communication in Kubernetes, we need a way to discover them at runtime. Kubernetes supports two methods for discovering Services:\n Environment Variables DNS  Kubernetes has an add-on for DNS, which creates a DNS record for each Service and its format is my-svc.my-namespace.svc.cluster.local.\nServie Type While defining a Service, we can also choose its access scope. We can decide whether the Service:\n Is only accessible within the cluster Is accessible from within the cluster and the external world Maps to an entity which resides either inside or outside the cluster.  Access scope is decided by ServiceType, which can be configured when creating the Service.\n Cluster IP ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address is used for communicating with the Service and is accessible only within the cluster.\nNodePort The NodePort ServiceType is useful when we want to make our Services accessible from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. To access multiple applications from the external world, administrators can configure a reverse proxy - an ingress, and define rules that target Services within the cluster.\nkubectl expose pod \u0026lt;podname\u0026gt; --type=NodePort --port=80  LoadBalancer With the LoadBalancer ServiceType:\n NodePort and ClusterIP are automatically created, and the external load balancer will route to them The Service is exposed at a static port on each worker node The Service is exposed externally using the underlying cloud provider\u0026rsquo;s load balancer feature.  The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, and the Service will work the same way as a NodePort type Service.\n ExternalIP A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform or AWS.\nExternalName ExternalName is a special ServiceType, that has no Selectors and does not define any endpoints. When accessed within the cluster, it returns a CNAME record of an externally configured Service.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/07-application-integration/",
	"title": "07 - Application Integration",
	"tags": [],
	"description": "",
	"content": " Amazon Simple Notification Service (SNS)  SNS is a fully managed pub/sub messaging service Topics are logical access point and communication channel within SNS Topics can be encrypted via KMS Subscribers are endpoints that receive message for topic. When a topic received a message it automatically and immediately pushes messages to subscribers SNS supports notification over multiple protocols:  HTTP/HTTPS - Subscribers specify a URL as part of the subscription registration Email/Email Json - Messages are sent to registered address as email SQS - User can specify an SQS standard queue as the endpoint SMS - Messages are sent to registered phone number as SMS text messages   Amazon Simple Queue Service (SQS)  SQS provides fully managed, highly available message queuing service for inter-process /service /service messaging SQS is used for application integration, it lets decouple different systems SQS supports both Standard and FIFO Queues Standard queues are distributed and scalable to nearly unlimited message volume. The order is not guaranteed, best-effort only, and messages are guaranteed to be delivered at least once but sometimes more than once. FIFO queues ensure first-in,first-out delivery. Messages are delivered once only - duplicates do not occur. The throughput is limited to ~ 3,000 messages per second with batching or ~300 without by default. There are two types of polling  Short Polling - Available messages are returned immediately, even if the message queue being polled is empty Long Polling - Waits for message for a given WaitTimeSeconds (More Efficient)  Each SQS message can contain up to 256KB of data but can link data stored in S3 for any larger payloads Visibility time-out is the period of time that messages are invisible in the SQS queue Messages will be deleted from queue after a job has processed When a message is polled, it is hidden in the queue. It can be deleted when processing is completed - otherwise, after a VisibilityTimeout period, it will return to the queue The default Visibility time-out is 30 seconds. Timeout can be 0 seconds to a maximum of 12 hours. Retention period of SQS can be from 60 seconds to 14 days (Default is 4 Days) Message size is between 1 byte to 256 kB, Extended Client Library for Java can increase to 2 GB  AWS Step Functions  Step Functions are Serverless visual workflow service that provides state machines A state machine can orchestrate other AWS services with simple logic, branching, and parallel execution, and it maintains a state Workflow steps are known as states, and they can perform work via tasks State machines maintain state and allow longer running processes.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/07_deploying_standalone_app/",
	"title": "07 - Deploying Standalone App",
	"tags": [],
	"description": "",
	"content": "  Creating Deployment using YAML File Exposing Application Liveness and Readiness Probes  Liveness Probe Readiness Probe   Creating Deployment using YAML File webserver.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: webserver labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80  kubectl create -f webserver.yaml  This will also create ReplicaSet and Pods as defined in the YAML configuration.\nExposing Application webserver-svc.yaml\napiVersion: v1 kind: Service metadata: name: web-service labels: run: web-service spec: type: NodePort ports: - port: 80 protocol: TCP selector: app: nginx  kubectl create -f webserver-svc.yaml  Liveness and Readiness Probes Liveness Probe Liveness probe checks on an application\u0026rsquo;s health, and if the health check fails, kubelet restarts the affected container automatically.\nLiveness Probes can be set by defining:\n Liveness command  apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5   Liveness HTTP request  livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3   TCP Liveness Probe  livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20  Readiness Probe Readiness Probe can be used to ensure certain conditions are met before application can serve traffic. These conditions include ensuring that the depending service is ready, or acknowledging that a large dataset needs to be loaded, etc. In such cases, we use Readiness Probes and wait for a certain condition to occur.\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/08-hybrid-scaling/",
	"title": "07 - Hybrid and Scaling",
	"tags": [],
	"description": "",
	"content": "  Load Balancing and Auto Scalling  Elastic Load Balancing (ELB)  Launch Templates and Configurations Auto Scalling Groups  Scaling  VPN and Direct Connect  VPN VPC VPN Components Single Tunnel with Single Customer Gateway Full AWS Resilency with Two Tunnel Endpoint Full High Available VPN Connection Direct Connect (DX) VPN vs Direc Connect or Both VPN Direct Connect Both  Snowball and Snowmobile  Snowball Snowball Edge Snowmobile  Data and DB Migration  Storage Gateway Database Migration Service (DMS)  Identity Federation and SSO  Identity Federation (IDF) SAML 2.0 Federation Web Identity Federation When to Use Identity Federation Enterprise Access to AWS Resources Mobile and Web Application Centralized Identity Management (AWS Accounts)   Load Balancing and Auto Scalling Load balancing is a method used to distribute incoming connections across a group of servers or services. Incoming connections are made to the load balancer, which distributes them to associated services.\nElastic Load Balancing (ELB)  ELB is a service that provides a set of highly available and scalable load balancers in one of three versions: Classic (CLB), Application (ALB) and Network (NLB). ELBs can be paired with Auto Scaling groups to enhance high availability and fault tolerance - automating scaling / elasticity. An elastic load balancer has a DNS record, which allows access to the external side. An elastic load balancer can be public facing, meaning it accepts traffic from the public internet or internal, which is only accessible from inside a VPC and is often used betwen application tiers. An elastic load balancer accepts traffic via listerers using protocol and ports. It can stirp HTTPS at this point, meaning it handles encryption/decryption, reducing CPU usage on instances.  Classic Load Balancers\n CLB are the oldest type of load balancers and genrally should be avoided for new projects. Support L3 and L4 (TCP and SSL) and some HTTP/S features It isn\u0026rsquo;t L7 device, so no real HTTP/S One SSL certificate per CLB - can get expensive for complex projects Can offload SSL connections - HTTPS to the load balancer and HTTP to the instance (lower CPU and admin overhead on instances) Can be associated with Auto Scaling groups DNS A Record is used to connect to the CLB  Application Load Balancers\n ALB operates on L7 of the OSI model. They understand HTTP and HTTPS and can load balance based on this protocol layer. ALBs are now recommend as the default LB for VPCs. They perform better than CLBs and are most always cheaper. Content rules can direct certain traffic to specific target groups.  Host-based rules: Route traffic based on the host used Path-based rules: Route traffic based on URL path  ALBs support EC2, ECS, EKS, Lambda, HTTPS, HTTP/2 and WebSockets, and they can be integrated with AWS Web Application Firewall (WAF) Use an ALB if you need to use containers or microservices.  Network Load Balancers\nNetwork Load Balancers (NLB) are the newest type of load balancers and operate at Layer 4 of the OSI network model.\nBest load balancing in terms of performance within AWS\n Launch Templates and Configurations Launch templates and launch configuration allow you to configure various configuration attributes that can be used to launch EC2 instances. Typical configuration that can be set include:\n AMI to use for EC2 launch Instance type Storage Key pair IAM role User data Purchase options Network configuration Security group(s)  Launch template address some of the weaknesses of the legacy launch configurations and add the following features:\n Versioning and inheritance Tagging More advanced purchasing options  Launch templates should be used over launch configuration where possible. Neither can be edited after creation\n Auto Scalling Groups Auto Scaling groups use launch configuration or launch template and allow automatic scale-out or scale-in based on configurable metrics. Auto Scaling groups are often paired with elastic load balancers.\nMetrics such as CPU utilization or network transfer can be used either to scale out or scale in using scaling policies. Scaling can be manual, scheduled, or dynamic. Cooldowns can be defined to ensure rapid in/out events don\u0026rsquo;t occur.\nScaling Scaling can be Manual, Scheduled or Dynamic. Scaling policies can be simple, step scaling, or target tracking.\nCooldowns can be defined to ensure rapid in/out events don\u0026rsquo;t occur.\n VPN and Direct Connect VPN VPC Virtual Private Network (VPN) provide a software based secure connection between a VPC and on-premise networks.\nVPC VPN Components  A customer gateway (CGW) - configuration for on-premise router Virtual Private Gateway attached to VPC VPN connection (using one or two IPSec tunnels)  Single Tunnel with Single Customer Gateway  Simple  Full AWS Resilency with Two Tunnel Endpoint  Two Tunnels in two different AZs  Full High Available VPN Connection  Two VPN connections between two different Customer Gateways Resilency against Failure of Customer Gateway or VPN  Direct Connect (DX) A Direct Connect (DX) is a physical connection between your network and AWS either directly via a cross-connect and customer router at a DX location or DX partner.\nVirtual Interfaces (VIFs) run on top of a DX. Public VIFs can access AWS public services such as S3 only. Private VIFs are used to connect into VPCs. DX is not highly available or encrypted.\nVPN vs Direc Connect or Both VPN  Urgent need - can be deployed in minutes Cost constrained - cheap and economical Low end or consumer hardware - DX requires BGP Encryption Required Flexible to change locations Highly available options available  Direct Connect  Higher throughput Consistent performance (throughput) Consistent low latency Large amount of data - cheaper than VPN for higher volume No contention with existing internet connection  Both  VPN as a cheap HA option for DX VPN as an additional layer of HA If some form of connectivity is needed immediately, use VPN before DX connection is live Can be used to add encryption over the top of a DX  Snowball and Snowmobile Used when moving large amount of data quickly in and out of AWS\nWith any of the snow* devices, you don\u0026rsquo;t need to worry about writing code or the speed or data allocation of your internet, VPN or DX connection. With snow*, you log a job and receive an empty device or one full of the data requested. You can perform a data copy with your usual tooling and ship the device back.\nSnowball  Can be used for in and out jobs Ideal for TB or PB data transfers - 50 TB or 80TB capacity per Snowball 1 GBps or 10 GBps using a SFP Data encryption using KMS Generally used from 10 TB -\u0026gt; 10 PB End-to-end process time is low for the amount of data week(s)  Snowball Edge  Includes both storage and compute Larger capacity 10 Gbps Compute can be used for local instances or Lambda functionality Three versions:  Edge Storage Optimized: 80TB, 24vCPU, 32 GB RAM Edge Compute Optimized: 100TB, 52vCPU, 208 GB RAM Edge Compute Optimized with GPU  Compute can be used for local IoT for data processing prior to ingestion into AWS, and much more Used in the same type of situations as Snowballs but when compute is required  Snowmobile  Portable storage data center within a shipping container on a semi-truck Available in certain areas via special order from AWS Used when single location 10PB+ is required Each Snowmobile can transfer up to 100 PB Not economical for sub 10 PB and where multiple locations are required Situated on side and connected into your data center for the duration of the transfer  Data and DB Migration Storage Gateway Storage Gateway is a hybrid storage service that allows you to migrate data into AWS, extending your on-premises storage capacity using AWS. There are three main types of Storage Gateway: file gateway, volume gateway and tape gateway.\nA file gateway supports a file interface into AWS S3 and combines a server and a virtual software appliance. Using File gateway, you can store and retreive objects in Amazon S3 using NFS and SMB.\nVolume Gateway proives cloud-backed stoage volumes that you can mount as Internet Small Computer System Interface (iSCSI). The volume gateway is deployed into your on-premises environment as a VM running on VMWare ESXI, KVM or Microsoft Hyper-V hypervisor.\nA tape gateway provides cloud-backed virtual tape storage. The tape gateway is deployed into your on-premises environment as a VM running on VMware ESXi, KVM or Microsoft Hyper-V hypervisor.\nDatabase Migration Service (DMS) Database Migration Service (DMS) is a service to migrate relational database. It can migrate to and from any location with network connectivity to AWS.\n DMS is compatible with a broad range of DB Sources, including Oracle, MS SQL, MySQL, MariaDB, PostgreSQL, MongoDB, Aurora, and SAP. Data can be synced to most of the above engines, as well as Redshift, S# and DynamoDB.  With DMS at high level, you provision a replication instance, define source and destination endpoints that point at source and target databases, and create a replication task. DMS handles the rest, and you can continue using your database while the process runs. DMS is userful in a number of common scenarios:\n Scaling database resources up and down without downtime Migrating databases from on-premises to AWS, from AWS to on-premises or to/from other cloud platforms. Moving data between different DB engines, including schema conversion. Partial / subset data migration Migration with little to no admin overhead, as a service  Identity Federation and SSO Identity Federation (IDF) Identity Federation (IDF) is an architecture where identities of an external identity provider (IDP) are recognized. Single-sign-on (SSO) is where the credential of an external identity are used to allow access to a local system (e.g. AWS)\nTypes of IDF inlude:\n Cross-account roles - A remote account (IDP) is allowed to assume a role and access your account\u0026rsquo;s resources. SAML 2.0 IDF - An on-premises or AWS-hosted directory service instance is configured to allow Active Directory users to log in to the AWS console. Web Identity Federation - Identity Providers such as Google, Amazon, and Facebook are allowed to assume roles and access resources in your account.  Cognito and the Secure Token Service (STS) are used for IDF. A federated identity is verified using an external IDP and by providing the identity (using a token or assertions of some kind) is allowed to swap that ID for temporary AWS credentitals by assuming a role.\nSAML 2.0 Federation Web Identity Federation  Users starts with getting redirected to external Identity Provider such as Google, Facebook. For valid login, you get a token back You use the Token and perform a Token Exchange with Amazon Cognito to get a temporary credentials from STS You use the Temporary credentials to perform action on the AWS resource.  When to Use Identity Federation Enterprise Access to AWS Resources  Users / staff have an existing pool of identities. You need those identities to be used across all enterprise systems, including AWS Access to AWS resources using SSO Potentially tens of hundreds of thousands of users - more than IAM can handle You might have an ID team within your business  Mobile and Web Application  Mobile or web application requires access to AWS resources You need a certain level of guest access You don\u0026rsquo;t want credentials stored within the application Could be millions or more users - beyond the capabilities of IAM Customers might have multiple third-party logins, but they represent one real person.  Centralized Identity Management (AWS Accounts)  Tens of hundreds of AWS accounts in an organization. Need central store of IDs - either IAM or an existing provider. Role switching used from an ID account into member accounts.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/06-storage_and_content_delivery/",
	"title": "07 - Storage and Content Delivery",
	"tags": [],
	"description": "",
	"content": "  S3 Architecture and Features  Permissions Transferring Data to S3 Single PUT Upload (Default) Multi Part Upload Encryption Static Websites and CORS Object Versioning Presigned URL  S3 Performance and Resilience  Storage Tiers / Classes Standard Standard Infrequent Access (Standard - IA) One Zone - IA Glacier Glacier Deep Archive Intellgent Tiering Lifecycle Rules S3 Cross Region Replication  Cloud Front  CloudFront Components Origin Access Identity (OAI)  Network File System  Amazon EFS Throughput modes Performance Mode   S3 Architecture and Features Permissions Bucket authorization within S3 is controlled using Identity Policies on AWS identities, as well as Bucket policies in the form of resource policies on the bucket and bucket or object ACLs.\nIdentity Policies can be applied to roles, groups or users. Identity policies can be used to give permission within the same account.\nBucket Policies\nBucket policies are resource policies that are applied to S3. They can be used to authorize access to a bucket or objects inside a bucket to large numbers of identities. Bucket policies can also apply to anonymous accesses. Bucket policies grant permissions to IAM identities in a different AWS account, known as cross-account access. These permissions can be set instead of using IAM roles.\nACLs are legacy manner to control access to S3 buckets.\nBlock public access is a setting applied on top of any existing settings as a protection. It can disallow all public access granted to a bucket and objects using ACLs or bucket policies.\n Transferring Data to S3 Uploads to S3 are generally done using the S3 console, CLI or directly using the APIs\nSingle PUT Upload (Default)  Limit of 5 GB of Data, can cause performance issues, and if the upload fails the whole upload fails. Object is uploaded in a single stream of data  Multi Part Upload  An object is broken up into parts, each part is 5MB to 5GB Multipart upload is faster, and the individual parts can fail and can be retried individually. In a S3 multipart upload, an object can be broken up into 10,000 parts.  AWS recommends multipart for anything over 100 MB, but it\u0026rsquo;s required for anything beyond 5 GB.\n S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.\nEncryption Data between a client and S3 is encrypted in transit. Encryption at rest can be configured on a per-object basis.\n Client-Side Encryption - The client/application is responsible for managing both the encryption/decryption process and its keys. This method is genrally only used when strict security complaince is required. It has significant admin and processing overhead.\n Server-Side Encryption with Customer Managed Keys (SSE-C) - S3 handles the encryption and decryption process. The customer is still responsible for key management, and key must be supplied with each PUT or GET request.\n Server-Side Encryption with S3-Managed Keys (SSE-S3) - Objects are encrypted using AES-256 by S3. The keys are generated by S3. Keys are stored with objects in an encrypted form.\n Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS) - Objects are encypted using individual keys generated by KMS. Encrypted keys are stored with the encrypted objects. Decryption of an object needs both S3 and KMS key permission (role seperation).\n  Object Enryption is done on per-object basis. You can specify default encryption type.\n Static Websites and CORS Amazon S3 buckets can be configured to host websites. Content can be uploaded to the bucket and when enabled, static web hosting will provide a unique endpoint URL that can be accessed by any web browsers. S3 can host many types of content, including:\n HTML, CSS, JavaScript Media(Audio, Movies, Images)  S3 can be used to host front-end code for serverless application or an offload location for static content. CloudFront can also be added to improve the speed and efficiency of content delivery for global users or to add SSL for custom domains.\nCORS is a security measure allowing a web application running in one domain to reference resources in another.\nObject Versioning By Default Object Versioning is turned off. Once enabled, any operation that would otherwise modify objects generate new versions of that original object. Once a bucket is version-enabled, it can never be fully switched off - only suspended.\nWith versioning enabled, an AWS account is billed for all versions of all objects. Object deletions by default don\u0026rsquo;t delete an object - instead, a delete marker is added to indicate the object is deleted.\nMFA Delete is a feature designed to prevent accidental deletion of object. Once enabled, a one-time password is required to delete an object version or when changing the version state of a bucket.\nPresigned URL A presigned URL is a temporary URL that allows users to see assigned S3 objects using the creator\u0026rsquo;s credentials. A presigned URL can be created by an identity in AWS, providing access to an object using the creator\u0026rsquo;s access permission. When the presigned URL is used, AWS verifies the creator\u0026rsquo;s access to the object. The URL is encoded with authentication built in and has an expirty time.\nPresigned URLs can be used to download or upload objects.\nFor generating pre-signed URLs, do not generate presigned URLs using Roles and use identities with long term credentials.\n S3 Performance and Resilience Storage Tiers / Classes All objects within a S3 bucket use a storate class, also known as storage tier. Storage classes influence that cost, durability, availability and \u0026ldquo;first byte latency\u0026rdquo; for objects in S3. The class used for an object can be changed manually or using lifecycle policies.\nStandard  Default, all purpose storage or when usage is unknown 99.999999999999 (11 nines) durability and four nines availibility Replicated in +AZs - no minimum object size or retreival fee  Standard Infrequent Access (Standard - IA)  Objects where real-time access is required but infrequent 99.9% Availability, 3+ AZ replication, cheaper than Standard Minimum storage duration of 30-day and 128 KB minimum charge and object retrieval fee  One Zone - IA  Non-critical and/or reproducible objects (Non Mission Critical) 99.5% availability, only 1 AZ, 30 days and 128KB minimum charges Cheaper than Standard and Standard IA  Glacier  Long-term archival storage (warm and cold backups) Retrievals could take minutes or hours (faster = higher cost) 3+ AZ replication, 90 day and 40KB minimum charge and retrieval  Glacier Deep Archive  Long-term archival(cold backups) - 180 days and 40KB minimums Longer retrievals but cheaper than Glacier - replacement for tape-style storage  Intellgent Tiering Intellegent-Tiering is a special type of storage class designed for unknown or unpredictable access patterns. It moves objects automatically between two tiers - one designed for frequent access, the others for infrequent.\nLifecycle Rules Storage classes can be controlled via Lifecycle Rules, which allow for the automated transition of object between storage classess, or in certain cases allow for the expiration of objects that are no longer required.\nS3 lifecycle rules can apply to buckets, prefixes, and tags for buckets or objects, along with current or previous versions of an object.\n S3 Cross Region Replication S3 cross-region replication (S3 CRR) is a feature that can be enabled on S3 buckets allowing one-way replication of data from a source bucket to a destination bucket in another region.\nBy default, replicated objects keep their:\n Storage Class Object name(key) Owner Object permission  S3 CRR requires versioning enabled on both end. Replication requires an IAM role with permission to replicate objects, With the replication configuration, it is possible to override the storage class and object permission as they are written to the destination.\nObjects added before the replication configuration added won\u0026rsquo;t be added as part of the replication process.\n  Object ownership and its associated storage tiers can be altered when going to a new region. Lifecycle rules do not carry over for S3 CRR SSE-C is not supported for CRR  Cloud Front  Content Delivery Network (CDN) Benefits  Lower Latency Higher Transfer Speed Reduced Load on Content Server   CloudFront Components  Origin - The Server or service that hosts your content Distribution - The \u0026ldquo;configuration\u0026rdquo; entity with CloudFront Edge Location - The local infrastructure that hosts caches of your data. Regional Edge Caches - Large versions of edge locations.  Origin Access Identity (OAI)  OAI is a virtual identity associated with Cloud Front Distribution. It only works for S3 Buckets. Restrict S3 Bucket access only via Cloud Front Why ?  Performance User Experience   Network File System Amazon EFS Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services, and on-premises resource. File system can be created and mounted on multiple Linux instances at the same time. It is a popular shared storage system that can be natively mounted as a file system within Linux instances.\nAmazon EFS offers two storage classes: The Standard storage class, Infrequent Access Storage Class.\nThroughput modes  Bursting Throughput - (Default) Provisioned Throughput - Throoughput Mode  Performance Mode  General Purpose (Default) Max I/O (Designed for large number of instances)\n# Install Amazon EFS Utils sudo yum install -y amazon-efs-utils sudo mkdir /mnt/efs sudo mount -t efs fs-d358ff50:/ /mnt/efs   Designed for large scale parallel access of data. Shared media, Logging solutions where various clients need to access Shared Data, Amazon EFS is a good use case. BigData Analytics. Amazon EFS is Region Resilient. Security groups are used to control access to NFS mount targets.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/08-hybrid-scaling/",
	"title": "08 - Hybrid and Scaling",
	"tags": [],
	"description": "",
	"content": " Snowball and Snowball Edge  Snowball and Snowball Edge is a rugged container which contains a storage device It is used to move large amount of data quickly in and out of AWS. You can both export or import data using Snowball or Snowmobile Snowball and Snowball Edge is peta-scale migration; Snowmobile is for exabyte-scale migration Snowball comes in two sizes:  50 TB (42 TB of usable space) 80 TB (72 TB of usable space)  Snowball Edge comes in two sizes:  100 TB (83 TB of usable space) 100 TB Clustered (45 TB per node)  Snowball Edge includes both Storage and edge-computing workloads Snowball Edge provides three options  Edge Storage Optimized (24 vCPU) Edge Compute Optimized (52 vCPU)) Edge Compute Optimized with GPU  Snowball and Snowball Edge are idea for data transfer from 10 TB to 10 PB of data transfer  Snowmobile  It is 45 foot long ruggedize shipping container, pulled by a semi-trailer truck Snowmobile comes in one size: 100 PB Available in certain areas via special order from AWS Ideal for greater than 10PB Data Transfer for a single location Situated on side and connected into your data center for the duration of the transfer  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/08_kubernetes_volume_management/",
	"title": "08 - Kubernetes Volume Management",
	"tags": [],
	"description": "",
	"content": "  Volumes Volume Types PersistentVolumeClaims Using a Shared hostPath Volume Type  Volumes As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the kubelet will restart it with a clean slate, which means that it will not have any of the old data.\nTo overcome this problem, Kubernetes uses Volumes. A Volume is essentially a directory backed by a storage medium. The storage medium, content and access mode are determined by the Volume Type. In Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\nIn Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\n Volume Types  emptyDir - An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume\u0026rsquo;s life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever.\n hostPath - With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk - With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore - With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk - With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile - With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs - With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs - With nfs, we can mount an NFS share into a Pod. iscsi - With iscsi, we can mount an iSCSI share into a Pod. secret - With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. We will take a look at an example in a later chapter. configMap - With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim - We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. We will cover this in our next section.  PersistentVolumeClaims A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PersistentVolume resources based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.\nUsing a Shared hostPath Volume Type apiVersion: v1 kind: Pod metadata: name: share-pod labels: app: share-pod spec: volumes: - name: \u0026quot;host-volume\u0026quot; hostPath: path: \u0026quot;/home/docker/pod-volume\u0026quot; containers: - image: nginx name: nginx volumeMounts: - mountPath: \u0026quot;/usr/share/nginx/html\u0026quot; name: \u0026quot;host-volume\u0026quot; ports: - containerPort: 80 - image: debian name: debian volumeMounts: - mountPath: /host-vol name: \u0026quot;host-volume\u0026quot; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Hello Kubernetes \u0026gt; /host-vol/index.html; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/09-applicationanalyticsoperations/",
	"title": "09 - Application, Analytics, and Operations",
	"tags": [],
	"description": "",
	"content": "  Application Integration  Simple Notification Service (SNS) SNS Components  Topic Subscribers Publisher  Simple Queue Service (SQS) Polling Types Elastic Transcoder  Analytics  Athena Elastic MapReduce (EMR) Kinesis and Firehose Kinesis Stream Kinesis Shard Kinesis Data Record Kinesis Firehose Redshift  Logging and Monitoring  CloudWatch CloudWatch Metrics and Alarms CloudWatch Logs CloudTrail VPC Flow Logs  Operations  CloudWatch Events KMS  Deployment  Elastic Beanstalk Deployment Options OpsWorks OpsWorks Components   Application Integration Simple Notification Service (SNS)  SNS is a Publisher / Subscriber based fully managed Regional Service. It is a Public Service. SNS coordinates and manages the sending and delivery of messages. Messages sent to a topic are delivered to subscribers SNS is integrated with many AWS serviecs and can be used for certain event notifications (e.g. CloudFormation, Stack creation etc) Using SNS, CloudWatch can notifiy admins of important alerts SNS can be used for mobile push notifications  SNS Components Topic  An isolated configuration for SNS, including permissions Messages (\u0026lt;= 256 KB) are sent to a topic Subscribers to that topic receive messages  Subscribers  Endpoint that receive message for topic  HTTP(S) Email SQS (Message can be added to one or more queues) Mobile Push Notification Lambda Functions SMS   Publisher  An entity that publishes/sends message to queues  Application AWS services, including S3 (S3 events), CloudWatch, CloudFormation etc.   Simple Queue Service (SQS)  Simple Queue Service (SQS) provides fully managed, highly available message queues for inter-process /service /service messaging. SQS is used mainly to create decoupled architectures. Message are added to a queue and retreived via polling.  Polling Types  Short Polling: Available messages are returned ASAP - a short poll might return 0 messages. Causes increased number of API calls. Long polling - Waits for message for a given WaitTimeSeconds (More Efficient)\n Each SQS message can contain upto 256KB of data but can link data stored in S3 for any larger payloads.\n When a message is polled, it is hidden in the queue. It can be deleted when processing is completed - otherwise, after a VisibilityTimeout period, it will return to the queue.\n Lambda functions can be invoked based on message on a queue offering better scalling and faster response than Auto Scalling groups for any messages that can be processed quickly.\n  There are two types of queues: Standard Queues and FIFO queues.\n Standard queues are distributed and scalable to nearly unlimited message volume. The order is not guarantted, best-effort only, and messages are gurantted to be delivered at least once but sometimes more than once. FIFO queues ensure first-in,first-out delivery. Messages are delivered once only - duplicates do not occur. The throughput is limited to ~ 3,000 messages per second with batching or ~300 without by default.\n## Get Queue Attributes aws sqs get-queue-attributes --queue-url https://URL --attribute-names All ## Send Message aws sqs send-message --queue-url https://URL --message-body \u0026quot;INSERTMESSAGE\u0026quot; ## Receive Message aws sqs receive-message --queue-url https://URL ## Delete Message aws sqs delete-message --queue-url https://URL --receipt-handle \u0026quot;INSERTHANDLE\u0026quot; ## Receive Message (Long Poll) aws sqs receive-message --wait-time-seconds 10 --max-number-of-messages 10 --queue-url https://URL aws sqs --region us-east-1 receive-message --wait-time-seconds 10 --max-number-of-messages 10 --queue-url https://URL   Elastic Transcoder Elastic Transcoder is an AWS service that allows you to convert media file from an input format to one or more output formats. It\u0026rsquo;s delivered as a service, and you are billed a per-minute charge while using the service.\nA pipeline is a queue for jobs. It stores source and destination settings, notification, security and other high settings. Jobs are processed in the order they are added as resources allow. Pipelines can send notifications as jobs progress through various sates. These might notify an administrator or initiate further even-driven processing.\nAnalytics Athena Amazon Athena is an interactive query servie that utilizes Schema-On-Read, allowing you to run ad-hoc SQL like queries on data from a range of sources. Results are returned in seconds, and you are billed only for the compute time used and any existing storage costs.\nAthena can query many forms of structured, semi-structured, and unstructured data in S3.\nAthena is used to query large dataset stored in S3 with in-frequent access pattern. You are charged for compute time only. You don\u0026rsquo;t need to maintain separate dataset for Athena, it can directly access S3 bucket.\n Elastic MapReduce (EMR) Amazon Elastic MapReduce (EMR) is a tool for large-scale parallel processing of big data and other large data workloads. It\u0026rsquo;s based on the Apache Hadoop framework and is delivered as a managed cluster using EC2 instances. EMR is used for huge-scale log analysis, indexing, machine learning, financial analysis, simulations, bioinformatics and many other large-scale applications.\nEMR cluster have zero or more core nodes, which are managed by the master node. They run tasks and manage data for HDFS.\nData can be input from and output to S3. Intermediate data can be stored using HDFS in the cluster or EMRFS using S3.\nKinesis and Firehose Kinesis is a scalable and resilent streaming service from AWS. It is designed to ingest large amounts of data from hundreds, thousands, or even millions of producers. Consumers can access a rolling window of that data, or it can be stored in persistent storage of database products.\nKinesis Stream A Kinesis stream can be used to collect, process, and analyze a large amount of incoming data. A stream is a public service accessible from inside VPCs or from the public internet by an unlimited number of producers.\nKinesis streams include storage for all incoming data within a 24 hour default window, which can be increased to seven days for an additional charge. Data records are added by producers and read by consumers.\nKinesis Shard Kinesis shards are added to streams to allow them to scale. A stream starts with at least one shard, which allows 1 MB of ingestion and 2MB of consumption. Shards can be added or removed from streams.\nKinesis Data Record The basis entity written to and read from Kinesis stream, a data record can be upto 1 MB in size.\nYou would use Kinesis rather than SQS when you need many producers and many consumers as well as a rolling window of data. SQS is a queue. Kinesis allows lots of independent consumers reading the same data window.\n SQS is designed as a Queing Solution to decouple systems whereas Kinesis is designed as more of Publish Subscribe model where message from a producer is consumed by multiple consumers.\nKinesis Firehose Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today.\nRedshift Redshift is a petabyte-scale data warehousing solution. It\u0026rsquo;s a column-based database designed for analytical workloads. Generally, a relational store like RDS would be used for OLTP workloads (e.g queries, inserts, updates, and deletes), and Redshift would be used for OLAP (e.g. retrieval and analytics). Multiple databases become source data to be injected into a data warehouse solutions such as Redshift.\nData can be loaded from S3 and unloaded to S3. Additionally, backups can be performed to S3, and various AWS services such as Kinesis can inject data into Redshift.\nLogging and Monitoring CloudWatch CloudWatch is a service that provides near real-time monitoring of AWS products. In essence, it\u0026rsquo;s a metrics repository. You can import customer metric data in real-time from some AWS services and on-premises platforms.\nData retention is based on granuality: * One-hour metrics are retained for 455 days * Five-minute metrics for 63 days * One-minute metrics for 15 days\nMetrics can be configured with Alarms that can take actions, and data can be presented as a dashboard.\nCloudWatch Metrics and Alarms A CloudWatch metric is a set of data points over time. An example is CPU utilization of an EC2 instance. Alarm can be created on metrics, thaking an action if the alarm is triggered.\nAlarms have a number of key components: * Metric - The data point over time being measured * Threshold - Threshold value to trigger an alarm * Period - Time duration for the metric to beyond the Threshold * Action - What to do when an alarm is triggered.\nYou can configure Dynamic Alarm so that it can also use Anomalyn detection and trigger an Alarm\n CloudWatch Logs CloudWatch Logs provides functionality to store, monitor, and access logs from EC2, on-premises servers, Lambda, CloudTrail, Route 53, VPC Flow Logs, customer applications, and much more. Metric filters can be used to analyze logs and create metrics.\n Log Groups - Container for Log Streams. Log Streams - A Log Stream is a sequence of log events within the same source. Log Events - A Log Event is a timestamp and a raw message.  You can set expiration date for the Log Groups\n Cloud Watch Log Metric Filer is creaetd on a Log Group with a pattern match and Alarm can also be created\n CloudTrail CloudTrail is a governance, compliance, risk management and auditing service that records account activity within an AWS account. Any actions taken by users, roles, or AWS services are recorded to the service. Activity is recorded as a CloudTrail event, and by default you can view 90 days via event history. Trails can be created, giving more control over logging and allowing events to be stored in S3 and CloudWatch Logs.\nEvents can be management events that log control plane events (e.g, user login, configuring security, and adjusting security groups) or data events (eg, Object-level events in S3 or funciton-level events in Lambda).\nCloud Trail service is not real-time and it is delivered in batches.\nVPC Flow Logs VPC Flow Logs allows you to capture metadaa about the traffic flowing in and ou of networking inerfaces within a VPC. Flow logs can be placed on a specific network interface, a subnet or an entire VPC and will capture metadata from the capture point and anything within it. Flow logs aren\u0026rsquo;t real-time and don\u0026rsquo;t capture the actutal traffic - only metadata on the traffic.\nVPC Flow Logs is not real ttime and it only capttures only certain metadata information. Flow Logs don\u0026rsquo;t capture all traffic logs.\nOperations CloudWatch Events CloudWatch Eventts has a near real-time visibility of changes that happen within an AWS account. Using rules, you can match against certain events within an account and delivery those events to a number of supported targets.\nWithin rules, many AWS services are natively supported as event sources and delivery the events directly. For others, CloudWatch allows event pattern matching against CloudTrail events. Additional rules support scheduled events as sources, allowing cron-style function for periodically passing events to targets.\nSome examples of event targets include: * EC2 instances * Lambda functions * Step Functions state machines * SNS topics * SQS Queues\nYou can use CloudWatch Events (CRON Style Rule) to stop EC2 Functions.\n KMS AWS Key Management Service (KMS) provides regional, secure key management and encryption and decryption services. KMS is FIPS 140-2 level 2 validated and certain aspects support level 3. Everything in KMS is regional, KMS can use CloudHSM via Custom Key Stores.\nKMS manages customer master keys (CMK), which are created in a region and never leave the region or KMS. They can encrypt or decrypt data upto 4KM. CMK\u0026rsquo;s have key policies and can be used to create other keys. There are three types of CMK\n AWS Managed CMK: Used by default if you pick encryption within most AWS services and formatted as aws/service-name. Only the service they belong to can use them directly.\n Customer Managed CMK: Certain services allow you to pick a CMK you manage. Customer managed CMKs allow key rotation configuration, and they can be controlled via key policies and enabled/disabled.\n AWS Owned CMK: Keys used by AWS on a shared basis across many accounts - you normally don\u0026rsquo;t see these.\n KMS can encrypt upto 4KB with a CMK - you supply the data and specify the key to use.\n It can decrypt data up to 4KB - you provide the ciphertext, and it returns the plaintext\n You can also re-encrypt up to 4KB - you supply the ciphertext, the new key to use, and you are returned new ciphertext.\n# Create Customer Managed Key aws kms create-key --description \u0026quot;LA KMS DEMO CMK\u0026quot; # Create Alias for KeyID aws kms create-alias --target-key-id XXX --alias-name \u0026quot;alias/lakmsdemo\u0026quot; --region us-east-1 # Encrypt a file with CMK echo \u0026quot;this is a secret message\u0026quot; \u0026gt; topsecret.txt aws kms encrypt --key-id KEYID --plaintext file://topsecret.txt --output text --query CiphertextBlob aws kms encrypt --key-id KEYID --plaintext file://topsecret.txt --output text --query CiphertextBlob | base64 --decode \u0026gt; topsecret.encrypted # Decrypt a Cipher Text aws kms decrypt --ciphertext-blob fileb://topsecret.encrypted --output text --query Plaintext | base64 --decode aws kms generate-data-key --key-id KEYID --key-spec AES_256 --region us-east-1   KMS can generate a data encryption key (DEK) using a CMK. You or a service can use a DEK to encrypt or decrypt data of any size. KMS supplies a plaintext version and an encrypted version\n# Generate Data Key aws kms generate-data-key --key-id KEYID --key-spec AES_256  Deployment Elastic Beanstalk Elastic Beanstalk (EB) is a Platform as a Service product. It allows you to deploy code and, with very little effort or modification, the service will provision the infrastructure on your behalf.\nElastic Beanstalk handles provisioning, monitoring, Auto Scalling, load balancing, and software updating for you - you just worry about the cost.\nIf you need to provision an environment for an application with little admin overhead and if you use one of the supported languages and can see EB-specific config.\n Donot use Elastic Beanstalk if you want low-level infrastructure control.\n Deployment Options  All at Once - An updated application version is deployed to all instances. Quick and simple but not recommended for production deployments. Rolling Deployment - Splits instances into batches and deploys one batch at a time Rolling with Additional Batch - Similar to Rolling Deployment, but provisions a new batch, deploying and testing before removing the old batch Blue/Green - Maintains two environments, deploy, and swap CNAME.  OpsWorks  OpsWork is an implementation of the Chef configuration management and deployment platform. OpsWorks move away from the low-level configurability of CloudFormation but not as far as Elastic Beanstalk OpsWork lets you create a stack of resources with layers and manage resources as a unit.  OpsWorks Components  Stacks  A unit of managed infrastructure Can use stacks per application or per platform Could use stacks for development, staging, or production environments  Layers  Comparable to application tiers within a stack e.g, Database Layer, Application Layer, Proxy Layer Receipes are generally associated with layers and configuration what to install on instances in that layer  Instances  Instances are EC2 instances associated with a layer Configured as 24\u0026frasl;7, load based, or time based  Apps  Apps are deployed to layers from a source code repo or S3 Actual deployment happens using receipes on layers  Recipes  Setup - Executed on an instance when first provisioned Configure - Executed on all instances when instances are added or removed Deploy and UnDeploy - Executed when apps are added or removed Shutdown - Executed when an instance is shut down but before it\u0026rsquo;s stoppepd   "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/09_configmaps_secrets/",
	"title": "09 - ConfigMaps and Secrets",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps  Creating ConfigMaps Using CommandLine Using Configuration File Using Properties File Using ConfigMaps Inside Pods As Environment Variable As Volume  Secrets  Creating Secret Create a Secret from Literal and Display Its Details Create a Secret from YAML File Use Secrets Inside Pods   ConfigMaps ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories.\nCreating ConfigMaps Using CommandLine kubectl create configmap my-config \\ --from-literal=key1=value1 \\ --from-literal=key2=value2  Using Configuration File apiVersion: v1 kind: ConfigMap metadata: name: customer1 data: TEXT1: Customer1_Company TEXT2: Welcomes You COMPANY: Customer1 Company Technology Pct. Ltd.  Using Properties File permission-reset.properties\npermission=read-only allowed=\u0026quot;true\u0026quot; resetCount=3  kubectl create configmap permission-config --from-file=\u0026lt;path/to/\u0026gt;permission-reset.properties  Using ConfigMaps Inside Pods As Environment Variable Inside a Container, we can retrieve the key-value data of an entire ConfigMap or the values of specific ConfigMap keys as environment variables.\ncontainers: - name: myapp-full-container image: myapp envFrom: - configMapRef: name: full-config-map  containers: - name: myapp-specific-container image: myapp env: - name: SPECIFIC_ENV_VAR1 valueFrom: configMapKeyRef: name: config-map-1 key: SPECIFIC_DATA - name: SPECIFIC_ENV_VAR2 valueFrom: configMapKeyRef: name: config-map-2 key: SPECIFIC_INFO  As Volume We can mount a vol-config-map ConfigMap as a Volume inside a Pod. For each key in the ConfigMap, a file gets created in the mount path (where the file is named with the key name) and the content of that file becomes the respective key\u0026rsquo;s value:\ncontainers: - name: myapp-vol-container image: myapp volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: vol-config-map  Secrets Secret object can help by allowing us to encode the sensitive information before sharing it. With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, similar to ConfigMaps; thus, we can control how the information in a Secret is used, reducing the risk for accidental exposures. In Deployments or other resources, the Secret object is referenced, without exposing its content.\nIt is important to keep in mind that the Secret data is stored as plain text inside etcd, therefore administrators must limit access to the API server and etcd. A newer feature allows for Secret data to be encrypted at rest while it is stored in etcd; a feature which needs to be enabled at the API server level.\n Creating Secret Create a Secret from Literal and Display Its Details # Create Secret kubectl create secret generic my-password --from-literal=password=mysqlpassword # Get Secret kubectl get secret my-password # Describe Secret kubectl describe secret my-password  Create a Secret from YAML File echo mysqlpassword | base64 bXlzcWxwYXNzd29yZAo=  mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque data: password: bXlzcWxwYXNzd29yZAo=  Please note that base64 encoding does not mean encryption, and anyone can easily decode our encoded data\n mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque stringData: password: mysqlpassword  # Create Secret from YAML kubectl create -f mypass.yaml  Use Secrets Inside Pods Using Secrets as Environment Variables\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress env: - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: my-password key: password  Using Secrets as Files from a Pod\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress volumeMounts: - name: secret-volume mountPath: \u0026quot;/etc/secret-data\u0026quot; readOnly: true volumes: - name: secret-volume secret: secretName: my-password  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/10_ingress/",
	"title": "10 - Ingress",
	"tags": [],
	"description": "",
	"content": " With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.\n An Ingress is a collection of rules that allow inbound connections to reach the cluster Services.\n To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following:\n TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules  Name-Based Virtual Hosting virtual-host-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: virtual-host-ingress namespace: default spec: rules: - host: blue.example.com http: paths: - backend: serviceName: webserver-blue-svc servicePort: 80 - host: green.example.com http: paths: - backend: serviceName: webserver-green-svc servicePort: 80  fan-out-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: fan-out-ingress namespace: default spec: rules: - host: example.com http: paths: - path: /blue backend: serviceName: webserver-blue-svc servicePort: 80 - path: /green backend: serviceName: webserver-green-svc servicePort: 80  The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller, which we will discuss next.\n # Start the Ingress Controller with Minikube minikube addons enable ingress # Deploy an Ingress Resource kubectl create -f virtual-host-ingress.yaml  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/11_advanced_topics/",
	"title": "11 - Advanced Topics",
	"tags": [],
	"description": "",
	"content": " Annotations With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:\n\u0026quot;annotations\u0026quot;: { \u0026quot;key1\u0026quot; : \u0026quot;value1\u0026quot;, \u0026quot;key2\u0026quot; : \u0026quot;value2\u0026quot; }  Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:\n Store build/release IDs, PR numbers, git branch, etc. Phone/pager numbers of people responsible, or directory entries specifying where such information can be found Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc.  apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019  Jobs and CronJobs A Job creates one or more Pods to perform a given task. The Job object takes the responsibility of Pod failures. It makes sure that the given task is completed successfully. Once the task is complete, all the Pods have terminated automatically. Job configuration options include\n parallelism - to set the number of pods allowed to run in parallel; completions - to set the number of expected completions; activeDeadlineSeconds - to set the duration of the Job; backoffLimit - to set the number of retries before Job is marked as failed; ttlSecondsAfterFinished - to delay the clean up of the finished Jobs.  Quota Management Autoscalling Autoscaling can be implemented in a Kubernetes cluster via controllers which periodically adjust the number of running objects based on single, multiple, or custom metrics. There are various types of autoscalers available in Kubernetes which can be implemented individually or combined for a more robust autoscaling solution:\nHorizontal Pod Autoscaler (HPA) HPA is an algorithm based controller API resource which automatically adjusts the number of replicas in a ReplicaSet, Deployment or Replication Controller based on CPU utilization.\nVertical Pod Autoscaler (VPA) VPA automatically sets Container resource requirements (CPU and memory) in a Pod and dynamically adjusts them in runtime, based on historical utilization data, current resource availability and real-time events.\nCluster Autoscaler Cluster Autoscaler automatically re-sizes the Kubernetes cluster when there are insufficient resources available for new Pods expecting to be scheduled or when there are underutilized nodes in the cluster.\nDaemonSets A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\n running a cluster storage daemon, such as glusterd, ceph, on each node. running a logs collection daemon on every node, such as fluentd or logstash. running a node monitoring daemon on every node, such as Prometheus Node Exporter, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or Instana Agent.  StatefulSets The StatefulSet controller is used for stateful applications which require a unique identity, such as name, network identifications, strict ordering, etc. For example, MySQL cluster, etcd cluster.\nThe StatefulSet controller provides identity and guaranteed ordering of deployment and scaling to Pods. Similar to Deployments, StatefulSets use ReplicaSets as intermediary Pod controllers and support rolling updates and rollbacks.\nKubernetes Federation With Kubernetes Cluster Federation we can manage multiple Kubernetes clusters from a single control plane. We can sync resources across the federated clusters and have cross-cluster discovery. This allows us to perform Deployments across regions, access them using a global DNS record, and achieve High Availability.\nHelm Chart To deploy an application, we use different Kubernetes manifests, such as Deployments, Services, Volume Claims, Ingress, etc. Sometimes, it can be tiresome to deploy them one by one. We can bundle all those manifests after templatizing them into a well-defined format, along with other metadata. Such a bundle is referred to as Chart. These Charts can then be served via repositories, such as those that we have for rpm and deb packages.\nHelm is a package manager (analogous to yum and apt for Linux) for Kubernetes, which can install/update/delete those Charts in the Kubernetes cluster.\nHelm has two components:\n A client called helm, which runs on your user\u0026rsquo;s workstation A server called tiller, which runs inside your Kubernetes cluster.  The client helm connects to the server tiller to manage Charts.\nSecurity Contexts and Pod Security Policies At times we need to define specific privileges and access control settings for Pods and Containers. Security Contexts allow us to set Discretionary Access Control for object access permissions, privileged running, capabilities, security labels, etc. However, their effect is limited to the individual Pods and Containers where such context configuration settings are incorporated in the spec section.\nIn order to apply security settings to multiple Pods and Containers cluster-wide, we can define Pod Security Policies. They allow more fine-grained security settings to control the usage of the host namespace, host networking and ports, file system groups, usage of volume types, enforce Container user and group ID, root privilege escalation, etc.\nNetwork Policies Kubernetes was designed to allow all Pods to communicate freely, without restrictions, with all other Pods in cluster Namespaces. In time it became clear that it was not an ideal design, and mechanisms needed to be put in place in order to restrict communication between certain Pods and applications in the cluster Namespace. Network Policies are sets of rules which define how Pods are allowed to talk to other Pods and resources inside and outside the cluster. Pods not covered by any Network Policy will continue to receive unrestricted traffic from any endpoint. Network Policies are very similar to typical Firewalls. They are designed to protect mostly assets located inside the Firewall but can restrict outgoing traffic as well based on sets of rules and policies.\nThe Network Policy API resource specifies podSelectors, *Ingress* and/or *Egress* policyTypes, and rules based on source and destination ipBlocks and ports. Very simplistic default allow or default deny policies can be defined as well. As a good practice, it is recommended to define a default deny policy to block all traffic to and from the Namespace, and then define sets of rules for specific traffic to be allowed in and out of the Namespace. Let\u0026rsquo;s keep in mind that not all the networking solutions available for Kubernetes support Network Policies. Review the Pod-to-Pod Communication section from the Kubernetes Architecture chapter if needed. By default, Network Policies are namespaced API resources, but certain network plugins provide additional features so that Network Policies can be applied cluster-wide.\nMonitoring and Logging In Kubernetes, we have to collect resource usage data by Pods, Services, nodes, etc., to understand the overall resource consumption and to make decisions for scaling a given application. Two popular Kubernetes monitoring solutions are the Kubernetes Metrics Server and Prometheus.\n Metrics Server\nMetrics Server is a cluster-wide aggregator of resource usage data - a relatively new feature in Kubernetes.  PrometheusPrometheus, now part of CNCF (Cloud Native Computing Foundation), can also be used to scrape the resource usage from different Kubernetes components and objects. Using its client libraries, we can also instrument the code of our application.  Another important aspect for troubleshooting and debugging is Logging, in which we collect the logs from different components of a given system. In Kubernetes, we can collect logs from different cluster components, objects, nodes, etc. Unfortunately, however, Kubernetes does not provide cluster-wide logging by default, therefore third party tools are required to centralize and aggregate cluster logs. The most common way to collect the logs is using Elasticsearch, which uses fluentd with custom configuration as an agent on the nodes. fluentd is an open source data collector, which is also part of CNCF.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/12_resources/",
	"title": "12 - Resources",
	"tags": [],
	"description": "",
	"content": " Labs CKA Curriculum   "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/",
	"title": "AWS Certified Solutions Architect Course",
	"tags": [],
	"description": "",
	"content": "  01 - Architecture 101   02 - AWS Fundamentals   03 - Identity and Access Control   04 (A) - Compute Services - Server Based   04 (B) - Compute Services - Serverless   05 - Networking   06 - Databases   07 - Hybrid and Scaling   07 - Storage and Content Delivery   09 - Application, Analytics, and Operations   Hands on Labs   "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/",
	"title": "AWS Solutions Architect Associate - Study Guide",
	"tags": [],
	"description": "",
	"content": "  01 - AWS Compute Services   02 - VPC Networking   03 - AWS Storage   04 - Network and Content Delivery   05 - AWS Database Services   06 - Analytics   07 - Application Integration   08 - Hybrid and Scaling   Resources   "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/",
	"title": "Amazon Web Services",
	"tags": [],
	"description": "",
	"content": "  AWS Certified Solutions Architect Course   AWS Solutions Architect Associate - Study Guide   "
},
{
	"uri": "http://learn.aayushtuladhar.com/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/",
	"title": "Arrow Functions",
	"tags": [],
	"description": "",
	"content": " Arrow Function Express allows you to write shorter syntax than it\u0026rsquo;s predecessor Function expression. In addition and more exciting is how the new Arrow function bind their context.\n(param1, param2, param3) =\u0026gt; { statements } singleParam =\u0026gt; { statements } () =\u0026gt; { statements }  Example var materials = [ \u0026#39;Iron\u0026#39;, \u0026#39;Calcium\u0026#39;, \u0026#39;Sodium\u0026#39;, \u0026#39;Magnanese\u0026#39; ] materials.map(material =\u0026gt; material.length)  An arrow function does not newly define its own this when it\u0026rsquo;s being executed.The value of this is always inherited from the enclosing scope.\n// ES5 function CounterES5(){ this.seconds = 0; window.setInterval(function() { this.seconds++; console.log(seconds); }.bind(this), 1000); } //ES6 function CounterES6(){ this.seconds =0; window.setInterval( () =\u0026gt; { this.seconds++; console.log(seconds) },1000 ); }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/",
	"title": "Basic Essentials",
	"tags": [],
	"description": "",
	"content": " Creting Instance (Directly) # Create Compute Instance gcloud compute instances create gcelab2 --machine-type n1-standard-2 \\ --zone us-central1-c  Using Instance Templates / Instance Groups # Create Instance Template gcloud compute instance-templates create nginx-template \\ --metadata-from-file startup-script=startup.sh # Create Target Pool gcloud compute target-pools create nginx-pool # Create Instance Group gcloud compute instance-groups managed create nginx-group \\ --base-instance-name nginx \\ --size 2 \\ --template nginx-template \\ --target-pool nginx-pool # List Instances gcloud compute instances list  Create Filewall # Create Filewall rule to Allow 80 gcloud compute firewall-rules create www-firewall --allow tcp:80  SSH Instance gcloud compute ssh gcelab2 --zone us-central1-c   gcloud is a command-line tool for Google Cloud Platform\n  gsutil is a command-line tool to mange Cloud Storage resources\n Config # List Config gcloud config list # Sets Default Zone to us-central1-a gcloud config set compute/zone us-central1-a  Kubernetes Cluster Engine A cluster consists of at least one cluster master machine and multiple worker machines called nodes. Nodes are Compute Engine virtual machine (VM) instances that run the Kubernetes processes necessary to make them part of the cluster.\n# Create Cluster gcloud container clusters create my-precious-cluster # Updates a kubeconfig file with appropriate credentials to point kubectl at a specific cluster in GKE gcloud container cluster get-credentials my-precious-cluster # Delete Cluster gcloud container clusters delete my-precious-cluster  Network Load Balancer Network load balancing allows you to balance the load of your systems based on incoming IP protocol data, such as address, port, and protocol type. You also get some options that are not available, with HTTP(S) load balancing. For example, you can load balance additional TCP/UDP-based protocols such as SMTP traffic. And if your application is interested in TCP-connection-related characteristics, network load balancing allows your app to inspect the packets, where HTTP(S) load balancing does not.\n# Create Forwarding Rules gcloud compute forwarding-rules create nginx-lab \\ --region us-central1 --port=80 --target-pool nginx-pool # List Forwarding Rules gcloud compute forwarding-rules list  Creating HTTP(s) Load Balancer # Create Health Check gcloud compute http-health-checks create http-basic-check # Defining a HTTP service and map a port name to the relevant port gcloud compute instance-groups manged set-named-ports nginx-group --named-ports http:80 # Creating Backend Service gcloud compute backend-services create nginx-backend \\ --protocol HTTP \\ --http-health-checks http-basic-check \\ --global # Adding instance group to the backend services gcloud compute backend-services add-backend nginx-backend \\ --instance-group nginx-group \\ --instance-group-zone us-central1-a \\ --global # Create a default URL map that directs all incoming requests to all your instances gcloud compute url-maps create web-map --default-service nginx-backend # Create a target HTTP proxy to route requests to URL map gcloud compute target-http-proxies create http-lb-proxy --url-map web-map # Create global forwarding rule to handle and route incoming requests gcloud compute forwarding-rules create http-content-rule \\ --global --target-http-proxy http-lb-proxy --ports 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2016-02-29-berks/",
	"title": "Berkshelf",
	"tags": [],
	"description": "",
	"content": " Berkshelf Reference External Dependent Cookbooks so that it can download those cookbooks rather than manually downloading via knife cookbook command.\nknife cookbook site\nBerkshelf lets you treat your cookbooks the way you treat gem in a Ruby project. When external cookbooks are used, Berkshelf doesn\u0026rsquo;t requite knife cookbook site to install community cookbooks.\nImplementing Berkshelf gem install berkshelf\nInstall Cookbooks via Berks berks install\nUpload berks to Chef Server berks upload \u0026lt;cookbook\u0026gt;\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/data/",
	"title": "Big Data",
	"tags": [],
	"description": "",
	"content": "  Hive Query Optimizations   Hive Tables [Best Practices]   Jupyter NoteBook - Shortcuts   Setting up Apache Storm   Using Pandas   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/ci-cd/",
	"title": "CI / CD",
	"tags": [],
	"description": "",
	"content": "  Circle CI   Drone   Jenkins   Jenkins Shared Library   "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/",
	"title": "Certified Kubernetes Application Developer (CKAD)",
	"tags": [],
	"description": "",
	"content": "  01 - Setting up Kubernetes Cluster   02 - Core Concepts   03 - Configuration   04 - Multi Container Pods   05 - Observability   06 - Pod Design   Lab1 - Creating Kubernetes Pod   Lab2 - Configuring Kubernetes Pod   Lab3 - Forwarding Port Traffic with an Ambassador Container   Reference  Linux Academy - CKAD Study Guide  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/",
	"title": "Chef",
	"tags": [],
	"description": "",
	"content": "  Berkshelf   Chef Databags   Knife Commands   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2018-01-30-chef-databags/",
	"title": "Chef Databags",
	"tags": [],
	"description": "",
	"content": " Databags are global variables that is stored as JSON Data and is accessible from Chef Server. A data bag is indexed for searching and can be loaded by recipe or accessed during a search.\nCreating Data Bag (Using Knife) $ knife data bag create DATA_BAG_NAME (DATA_BAG_ITEM) knife data bag create TEST_BAG  Adding File to Data Bag knife data bag from file TEST_BAG test.json  Data Bag Items A data bag is container of related data bag items, where each individual data bag item is a JSON file.\nUsing Data Bags data_bag_item('keystore_file', 'dev')  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/ci-cd/circle-ci/",
	"title": "Circle CI",
	"tags": ["devops"],
	"description": "",
	"content": "  Concepts Workflows Jobs Steps Image Example  Concepts Workflows Workflows define a list of jobs and their run order. It is possible to run jobs concurrently, sequentially, on a schedule, or with a manual gate using an approval job.\nJobs Jobs are a collection of Steps. All of the steps in the job are executed in a single unit which consumes a CircleCI container from your plan while it’s running. Each job must declare an executor that is either docker, machine, windows or macos. machine includes a default image if not specified, for docker you must specify an image to use for the primary container, for macos you must specify an Xcode version, and for windows you must use the Windows orb.\nSteps Steps are actions that need to be taken to perform your job. Steps are usually a collection of executable commands. For example, the checkout step checks out the source code for a job over SSH. Then, the run step executes the make test command using a non-login shell by default.\nImage An image is a packaged system that has the instructions for creating a running container. The Primary Container is defined by the first image listed in .circleci/config.yml file. This is where commands are executed for jobs using the Docker or machine executor. The Docker executor spins up a container with a Docker image. The machine executor spins up a complete Ubuntu virtual machine image. See Choosing an Executor Type document for a comparison table and considerations.\nExample version: 2 jobs: one: docker: - image: circleci/ruby:2.4.1 steps: - checkout - run: echo \u0026quot;Hello Ruby\u0026quot; - run: mkdir -p my_workspace - run: echo \u0026quot;Trying out workspaces\u0026quot; \u0026gt; my_workspace/echo-output - persist_to_workspace: root: my_workspace paths: - echo-output - run: sleep 25 two: docker: - image: circleci/ruby:2.4.1 steps: - checkout - run: echo \u0026quot;A more familiar Hi\u0026quot; - attach_workspace: at: my_workspace - run: | if [[ $(cat my_workspace/echo-output) == \u0026quot;Trying out workspaces\u0026quot; ]]; then echo \u0026quot;It worked!\u0026quot;; else echo \u0026quot;Nope!\u0026quot;; exit 1 fi - run: sleep 15 workflows: version: 2 one_two_wf: jobs: - one - two: requires: - one  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/cloud-devops-engineer/",
	"title": "Cloud DevOps Engineer",
	"tags": [],
	"description": "",
	"content": " Networking Communication Model Three components of networking\n Medium - How are you connected ? Addressing - How do you locate / identify another party ? Content - What information are you sharing ?  Addressing\nIPv4 - 32 Bit address (192.168.112.20) ~ 4 Billion\nIPv6\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/",
	"title": "Cloud Storage",
	"tags": [],
	"description": "",
	"content": " Create a Stoage Bucket gsutil mb gs://unique-name  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/",
	"title": "Continuous Delivery with Jenkins in Kubernetes Engine",
	"tags": [],
	"description": "",
	"content": "# Create Kubernetes Cluster gcloud container clusters create jenkins-cd \\ --num-nodes 2 \\ --machine-type n1-standard-2 \\ --scopes \u0026quot;https://www.googleapis.com/auth/projecthosting,cloud-platform\u0026quot; # Update KubeConfig with Cluster credentials gcloud container clusters get-credentials jenkins-cd # Verify Kubernetes can connect to GCP Kubernetes Cluster kubectl cluster-info  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/creating_static_website_using_s3/",
	"title": "Creating a Static Website Using Amazon S3",
	"tags": [],
	"description": "",
	"content": " Create S3 Bucket  Navigate to the S3 portion of the AWS Management Console. Create a bucket, choosing a globally unique name. Select US East (N. Virginia) region. Click Next. Leave options as defaults; click Next. Under permissions, uncheck all four permissions restrictions. Click Next. Click Create bucket. Select bucket name. Click Upload. Add files (use your own or those from the sample website). Click Upload.  Enable Static Website Hosting  Click the bucket name. Navigate to Properties \u0026gt; Static website hosting. Select Use this bucket to host a website. For Index document, enter index.html. For Error document, enter error.html. Click Save.  Apply Bucket Policy Navigate to Permissions \u0026gt; Bucket policy. Add the following JSON statement (replacing  with your bucket name):\n{ \u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;:[{ \u0026quot;Sid\u0026quot;:\u0026quot;PublicReadGetObject\u0026quot;, \u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Action\u0026quot;:[\u0026quot;s3:GetObject\u0026quot;], \u0026quot;Resource\u0026quot;:[\u0026quot;arn:aws:s3:::\u0026lt;my-bucket\u0026gt;/*\u0026quot;] }] }  Ensure the trailing /* is present so that the policy applies to all objects within the bucket.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/network/2017-01-29-dns-basics/",
	"title": "DNS Basics",
	"tags": [],
	"description": "",
	"content": " DNS (Domain Name System) is essential component of modern internet communication. It allows us to reference computers by human friendly names instead of IP Addresses.\nTerminologies  Domain Name IP Address  DNS Lookup using Dig Dig is a flexible tool for interrogating DNS name servers. It performs DNS lookup and is very helpful to troubleshoot DNS problems.\ndig \u0026lt;serverName\u0026gt; +nostats +nocomments +nocmd  $ dig google.com +nostats ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; google.com +nostats ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 9995 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;google.com.\tIN\tA ;; ANSWER SECTION: google.com.\t142\tIN\tA\t172.217.8.206   The first line tell us the version of dig (9.10.6) command Next, dig shows the header of the response it received from the DNS server The question section tells us about the A Record and IN means this is an internet lookup The answer section tells us that google.com has IP address of 172.217.8.206  References  DNS Concepts  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/vpc/",
	"title": "Designing and Building a Custom VPC from Scratch",
	"tags": [],
	"description": "",
	"content": " This hands-on lab provides you with some experience building and connecting the following services inside AWS: VPC, subnets, internet gateway, NAT gateways, Bastion host, route tables, security groups, and network access control lists (NACLs). These services are the foundation of networking architecture inside of AWS and cover concepts such as infrastructure, design, routing, and security.\nCreate a VPC  Select Your VPCs. Click Create VPC, and set the following values:  labVPC 10.0.0.0/16 Amazon Provided IPv6 block Default Tenancy  Click Create.  Create Subnets Create a three-AZ, three-app tier subnet layout (leaving spaces for a fourth AZ and fourth tier).\n Select Subnets. Click Create subnet. Enter the following values in order for Name, VPC, Availability Zone, and IPv4 CIDR Block. Don\u0026rsquo;t assign IPv6 block.  publicA, labVPC, us-east-1a, 10.0.0.0/24 publicB, labVPC, us-east-1b, 10.0.1.0/24 publicC, labVPC, us-east-1c, 10.0.2.0/24 Skip 10.0.3.0/24 as the reserved space for a fourth AZ public subnet privateA, labVPC, us-east-1a, 10.0.4.0/24 privateB, labVPC, us-east-1b, 10.0.5.0/24 privateC, labVPC, us-east-1c, 10.0.6.0/24 Skip 10.0.7.0/24 as the reserved space for a fourth AZ private subnet dbA, labVPC, us-east-1a, 10.0.8.0/24 dbB, labVPC, us-east-1b, 10.0.9.0/24 dbC, labVPC, us-east-1c, 10.0.10.0/24 Skip 10.0.11.0/24 as the reserved space for a fourth AZ db subnet   10.0.12.0/24, 10.0.13.0/24, 10.0.14.0/24, and 10.0.15.0/24 can be used for the fourth tier in four AZs, but we won\u0026rsquo;t create them for now.\ncheck_circleCreate Internet Gateway, Public Routing, and Bastion Host\nFrom the VPC console:\n Select Subnets. Select publicA, Actions, and Modify auto-assign IP settings. Enable Enable auto-assign public IPv4 address. Repeat for publicB and publicC.  Configure Internet Gateway  Select Internet Gateways, and click Create internet gateway. Set the name tag as labVPCIGW, and click Create. Select the newly created IGW, click Actions and then Attach to VPC. Select labVPC and click Attach.  Configure Routing  Click Route Tables. Click Create route table. Set the name tag as publicRT and the VPC as labVPC. Click Create.  Add Default Public Route  Select publicRT, click Routes, Edit routes, and Add route. Set the destination as 0.0.0.0/0, target as Internet Gateway, and select labVPCIGW. Click Add route again, set the destination as ::/0 , Internet Gateway, and select labVPCIGW. Click Save routes. Click Close.  Associate with Subnets  Select publicRT, and click the Subnet Associations tab. Click Edit subnet associations. Select publicA, publicB, and publicC. Click Save.  Create a Bastion Host From the EC2 console:\n Click Launch Instance. Choose Amazon Linux 2, check 64-bit (x86), and click Select. Choose t3.micro, and click Next: Configure Instance Details. Leave all as defaults, except set the subnet to publicB and make sure *Auto-assign Public IP* is Use subnet setting (Enable). Click Next: Add Storage. Click Next: Add Tags. Add a tag, and set the *Key* to Name and *Value* to BastionHost. Click Next: Configure Security Group. For security group, create a new one with the name and description bastionSG. Click Review and Launch. Click Launch, select to Create a new key pair, call it vpclab, and click Download Key Pair. Click Launch Instances and then View Instances.  Verify Bastion Host Is Working From the EC2 console:\n When the bastion host has 2\u0026frasl;2 status checks, right-click, click Connect, and copy the connection command.  Linux/macOS users will need to run a chmod 400 vpclab.pem command first to avoid errors. Windows users can connect using this as a guide.  Run the connection command to connect to your bastion host.  check_circleConfigure Private Internet Connectivity Using NAT Gateway\nFrom the VPC Console:\nCreate the NAT Gateways  Click NAT Gateways and then Create NAT Gateway. Set the subnet to publicA. Click Create New EIP and then Create a NAT Gateway. Click Close. Repeat the process for publicB and publicC for a total of three NAT gateways. Select each NAT gateway in turn, and make a note of the NAT Gateway ID and which public subnet it\u0026rsquo;s in.  Create Three Private Route Tables  Click Route Tables. Click Create route table. Set the name as privateA-RT and VPC as labVPC. Click Create and then Close. Repeat for privateB-RT and privateC-RT.  Route Table Associations Do the following for each route table in privateA-RT, privateB-RT, and privateC-RT.\n Select the Subnet Associations tab, click Edit subnet associations, select the db and private subnets in the same AZ.  privateA-RT = privateA and dbA  Click Save. On the same route table, click Routes, Edit routes, and Add route. Set the destination as 0.0.0.0/0, target as NAT Gateway, and select the NAT Gateway ID in the same AZ (in the list you made earlier). Click Close. Repeat these steps for each route table.  check_circleConfigure and Test VPC Security\n Create an App Server privateA using the same bastion vpclab key. Configure security group, only allowing incoming SSH from the bastion security group. Log in via SSH to the App Server. In the AWS console, navigate to VPC \u0026gt; Network ACLs. Select the default NACL, and click Edit inbound rules. Click Add Rule, and set the following values:  Rule #: 50 Type: ALL Traffic Source: Your IP address (which you can get by googling \u0026ldquo;what is my IP\u0026rdquo; in a new browser tab), and append /32 at the end Allow / Deny: DENY  In the terminal, try to log in to the bastion host. In the AWS console, remove the NACL\u0026rsquo;s rule #50 to remove the explicit DENY. In the terminal, try connecting to the bastion host again.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/destructuring/",
	"title": "Destructuring JavaScript Objects",
	"tags": [],
	"description": "",
	"content": "const person = { firstName: \u0026#39;Aayush\u0026#39;, lastName: \u0026#39;Tuladhar\u0026#39;, country: \u0026#39;Nepal\u0026#39;, twitter: \u0026#39;@aayushtuladhar\u0026#39; } /* Problem */ const first = person.firstName; const last = person.lastName; console.log(`Hello ${first}${last}`); /* Solution */ const { firstName, lastName } = person; console.log(`Hello ${firstName}${lastName}`); /* ------------------ */ const art = { first: \u0026#39;ART\u0026#39;, last: \u0026#39;Ratna\u0026#39;, links: { social: { twitter: \u0026#39;https://twitter.com/aayushtuladhar\u0026#39;, facebook: \u0026#39;https://facebook.com/aayush.tuladhar\u0026#39;, }, web: { blog: \u0026#39;https://aayushtuladhar.com\u0026#39; } } }; const { twitter, facebook } = art.links.social; console.log(twitter); console.log(facebook);  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "  CI / CD   Chef   Cloud DevOps Engineer   DNS Basics   Docker Compose   Gradle Wrapper   Installing Docker CE on Centos 7   Linux Command Line Hacks   Mqtt   Network Tools / Command Essentials   OWASP Dependency Check   OpenStack   Openshift   Security   Setting Apache Virtual Host   Setting FQDN   Setting Selenium Grid   Setting up Jekyll Website   SupervisorD   Ubuntu Packages   Using Apache Server Benchmarking   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/docker/2019-09-17-docker-compose/",
	"title": "Docker Compose",
	"tags": [],
	"description": "",
	"content": " Docker Compose is used to run multiple containers as a single service.\n# docker-compose.yml version: '2' services: web: build: . # build from Dockerfile context: ./Path dockerfile: Dockerfile ports: - \u0026quot;5000:5000\u0026quot; volumes: - .:/code redis: image: redis  Command Line # Start Service docker-compose start # Stop Service docker-compose stop # Pause Service docker-compose pause # UnPause Service docker-compose unpause # List containers docker-compose ps # Create and start containers docker-compose up # Stop and remove containers, networks, images, and volumes docker-compose down  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/ci-cd/2018-04-19-drone/",
	"title": "Drone",
	"tags": [],
	"description": "",
	"content": "  Creating Pipeline  Images Cloning Commands Services Plugins  Running Drone Locally Drone Secrets  Repo Level Secrets Org Level Secrets Using Secrets in Pipeline   Drone is a CI/CD platform built on Docker and written in Go.\nCreating Pipeline Drone pipeline are written in .drone.yml file in the root of the repository.\n Pipelines are event based, which can be triggered via push, pull_request, tag and deployment events\npipeline: backend: image: golang commands: - go get - go build - go test frontend: image: node:6 commands: - npm install - npm test notify: image: plugins/slack channel: developers username: drone   Images Drone executes your build inside an ephemeral Docker Image, which means you don\u0026rsquo;t have to setup or install any repository dependencies.\nCloning Drone automatically clones your repository into a local volume that is mounted into each Docker container.\nCommands Drone mounts the workspace into your build container (Defined Image) and executes bash commands inside your build container, using the root of your repository as the working directory.\nServices Drone supports launching service containers as part of the build process. This can be very helpful when your unit tests require database access etc.\nPlugins Drone supports publish, deployments and notification capabilities through external plugins.\nRunning Drone Locally drone exec  Drone Secrets Repo Level Secrets # Create Value Secret (Concealed) drone secret add --conceal \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create Value Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create File Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt; # View Secret for a Repo drone secret ls \u0026lt;repo_path\u0026gt;  Org Level Secrets # Create Org Level Secret (Concealed) drone org secret add --conceal \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level File Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt;  Using Secrets in Pipeline pipeline: docker: image: plugins/docker - username: ${DOCKER_USERNAME} - password: ${DOCKER_PASSWORD} + secrets: [ docker_username, docker_password ]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/eloquent_js/",
	"title": "Eloquent JavaScript",
	"tags": [],
	"description": "",
	"content": "  JavaScript Revisited   Eloquent JavaScript\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Learning the Basics Stuff Starting Server hugo server -D  Documentation Learn\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/cloud_formation/",
	"title": "Getting Started with Cloud Formation",
	"tags": [],
	"description": "",
	"content": "AWS CloudFormation provides users with a simple way to create and manage a collection of Amazon Web Services (AWS) resources by provisioning and updating them in a predictable way. AWS CloudFormation enables you to manage your complete infrastructure or AWS resources in a text file.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/reactjs/redux/",
	"title": "Getting Started with Redux",
	"tags": [],
	"description": "",
	"content": " Redux gives you a store, and lets you keep state in it, and get state out, and respond when the state changes. But that’s all it does.\nIt’s actually react-redux that lets you connect pieces of the state to React components.\n The state is the data, and the store is where it’s kept\n Redux Store Redux Reducer Reducer\u0026rsquo;s job is to take the current state and action and return the new state. It has another job, too. It should return the initial state the first time it\u0026rsquo;s called.\nReducer Rule # 1 = Never return undefined from a reducer Reduce Rule # 2 = Reduces must be a pure functions (They can\u0026rsquo;t modify their arguments, and they can\u0026rsquo;t have side effects)\nRedux Actions An action is Redux-speak for a plain object with a property called type. In order to keep things sane and maintainable, we Redux users usually give our actions types that are plain strings, and often uppercased, to signify that they’re meant to be constant values.\nAn action object describes a change you want to make (like “please increment the counter”) or an event that happenend (like “the request to the server failed with this error”).\nIn order to make an action DO something, you need to dispatch it.\nRedux Dispatch The store we created earlier has a built-in function called dispatch. Call it with an action, and Redux will call your reducer with that action (and then replace the state with whatever your reducer returned).\nReferences "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/",
	"title": "Google Cloud Platform",
	"tags": [],
	"description": "",
	"content": "  Basic Essentials   Cloud Storage   Continuous Delivery with Jenkins in Kubernetes Engine   Orchestrating Cloud with Kubernetes   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/gradle/2016-07-06-gradle-wrapper/",
	"title": "Gradle Wrapper",
	"tags": [],
	"description": "",
	"content": " Gradle Wrapper The Gradle wrapper allows you to run a Gradle task without requiring that Gradle is installed on your system.\nCreating Gradle Wrapper task wrapper(type: Wrapper) { gradleVersion = '2.10' //we want gradle 2.10 to run this project }  Running Gradle Wrapper gradle wrapper\nFollowing files will be created:\n|-gradle |--- wrapper |--- gradle-wrapper.jar |--- gradle-wrapper.properties |-gradlew |-gradlew.bat  Gradle wrapper are useful when you want to run gradle command without installing gradle\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/",
	"title": "Hands on Labs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/data/hive/2017-05-18-hive-query-optimizations/",
	"title": "Hive Query Optimizations",
	"tags": ["hive", "bigdata"],
	"description": "",
	"content": " Changing Engine for Hive Queries In general, Tez provides increased performance over Map Reduce specially where JOINS are required resulting in intermediate output being written to disk.\nset hive.execution.engine=tez  Using Partitions Partitions separate date in Hive tables by HDFS directories. If data is distributed based on partitions on certain fields, Using those fields to access data allows speeding up Hive queries. In other words, querying without partitions equates to full table scan.\nSplits and Number of Files Considering splits and number of files created while doing joins helps to analyze query performance.\nPerformance Considerations for using JOINS Joins are important aspects of the SQL queries. Avoid using correlated queries and in-line tables. Create temporary tables and try to use inner join wherever possible. You can also use the Hive WITH clause instead of temporary tables\nAlso you should avoid any queries that leads to CARTESIAN JOINS.\nAvoid loading Too Many Small Files Hadoop works well with large files and it applies to Hive as well. You should avoid ingestion process that produces large number of small files. When producing outside Hive, use Text file format. Once you have data into Hive tables, then you can convert that to ORC or Parquet file format.\nChoose Appropriate File Format for the Data Choosing right file format will improve the Hive performance. Hive supports ORCfile, a new table storage format that sports fantastic speed improvements through techniques like predicate push-down, compression etc. ORC file increases esponse times for your HIVE queries.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/data/hive/2018-01-02-hive-tables-best-practices/",
	"title": "Hive Tables [Best Practices]",
	"tags": ["hive", "bigdata"],
	"description": "",
	"content": " Hive Table Storage Formats Avoid using TEXT format, Sequence file format or complex storage format such as JSON. Ideally, RCFile (Row Columnar File) or Parquet files are best suited. If you are building data warehouse on Hive, for better performance use Parquet file format.\nCREATE TABLE IF NOT EXISTS test_table ( col1 int, col2 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS PARQUET;  Compression Techniques Try to split compression algorithms provided by Hadoop \u0026amp; Hive like Snappy. Also you should avoid Gzip because it is not splittable and is CPU intensive.\nCREATE TABLE IF NOT EXISTS test_table ( col1 int, col2 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS PARQUET TBLPROPERTIES (\u0026quot;parquet.compress\u0026quot;=\u0026quot;SNAPPY\u0026quot;) \u0026quot;;  set hive.exec.compress.output=true; set hive.exec.compress.intermediate=true;  Partition Tables Partition will significantly reduce the table scan time. You should partition large Hive tables if you are collecting time series data. Hive Partition will store data in subdirectories like year/month/day. When you query the data, instead of scanning entire table, Hive will go to particular subdirectory and get you required data.\nBucketing Tables Hadoop Hive bucket concept is dividing Hive partition into number of equal clusters or buckets. Hive Buckets distribute the data load into user defined set of clusters. Hive distribute on hash code of key mentioned in query. Bucketing is useful when it is difficult to create partition on a column as it would be having huge variety of data in that column on which we want to run queries.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/implementing_vpc_peering/",
	"title": "Implementing VPC Peering",
	"tags": [],
	"description": "",
	"content": " Create a VPC Peer Ensure you are logged in to the AWS account, INSTANCE1, and INSTANCE2 using the cloud_user credentials provided.\n Change the NACL for Public2 Subnet - change ICMP from 0.0.0.0/0 to 10.0.0.0/13 Create a VPC peer from VPC1 to VPC2 Accept the VPC peer between VPC1 and VPC2  Configure Routing Ensure the VPC peer is created and active from Task 1:\n Locate the route tables associated with PublicSubnet1 and PrivateSubnet1 In each - Add a route for the CIDR of VPC2 and the target of the VPC Peer created in Task 1 Locate the route tables associated with PublicSubnet2 and PrivateSubnet2 In each - Add a route for the CIDR of VPC1 and the target of the VPC Peer created in Task 1 Obtain the privateIP for Instance2 and ping it from Instance1  Create VPC Peer Mesh Ensure the VPC Peer from Task 1 is created and active:\n Create and Accept a VPC peer from VPC2 to VPC3 Locate the route tables associated with PublicSubnet2 and PrivateSubnet2 In each - Add a route for the CIDR of VPC3 and the target of the VPC Peer created in Task 1 Locate the route tables associated with PublicSubnet3 and PrivateSubnet3 In each - Add a route for the CIDR of VPC2 and the target of the VPC Peer created in Task 1 Edit the NACL associated with the subnet Instance3 is in. Add a INGRESS rule allowing ICMP IPv4 from 10.0.0.0/13 Edit the NACL associated with the subnet Instance3 is in. Add a EGRESS rule allowing ICMP IPv4 to 10.0.0.0/13 Ping the privateIP of Instance3 from Instance2 - does it work? Why?  VPC peering isn\u0026rsquo;t transitive. A pair of peers from VPC1 \u0026lt;-\u0026gt; VPC2 and from VPC2 \u0026lt;-\u0026gt; VPC3 does not mean VPC1 and VPC3 can communicate.\n  Create and accept a VPC peer from VPC1 to VPC3 Locate the route tables associated with PublicSubnet1 and PrivateSubnet1 In each - Add a route for the CIDR of VPC3 and the target of the VPC Peer created in Task 1 Locate the route tables associated with PublicSubnet3 and PrivateSubnet3 In each - Add a route for the CIDR of VPC1 and the target of the VPC Peer created in Task 1 From Instance1 ping the privateIP of Instance3  DNS Over VPC Peer Ensure that the VPC peer created in Task 1, the routing from Task 2, and the VPC peer mesh and routing from Task 3 are all active:\n From the EC2 console locate the public DNS name and private DNS name for Instance2 From Instance1 ping the public hostname of Instance2, and it should return a public IP From the VPC peer options between VPC1 and VPC2 enable both DNS resolution check boxes If you wait a few minutes and ping the public DNS name of Instance2 from Instance1, what happens?  Enabling DNS support for VPC peers allows the private IP usage to be forced, if applications always use the instance DNS name.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/autoscaling/",
	"title": "Implementing an Auto Scaling Group and Application Load Balancer",
	"tags": [],
	"description": "",
	"content": " In this AWS hands-on lab, we will integrate two powerful AWS services: Elastic Load Balancers and Auto Scaling groups.\nSpecifically, we will create an Auto Scaling group of EC2 instances operating as web servers, and we\u0026rsquo;ll configure an Application Load Balancer to load balance between the instances inside that Auto Scaling group.\nAfter everything is set up, we will simulate stress tests on the EC2 instances to confirm the Auto Scaling group works as expected. This experience is good practice for building highly available and cost-efficient applications on AWS.\nLearning Objectives  Create an Application Load Balancer Create a Launch Template Create an Auto Scaling Group Test Horizontal Scaling  Create an Application Load Balancer Create an Application Load Balancer to load balance between EC2 instances you will create later on inside your Auto Scaling group:\n Navigate to the EC2 portion of the console. Click on the Load Balancers section under Load Balancing. Press the Create button under the Application Load Balancer. Name your load balancer \u0026rdquo;LABALB\u0026rdquo;, leave the ALB set to internet facing, and set the IP address type as ipv4. Select the VPC, and add the us-east-1a and us-east-1b AZs to your ALB (*not* the us-east-1c AZ). Create a new security group for your ALB, and use \u0026rdquo;ALBSG\u0026rdquo; for the name and description. Configure rules ensuring HTTP is allowed from 0.0.0.0/0 and ::/0 (IPV6). Configure a target group for your ALB, naming it ALBTG. Expand Advanced health check settings, and reduce the healthy threshold check down to 2. Proceed to create your ALB.  Create a Launch Template Create an SSH key pair that the EC2 instances will use to control access:\n Navigate to EC2 \u0026gt; Network \u0026amp; Security \u0026gt; Key Pairs. Click Create Key Pair. Call it ALBASG, and download the file to your local machine.  Create a security group for EC2 instances:\n Navigate to EC2 \u0026gt; Network \u0026amp; Security \u0026gt; Security Groups. Click Create Security Group. The name and description are ec2web. Set the VPC to the lab VPC. Add a rule allowing SSH from 0.0.0.0/0 and ::/0 (IPV6). Add a rule allowing HTTP from the Security Group ID of the ALB. Create the security group.  Create a launch template that will be used by the Auto Scaling group:\n Navigate to EC2 \u0026gt; Instances \u0026gt; Launch Templates. Create a new template, and call it \u0026rdquo;LABLT\u0026rdquo; for the name and description. The *Source Template* is none. Search for AMI, use the Quick Start lab catalog, and pick the Amazon Linux 2 AMI (64-bit x86). Set the instance type as t3.micro (not T3a.micro). Select the key pair created earlier. Network type is VPC. Select the ec2web security group created earlier. Storage should automatically be populated with a volume, so leave that as default and don\u0026rsquo;t add anything to the network section. Expand Advanced Details, and paste the user data for this lesson in the box. Click Create Launch Template. Click Close.  Create an Auto Scaling Group  EC2 \u0026gt; Auto Scaling \u0026gt; Auto Scaling Groups Click Create Auto Scaling group. Select Launch Template, and choose the template you just created. NOTE: If you receive the message No default VPC found, you can ignore it as we are selecting our VPC below. Call the group \u0026rdquo;LABASG\u0026rdquo;. Start with two instances. Pick the VPC from the lab environment, and select us-east-1a and us-east-1b as subnets. Click Next: Configure Scaling Policies. For now, keep the group at the initial size. Click Next: Configure Notifications \u0026gt; Next: Configure Tags \u0026gt; Review \u0026gt; Create Auto Scaling group \u0026gt; Close.  Note: Make sure the load balancer is ready at this point.\nEnable Group Metrics Collection  Click the Monitoring tab of the ASG. Click Enable Group Metrics Collection.  Edit the ASG to Allow Scaling and Use the ALB  Click Actions \u0026gt; Edit Set *Max instances* as 6**, click **Target groups, and pick the target group of the ALB. Click Save.  Configure Auto Scaling Group Scaling Policies  Select the Scaling Policies tab of the ASG. Click Add policy. Click Create a simple scaling policy. Name it SCALEOUT, set it to *Take the action* to Add 1 instances, *And then wait* 300 seconds before allowing another scaling activity. Click Create new alarm, and uncheck notification. Average CPU Utilization is \u0026gt;= 40 percent for 1 period of 5 minutes. Call it HIGHCPU. Click Create alarm and Close. Click Create. Click Add policy again. Click Create a simple scaling policy. Name it SCALEIN, set it to *Take the action* to Remove 1 instances, *And then wait* 300 seconds before allowing another scaling activity. Click Create new alarm, and uncheck notification. Average CPU utilization is \u0026lt;= 20 percent for 1 period of 5 minutes. Call it LOWCPU. Click Create alarm \u0026gt; Close \u0026gt; Create.  Test Horizontal Scaling  Connect to one of the EC2 instances. Install the stress test application:\nsudo amazon-linux-extras install epel -y sudo yum install -y stress  Run the stress test on the EC2 instance:\nstress --cpu 2 --timeout 300  (Optionally, use 3000 for timeout.)\n After a few minutes, watch the number of instances increase.\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/docker/2017-09-22-install-docker-centos-7/",
	"title": "Installing Docker CE on Centos 7",
	"tags": [],
	"description": "",
	"content": "  Install Docker Community Edition Verify  Install Docker Community Edition sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast sudo yum install docker-ce sudo systemctl start docker  Verify sudo docker run hello-world  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/",
	"title": "Introduction-To-Kubernetes-LFS158",
	"tags": [],
	"description": "",
	"content": "  00 - Containers and Orchestration   01 - Kubernetes Architecture   02 - Installing Kubernetes   03 - Minikube   04 - Kubernetes Building Blocks   05 - Authentication, Authorization, and Admission Control   06 - Services   07 - Deploying Standalone App   08 - Kubernetes Volume Management   09 - ConfigMaps and Secrets   10 - Ingress   11 - Advanced Topics   12 - Resources   Kubernetes   "
},
{
	"uri": "http://learn.aayushtuladhar.com/java/",
	"title": "Java",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": "  Arrow Functions   Destructuring JavaScript Objects   Eloquent JavaScript   Getting Started with Redux   Spread Syntax   Teplate Literals   "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/eloquent_js/revisited/",
	"title": "JavaScript Revisited",
	"tags": ["JavaScript"],
	"description": "",
	"content": " JavaScript  Language of the Web Introduced in 1995 as a way to add programs to web pages ECMAScript Standard  Values, Types and Operators  Inside the computer\u0026rsquo;s world, there is only data. You can read data, modify data, create new data but anything that isn\u0026rsquo;t data simple does not exist.\n DataTypes  Numbers Strings  Single Quotes or Double Quotes Escaping using \\n String Concatenation (+)  Boolean Undefined Values  Null Undefined   Operators  Binary -\u0026gt; Takes Two Values Unary -\u0026gt; Takes One Value  Arithmetic Operators  Addition (+) Substraction (-) Multiplication (*) Division (/) Modulus (%)  Comparison Operator  Greater Than (\u0026gt;) Less Than (\u0026lt;) Greater Than or Equal To (\u0026gt;=) Less Than or Equal To (\u0026lt;=) Equal to (==) Not Equal to (!=) Equality (===)  Logical Operators  AND (\u0026amp;\u0026amp;) OR (||) NOT (!) Conditional Operators / Ternary Operator ( ?: )  Automatic Type Conversion Automatic Typc Converstion using Type Coercion\nJavaScript Program Structure  Expressions and Statements\n Javascript program is built out of statements, which themselves contains statements. Statements tend to contain expressions.  Variables\n Variables can be used to file piece of data under a name.  Functions\n Functions are special values that encapsulate piece of program.   Control Flow If..Then, Else if (expression1) statement1; else if (expression2) statement2; else statement3;  Example\nlet num = Number(prompt(\u0026quot;Enter a number\u0026quot;)); if (num \u0026lt;= 10) { alert(\u0026quot;Too Low\u0026quot;); } else if (num \u0026gt; 10 \u0026amp;\u0026amp; num \u0026lt;= 50) { alert(\u0026quot;Bingo !! Correct Answer\u0026quot;); } else { alert(\u0026quot;Way Too High\u0026quot;); }  While and Do Loops while (expression) { statement }  Example\nlet i=1; while (i\u0026lt;=10) { console.log(i); i++; }  For Loops for (let i=0; i\u0026lt;=10; i++ ){ statement }  Example\nlet result; for (let i=0; i\u0026lt;=10; i++) { result += i; } console.log(result);  Breaking Out of Loops  break keyword can be used to break out of loop continue keyword can be used to continue with next loop\u0026rsquo;s next execution  Switch let choice = prompt(\u0026quot;Enter option [A], [B], [C], [D]\u0026quot;); switch (choice) { case 'A': console.log(\u0026quot;A Selected\u0026quot;); break; case 'B': console.log(\u0026quot;B Selected\u0026quot;); break; case 'C': console.log(\u0026quot;C Selected\u0026quot;); break; case 'D': console.log(\u0026quot;D Selected\u0026quot;); break; default: console.log(\u0026quot;Unknown Option Selected\u0026quot;); break; }  Comments  Single Line Comment\n// Sinle Line Comment /* Example of Multiline Comment */   Functions  Functions are the Bread and Butter of JavaScript\n  Function definition is a regular variable definition where the value of the variable is a function. Function is created by an expression with the keyword function A return statement determines the value the function returns Parameters and variables declared inside a function are local to the function, re-created every time the function is called, and not visible from the outside Functions can take optional arguments but if the function is not using those arguments, it simiply ignores as undefined  Defining Function let fn_sayHello = function(name) { return \u0026quot;Hello \u0026quot; + name; } alert(fn_sayHello(\u0026quot;User\u0026quot;));  Declaration Notation Declaring fn_sayHello to be a function\nfunction fn_sayHello(name) { return \u0026quot;Hello \u0026quot; + name; }  Closure in function  A function that \u0026ldquo;closes over\u0026rdquo; some local letiable is called a Closure.\n function multiplier(factor) { return function(number) { return factor * number; } } let twice = multiplier(2); console.log(twice(5)); \u0026gt;\u0026gt;10  Arrays //Array.forEach function logger (element, index) { console.log(\u0026quot;Index: \u0026quot; + index + \u0026quot;: \u0026quot; + element); } let countries = [\u0026quot;India\u0026quot;, \u0026quot;Nepal\u0026quot;, \u0026quot;China\u0026quot;] countries.forEach(logger); //Array.filter function filterAge(element) { return element.age \u0026gt;= 21; } let people = [ {name: \u0026quot;Aayush\u0026quot;,age: 28}, {name: \u0026quot;Aadesh\u0026quot;,age: 18}, {name: \u0026quot;Bob\u0026quot;,age: 52} ] let peopleLegal = people.filter(filterAge); console.log(peopleLegal);  Objects  Enclosed in Curly Braces Consists of Properties and Methods  Properties are accessed using obj.property1 or obj[\u0026ldquo;property1\u0026rdquo;]   let obj = { property1: \u0026quot;Value1\u0026quot;, property2: \u0026quot;Value2\u0026quot;, method1: function(args){ console.log(args); } }  Looping Over Properties let obj = {name: \u0026quot;A\u0026quot;, date: new Date(), scores: [1, 2, 3]}; for (let p in obj) { if (obj.hasOwnProperty(p)) { console.log(p + ': ' + object[p]); } }  String  Strings can be Single or Double Quoted String indexes are zero based Immutable  let str_message = \u0026quot;Welcome to JavaScript\u0026quot;; let str_Hello = 'I am String to '; console.log( str_message.slice(0,5) ); console.log( str_message.indexOf(\u0026quot;to\u0026quot;) ); console.log( str_Hello.indexOf(\u0026quot;to\u0026quot;) ); console.log( str_Hello.trim() );  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/ci-cd/2019-08-20-jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "  Installing Plugins  Pipeline Script   Installing Jenkins docker pull jenkins/jenkins # Persist Jenkins Data within the Docker Container docker volume create jenkins-data docker run --name jenkins-production \\ --detach \\ -p 50000:50000 \\ -p 8080:8080 \\ -v jenkins-data:/var/jenkins_home \\ jenkins/jenkins:2.164.2  Access Jenkins at http://localhost:8080/\nInstalling Plugins  Blueocean plugin  Manage Jenkins \u0026gt; Manage Plugins\nPipeline Script Hello World Pipeline Script\npipeline { agent none environment { APPLICATION_NAME = 'hello-jenkins-pipeline' } stages { stage('build') { steps { echo \u0026quot;Hello World\u0026quot; } } } }  Pipeline script to Build Another Jenkins Job\npipeline { agent none stages{ stage('build'){ steps { echo \u0026quot;Hello World\u0026quot; build 'project2-child' } } } }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/ci-cd/2019-09-13-jenkins-shared-libs/",
	"title": "Jenkins Shared Library",
	"tags": [],
	"description": "",
	"content": " Jenkins Shared library is the concept of having a common pipeline code in the version control system that can be used by any number of pipeline just by referencing it. In fact, multiple teams can use the same library for their pipelines. Pipeline has support for creating \u0026ldquo;Shared Libraries\u0026rdquo; which can be defined in external source control repositories and loaded into existing Pipelines.\n A shared library is a collection of independent Groovy scripts which you pull into your Jenkinsfile at runtime.\n Getting Started with Shared Library A Shared Library is defined with a name, a source code retrieval method such as by SCM, and optionally a default version. The name should be a short identifier as it will be used in scripts.\nDirectory structure jenkins-shared-library |____vars |____src |____resources  Resources  https://devopscube.com/create-jenkins-shared-library/ https://tomd.xyz/articles/jenkins-shared-library/ https://dev.to/kuperadrian/how-to-setup-a-unit-testable-jenkins-shared-pipeline-library-2e62 https://tomd.xyz/articles/jenkins-shared-library/  "
},
{
	"uri": "http://learn.aayushtuladhar.com/data/2018-05-10-jupyter-notebooks/",
	"title": "Jupyter NoteBook - Shortcuts",
	"tags": [],
	"description": "",
	"content": " Jupyter Shortcuts Command Mode gives to the ability to create, copy, paste, move, and execute cells. A few keys to know: To enter Command Mode (control + m)\n   Keywork Description     h Bring up help (ESC to dismiss)   b Create cell below   a Create cell above   c Copy cell   v Paste cell below   Enter Go into Edit Mode   m Change cell type to Markdown   y Change cell type to code   ii Interrupt kernel   oo Restart kernel    "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2016-04-05-chef-knife-commands/",
	"title": "Knife Commands",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_intro/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. This tutorial provides an overview of different kind of features and functionalities of Kubernetes and teaches how to manage the containerized infrastructure and application deployment.\nFeatures Automatic bin packing\nKubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability.\nSelf-healing\nKubernetes automatically replaces and reschedules containers from failed nodes. It kills and restarts containers unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers.\nHorizontal scaling\nWith Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization.\nService discovery and Load balancing\nContainers receive their own IP addresses from Kubernetes, white it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set.\nAutomated rollouts and rollbacks\nKubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application\u0026rsquo;s health to prevent any downtime.\nSecret and configuration management\nKubernetes manages secrets and configuration details for an application separately from the container image, in order to avoid a re-build of the respective image. Secrets consist of confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub.\nStorage orchestration\nKubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, or network storage systems.\nBatch execution\nKubernetes supports batch execution, long-running jobs, and replaces failed containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": "  Certified Kubernetes Application Developer (CKAD)   Introduction-To-Kubernetes-LFS158   "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab1-creating-kubernetes-pods/",
	"title": "Lab1 - Creating Kubernetes Pod",
	"tags": [],
	"description": "",
	"content": "nginx.yml\napiVersion: v1 kind: Pod metadata: name: nginx namespace: web spec: containers: - name: nginx image: nginx command: ['nginx'] args: ['-g', 'daemon off;', '-q'] ports: - containerPort: 80  ## Applying Spec Definition for Creating Pod kubectl -n web apply -f nginx.yml ## Verifying Pod running in web namespace kubectl get pods -n web  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab2-configuring-kubernetes-pods/",
	"title": "Lab2 - Configuring Kubernetes Pod",
	"tags": [],
	"description": "",
	"content": " Create a ConfigMap called candy-service-config to store the container\u0026rsquo;s configuration data. candy-service-config.yml\napiVersion: v1 kind: ConfigMap metadata: name: candy-service-config data: candy.cfg: |- candy.peppermint.power: 100000000 candy.nougat-armor.strength: 10  Create a Kubernetes secret called db-password to store the database password. candy-service-secret.yml\napiVersion: v1 kind: Secret metadata: name: db-password stringData: db-password: Kub3rn3t3sRul3s!  Create the pod for the candy-service application according to the provided specification. candy-service-pod.yml\napiVersion: v1 kind: Pod metadata: name: candy-service spec: securityContext: fsGroup: 2000 containers: - name: candy-service image: linuxacademycontent/candy-service:1 volumeMounts: - name: candy-service-volume mountPath: /etc/candy-service env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-password key: db-password resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; volumes: - name: candy-service-volume configMap: name: candy-service-config  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab3-forwarding-port-traffic/",
	"title": "Lab3 - Forwarding Port Traffic with an Ambassador Container",
	"tags": [],
	"description": "",
	"content": "fruit-service-ambassador-config.yml\n# Config Map for HA Proxy apiVersion: v1 kind: ConfigMap metadata: name: fruit-service-ambassador-config data: haproxy.cfg: |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8775 maxconn 32  fruit-service-pod.yml\n# Pod Definition for fruit-service apiVersion: v1 kind: Pod metadata: name: fruit-service spec: containers: - name: fruit-service-container image: linuxacademycontent/legacy-fruit-service:1 - name: fruit-service-ambassador image: haproxy:1.7 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /usr/local/etc/haproxy volumes: - name: config-volume configMap: name: fruit-service-ambassador-config  busybox.yml\n# BusyBox Pod for Testing apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: myapp-container image: radial/busyboxplus:curl command: ['sh', '-c', 'while true; do sleep 3600; done']  Running a Curl command from the BusyBox\nkubectl exec busybox -- curl $(kubectl get pod fruit-service -o=custom-columns=IP:.status.podIP --no-headers):80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/java/the-basics/lambda_expressions/",
	"title": "Lambda Expressions",
	"tags": [],
	"description": "",
	"content": " Lambda expressions are not unknown to many of us who have worked on other popular programming languages like Scala. In Java programming language, a Lambda expression (or function) is just an anonymous function, i.e., a function with no name and without being bounded to an identifier. They are written exactly in the place where it’s needed, typically as a parameter to some other function.\nThe most important features of Lambda Expressions is that they execute in the context of their appearance. So, a similar lambda expression can be executed differently in some other context (i.e. logic will be same but results will be different based on different parameters passed to function).\nSynatax // This function takes two parameters and returns their sum //(parameters) -\u0026gt; expression (x, y) -\u0026gt; x + y (int a, int b) -\u0026gt; a * b //(parameters) -\u0026gt; { statements; } (x, y) -\u0026gt; { x+y; } //() -\u0026gt; expression () -\u0026gt; z () -\u0026gt; 100  Functional Interface A functional interface is an interface with a single abstract method (SAM). A class implements any interface by providing implementations for all the methods in it.\n@FunctionalInterface public interface Runnable { public abstract void run(); }  Type in which lambda expressions are converted, are always of functional interface type.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_2/",
	"title": "Layouts",
	"tags": [],
	"description": "",
	"content": " Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading  Renders to:\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading Typography I am just being Bold\nI Love my Italics Style\nStrike Through\n I love to give a quotation\n Images ![Minion](https://octodex.github.com/images/minion.png)  Resizing Images ![Minion](https://octodex.github.com/images/minion.png?width=20pc)  Buttons Get Grav  Note / Info/ Tip / Warning A notice disclaimer\n An information disclaimer\n A tip disclaimer\n A warning disclaimer\n Expand   Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/",
	"title": "Learning Notebook",
	"tags": [],
	"description": "",
	"content": " My Digital Learning Notebook Welcome to my digital notebook where I share my learnings and capture important notes regarding various aspects of Software Engineering, Web Development and Cloud Engineering and other miscellenious topics.\nTopics  Amazon Web Services   Architecture   Big Data   DevOps   Google Cloud Platform   Java   JavaScript   Kubernetes   Terraform   Testing   Using Hugo   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/linux/2016-11-27-issue-remote-commands/",
	"title": "Linux Command Line Hacks",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-02-26-mqtt/",
	"title": "Mqtt",
	"tags": [],
	"description": "",
	"content": " Introduction  MQTT is a featherweight, ISO complaint PUB-SUB messaging protocol. Designed for low powered devices PRAM consistent: Guaranteed in-order delivery per-publisher Multiple Transport: TCP, TLS, Websockets Flexible: Arbitrary message up to 256 MB Topics can also be used for Key-Value storage  Topic based Pub/Sub  Decouples Publisher and Subscribers  Quality of Service  QoS 0 - \u0026ldquo;Fire and Forget\u0026rdquo; Q0S 1 - \u0026ldquo;At least once\u0026rdquo; QoS 2 - \u0026ldquo;Exactly once; 2 phase commit\u0026rdquo;  Ideal for intermittent connectivity; Sessions may last weeks or months Supports Disconnect \u0026amp; Last Will \u0026amp; Testament message\n MQTT supports Retained messages which are automatically delivered when a client subscribes to a topic.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/network/2016-10-01-network-command-essentials/",
	"title": "Network Tools / Command Essentials",
	"tags": [],
	"description": "",
	"content": " Must Know Network Tools / Commands NetStat netstat stands for network statistics. This command displays incoming and outgoing network connections as well as other network information. The netstat utility can show you the open connections on your computer, which programs are making which connections, how much data is being transmitted, and other information.\n## List all connections netstat -a ## List TCP or UDP Connections netstat -at //TCP Connections netstat -au //UDP Connections ## List all Ports being Listened to netstat -an | grep \u0026quot;LISTEN \u0026quot;  IpTables ## Open 9001 Port sudo iptables -A INPUT -p tcp --dport 9001 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 9001 -m conntrack --ctstate ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp --dport 3306 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 3306 -m conntrack --ctstate ESTABLISHED -j ACCEPT  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-10-8-owasp/",
	"title": "OWASP Dependency Check",
	"tags": [],
	"description": "",
	"content": " OWASP dependency-check is an open source solution that can be used to scan Java and .NET applications to identify the use of known vulnerable components.\nLink\nAdding OWASP Check to Gradle Projects Adding following fragments to build.gradle\nbuildscript { repositories { mavenCentral() } dependencies { classpath 'org.owasp:dependency-check-gradle:5.2.2' } } plugins { id 'org.owasp.dependencycheck' version '5.2.2' }  Gradle Task\n./gradlew dependencyCheckAggregate  Configuring DependencyCheck dependencyCheck { format='ALL' cveValidForHours=1 outputDirectory = file(\u0026quot;$project.buildDir/reports/dependencycheck\u0026quot;) suppressionFile = 'config/dependencyCheck/suppressions.xml' failBuildOnCVSS = 5 failOnError = true }  Sample Suppressions.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;suppressions xmlns=\u0026quot;https://jeremylong.github.io/DependencyCheck/dependency-suppression.1.1.xsd\u0026quot;\u0026gt; \u0026lt;suppress\u0026gt; \u0026lt;notes\u0026gt;\u0026lt;![CDATA[file name: postgresql-42.2.5.jar]]\u0026gt;\u0026lt;/notes\u0026gt; \u0026lt;gav regex=\u0026quot;true\u0026quot;\u0026gt;^org\\.postgresql:postgresql:.*$\u0026lt;/gav\u0026gt; \u0026lt;cve\u0026gt;CVE-2016-7048\u0026lt;/cve\u0026gt; \u0026lt;/suppress\u0026gt; \u0026lt;/suppressions\u0026gt;  Resources  Dependency Check Gradle  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-03-24-open-stack/",
	"title": "OpenStack",
	"tags": [],
	"description": "",
	"content": " Getting Started with OpenStack What is OpenStack OpenStack is elastic cloud software that provides software developers with the ability to control the virtual infrastructure on which to deploy their applications. It is a set of software tools for building and managing cloud computing platforms for public and private clouds.\nIt accelerates time-to-market by dramatically reducing application provisioning times, giving companies full control of their software development lifecycle and ultimately giving them a significant competitive advantage. OpenStack also provides application portability by allowing enterprises to freely move between OpenStack clouds without vendor lock-in.\nIntroduction to OpenStack OpenStack lets users deploy virtual machines and other instances that handles different tasks for managing a cloud environment on the fly.\nKey Components of OpenStack  Nova - Primary Computing Engine behind OpenStack. It is used for deploying and managing large numbers of virtual machines and other instances to handle computing tasks.\n Swift - Storage System for Objects and Files.\n   Network Commands ### List floating IP Pools openstack ip floating pool list ### Get FloatingIP from the external Network openstack ip floating create \u0026lt;poolname\u0026gt; openstack ip floating create ext_vlan1767_net ### Create a Neutron Network neutron net-create ART-Land #### View list of Networks neutron net-list #### Create network for project neutron net-create \u0026lt;NetworkName\u0026gt; #### Creating Subnet neutron subnet-create --name ART-Subnet ART-Land 192.168.10.0/24 #### View Subnet neutron subnet show \u0026lt;subnetName\u0026gt; neutron subnet-show ART-Subnet ### Create Router neutron router-create \u0026lt;routername\u0026gt; neutron router-create ART-router ### Add Interface to router neutron router-interface-add \u0026lt;router\u0026gt; \u0026lt;subnet\u0026gt; neutron router-interface-add ART-router ART-Subnet ### Attach Router to External Network neuter router-gateway-set ART-router ext_vlan1767_net # Security Groups and Rules ### Creating Security Group openstack security group \u0026lt;groupname\u0026gt; --description 'Allow SSH and pings' openstack security group create BasicSG --description 'Allow SSH and Pings' ### Adding Rules to Security Groups #### Create Rule to allow SSH openstack security group rule create BasicSG --proto tcp --dst-port 22:22 #### Create Rule to allow Ping from any Source IP openstack security group rule create BasicSG --proto icmp --dst-port -1 ### Create Security Group to Open all TCP internal openstack security group create OpenSG --description 'All TCP internal' #### Add a new security rule to our OpenSG group that will allow all traffic from the private subnet in openstack security group rule create OpenSG --proto tcp --dst-port 1:65535 --src-ip 192.168.10.0/24  Creating Instances openstack server create --flavor smem-4vcpu --image ubuntu-latest --security-group BasicSG --nic net-id=ART-Land --key-name ArtKey ARTBastionHost  Working with Stacks ### Launching a Stacks openstack stack create --template \u0026lt;templateFile\u0026gt; --environment \u0026lt;envFile\u0026gt; \u0026lt;stackName\u0026gt; ### Listing Stacks openstack stack list openstack flavor list openstack image list  Neutron  Provides API to allow your users to create networks, subnets, routers etc.  Network - An isolated L2 segment, analogous to a VLAN in physical networking.\nSubnet - A block of v4 of v6 IP address and associated configuration state.\nPort - A connection point for attaching a single device (NIC) to Neutron network.\nopenstack network list # Create Network openstack network create mynetwork # Create Subnet openstack subnet create --network mynetwork \\ --subnet-range 10.0.0.0/29 --dns-nameserver 8.8.8.8 mynetwork-subnet  Nova Nova sits on top of Hypervisor Nova Flavors\n# List images openstack image list # List flavors openstack flavor list # List networks openstack network list # Boot an instance using flavor and image names (if names are unique) openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyFirstInstance # Boot another instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MySecondInstance # List instances, notice status of instance openstack server list # Show details of instance openstack server show MyFirstInstance # View console log of instance openstack console log show MyFirstInstance # Get the console url of the instance openstack console url show MyFirstInstance  Cinder Creates a volume on a Hypervisor. Allowing instances to attaching and detaching the volume without losing it\u0026rsquo;s persistence.\nCinder is powered by LVM and iSCSI. Cinder is Inspired by Elastic Block Storage.\nopenstack volume create --size 1 --type sata MyFirstVolume openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID  Use \u0026lsquo;myadmin\u0026rsquo; credentials source ~/credentials/myadmin\nVerify Cinder services are functioning and checking in :-) openstack volume service list  Use \u0026lsquo;myuser\u0026rsquo; credentials source ~/credentials/myuser  Create a new volume openstack volume create --size 1 --type sata MyFirstVolume  Boot an instance to attach volume to openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance  List instances, notice status of instance openstack server list  List volumes, notice status of volume openstack volume list  Get volume and instance IDs MYFIRSTVOLUME_ID=`openstack volume show MyFirstVolume | awk '/ id / { print $4 }'` MYVOLUMEINSTANCE_ID=`openstack server show MyVolumeInstance | awk '/ id / { print $4 }'` # Attach volume to instance after instance is active, and volume is available openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # Confirm the volume has been attached openstack volume list # Get the console url of the instance openstack console url show MyVolumeInstance # Login to the instance # username: cirros # password: cubswin:) # From inside the instance # List storage devices sudo fdisk -l # Make filesystem on volume sudo mkfs.ext3 /dev/vdb # Create a mountpoint sudo mkdir /extraspace # Mount volume at mountpoint sudo mount /dev/vdb /extraspace # Create a file on volume sudo touch /extraspace/helloworld.txt sudo ls /extraspace # Unmount volume sudo umount /extraspace # Log out of instance exit # Detach volume from instance openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # List volumes, notice status of volume openstack volume list # Delete instance openstack server delete MyVolumeInstance  # Use 'myuser' credentials source ~/credentials/myuser # Create a new instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyLastInstance --wait # Get the console url of the instance openstack console url show MyLastInstance # Login to the instance # username: cirros # password: cubswin:) # Ping openstack.org ( should fail, as we have nothing routing between the tenant network (private) and provider network (public) ) ping openstack.org # Create a router openstack router create MyRouter # Connect the private-subnet to the router openstack router add subnet MyRouter private-subnet # List interfaces attached to router openstack port list --router MyRouter # Connect router to public network neutron router-gateway-set MyRouter public # Examine details of router openstack router show MyRouter # Get instance ID for MyLastInstance MYLASTINSTANCE_ID=`openstack server show MyLastInstance | awk '/ id / { print $4 }'` # Find port id for instance MYLASTINSTANCE_IP=`openstack server show MyLastInstance | awk '/ addresses / { print $4 }' | cut -d '=' -f 2` MYLASTINSTANCE_PORT_ID=`openstack port list --device-owner compute:None | awk ' /'$MYLASTINSTANCE_IP'/{print $2}'` # Create a floating IP and attach it to instance openstack floating ip create --port $MYLASTINSTANCE_PORT_ID public # Create a new security group openstack security group create remote # Add rules to security group to allow SSH and ping openstack security group rule create --proto icmp --src-ip 0.0.0.0/0 remote openstack security group rule create --proto tcp --dst-port 22 --src-ip 0.0.0.0/0 remote # Apply security group to instance with floating IP openstack server add security group MyLastInstance remote # Ping instance with floating IP ping 172.16.0.12 # Delete all your servers openstack server delete MyLastInstance  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-08-16-openshift/",
	"title": "Openshift",
	"tags": [],
	"description": "",
	"content": "  Architecture Containers and Image  Container Registries  Pods and Services  Pods Services Labels  Builds and Image Streams  Builds Image Stream Image stream tag Image stream image Image stream trigger Templates  References Cheatsheet  Architecture OpenShift is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as acurately as possible, with a focus on easy composition of applications by a developer.\nOpenShift Container Platform (OCP) has a microservices based architecture of smaller, decoupled units that work together. It runs on top of Kubernetes cluster, with data about the objects stored in etcd, a realible key-value store.\n REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object.  Containers and Image The basic units of OpenShift Container Platform applications are called Containers. Linux container technologies are lightweight mechanism for isolating running processess so that they are limited to interacting with only their designated resources.\nContainers in OpenShift Container Platform are based on Docker formatted container Images. An image is a binary that includes all of the requirements for running a single container, as well as metadata describring its needs and capabilities.\nContainer Registries A container registry is a service for storing and retreving Docker formatted container images. A registry contains a collection of one or more image repositories. Each image repository contains one of more tagged images. Docker provides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a registry at registry.access.redhat.com for subscribers. OpenShift Container Platform can also supply its own internal registry for managing custom container images.\nPods and Services Pods OpenShift Enterprise leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.\nPods are the rough equivalent of a machine instance (physical or virtual) to a container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.\n# Get All Pods within a Namespace oc get pods # Get Pod information in YAML oc get pod hello-node-1-nz525 -o yaml # Get Pods Details using Label oc describe pods -l app=hello-node  Services A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.\nServices are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. A service uses a label selector to find all the containers running that provide a certain network service on a certain port.\nLabels Labels are used to organize, group, or select API objects. For example, pods are \u0026ldquo;tagged\u0026rdquo; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.\nMost objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.\nlabels: key1: value1 key2: value2  Builds and Image Streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process.\nOpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container registry.\nThe OpenShift build system provides extensible support for build strategies that are based on selectable types specified in the build API. There are three build strategies available:\n Docker build Source-to-Image (S2I) build Custom build  By default, Docker builds and S2I builds are supported.\nImage Stream An OpenShift Container Platform object that contains pointers to any number of Docker-formatted container images identified by tags. You can think of an image stream as equivalent to a Docker repository.\nImage stream tag A named pointer to an image in an image stream. An image stream tag is similar to a Docker image tag. See Image Stream Tag below.\nImage stream image An image that allows you to retrieve a specific Docker image from a particular image stream where it is tagged. An image stream image is an API resource object that pulls together some metadata about a particular image SHA identifier. See Image Stream Images below.\nImage stream trigger A trigger that causes a specific action when an image stream tag changes. For example, importing can cause the value of the tag to change, which causes a trigger to fire when there are Deployments, Builds, or other resources listening for those. See Image Stream Triggers below.\n# Get OpenShift from Project Openshift oc get is -n openshift oc describe is jenkins -n openshift # Get Image Stream Definition in YAML oc get is jenkins -o yaml  Templates A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift. A template can be processed to create anything you have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.\nYou can create a list of objects from a template using the CLI or, if a template has been uploaded to your project or the global template library, using the web console.\n# Get All Templates from a project oc get templates -n openshift # Create Template from YAML file oc create -f \u0026lt;filename\u0026gt;  References  Core Concepts - Builds and Image Streams\n CLI Reference - Basic CLI Operation\n Core Concepts - Architecture\n Core Concepts - Builds and Image Streams\n  Cheatsheet  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/",
	"title": "Orchestrating Cloud with Kubernetes",
	"tags": [],
	"description": "",
	"content": " # Creating Kubernetes Cluster gcloud container clusters create io  Quick Demo # Create Deployment kubectl create deployment nginx --image=nginx:1.10.0 # List Pods kubectl get pods # Expose Deployment via a Service using LoadBalancer kubectl expose deployment nginx --port 80 --type LoadBalancer # List Service kubectl get services  Pods Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods\n# Create Pod kubectl create -f pods/monolith.yaml # List Pods kubectl get pods # Describe Pod kubectl describe pods monolith # Port Forward Pod kubectl port-forward monolith 10080:80  Services An abstract way to expose an application running on a set of Pods as a network service.\nNo need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives pods their own IP addresses and a single DNS name for a set of pods, and can load-balance across them.\nPods aren\u0026rsquo;t meant to be persistent. They can be stopped or started for many reasons - like failed liveness or readiness checks - and this leads to a problem\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/rest_api_with_api_gateway/",
	"title": "REST API with",
	"tags": [],
	"description": "",
	"content": " IAM Setup Create Policy\nCreate Policy with name lambda_execute with following JSON, which will allow to perform lambda:InvokeFunction on all resources\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;lambda:InvokeFunction\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }  Create an IAM role lambda_invoke_function_assume_apigw_role\nEdit Trust Policy\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: [ \u0026quot;lambda.amazonaws.com\u0026quot;, \u0026quot;apigateway.amazonaws.com\u0026quot; ] }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] }  Creating Lambda Function\nServices \u0026gt; Lambda Enter name \u0026ldquo;Calc\u0026rdquo; Choose Create Function.\nAdd following code,\n console.log('Loading the Calc function'); exports.handler = function(event, context, callback) { console.log('Received event:', JSON.stringify(event, null, 2)); if (event.a === undefined || event.b === undefined || event.op === undefined) { callback(\u0026quot;400 Invalid Input\u0026quot;); } var res = {}; res.a = Number(event.a); res.b = Number(event.b); res.op = event.op; if (isNaN(event.a) || isNaN(event.b)) { callback(\u0026quot;400 Invalid Operand\u0026quot;); } switch(event.op) { case \u0026quot;+\u0026quot;: case \u0026quot;add\u0026quot;: res.c = res.a + res.b; break; case \u0026quot;-\u0026quot;: case \u0026quot;sub\u0026quot;: res.c = res.a - res.b; break; case \u0026quot;*\u0026quot;: case \u0026quot;mul\u0026quot;: res.c = res.a * res.b; break; case \u0026quot;/\u0026quot;: case \u0026quot;div\u0026quot;: res.c = res.b===0 ? NaN : Number(event.a) / Number(event.b); break; default: callback(\u0026quot;400 Invalid Operator\u0026quot;); break; } callback(null, res); };  Under Execution role, choose Choose an existing role and enter the ARN for the role we just created earlier lambda_invoke_function_assume_apigw_role\nChoose Save.\nTesting the Lambda Function { \u0026quot;a\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;b\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;op\u0026quot;: \u0026quot;+\u0026quot; }  Setup API Gateway Services \u0026gt; API Gateway\nMy Calculator\nUnder Resources Section, Click on the Actions \u0026gt; Create Resource.\nResource Name: calc Resource Path: /calc\nSelect the /calc resource you just created. In the Actions drop down menu, choose Create Method.\nIn the method drop down menu, choose GET\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/study-guide/resources/",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " AWS Well-Architected Amazon Elastic Compute Cloud Documentation Introduction to AWS Lambda \u0026amp; Serverless Applications  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "  Understanding SSH Keys   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-26-setting-apache-virtual-host/",
	"title": "Setting Apache Virtual Host",
	"tags": [],
	"description": "",
	"content": " Install Apache WebServer (Pre-Req) sudo apt-get update sudo apt-get install apache2  Apache Virtual Host To run more than one site on a single machine, you need to setup virtual hosts for sites your plan to host on an apache server.\nName Based Virtual Hosts (Most Common)  The server relies on the client to report the hostname as part of the HTTP headers. Using this technique, many different hosts can share the same IP address.  IP Based Virtual Hosts  IP-based virtual hosting is a method to apply different directives based on the IP address and port a request is received on. Most commonly, this is used to serve different websites on different ports or interfaces.  Setting up Virtual Directories sudo mkdir -p /var/www/aayushtuladhar.com/public_html sudo chown -R $USER:$USER /var/www/aayushtuladhar.com/public_html  Creating Virtual Host Files sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/aayushtuladhar.com.conf \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin admin@aayushtuladhar.com ServerName aayushtuladhar.com ServerAlias aayushtuladhar.com *.aayushtuladhar.com DocumentRoot /var/www/aayshtuladhar.com/public_html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;/VirtualHost\u0026gt;   ServerName - Base Domain that should match for the virtual host definition. ServerAlias - Lists names are other names which people can use to see that same web site.  Enable Virtual Host sudo a2ensite example.com.conf\nDisable Default Virtual Host sudo a2dissite 000-default.conf\nRestart Apache sudo service apache2 restart # Ubuntu 15.10 sudo systemctl restart apache2 # Ubuntu 14.10 and Earlier  Reference  http://www.unixmen.com/setup-apache-virtual-hosts-on-ubuntu-15-10/ https://httpd.apache.org/docs/2.4/vhosts/name-based.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/network/2016-03-01-setting-fqdn/",
	"title": "Setting FQDN",
	"tags": [],
	"description": "",
	"content": "  FQDN  Finding FQDN  Hostname  Finding Hostname  Setting hostname and FQDN  FQDN FQDN stands for Fully Qualified Domain Name. It is a domain name that specifies its exact location in the tree hierarhcy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone.\nExample, somehost.example.com\nFinding FQDN hostname -f  Hostname A hostname is a label that is assigned to a device connected to a computer network and that is used to identify the device. Example, kafka-dev\nFinding Hostname  $ hostname AayushTuladhar-Mac.local  Setting hostname and FQDN  Update /etc/hosts\n127.0.0.1 packer-20140710152503.aayushtuladhar.com packer-20140710152503 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 \u0026lt;ip-addr\u0026gt; kafka-dev.aayushtuladhar.com kafka-dev  Update /etc/sysconfig/network\n[vagrant@kafka-dev ~]$ cat /etc/sysconfig/network NETWORKING=yes HOSTNAME=kafka-dev.aayushtuladhar.com NOZEROCONF=yes  Restart the Hostname Service\n/etc/init.d/network restart  Verify Hostname and FQDN\nhostname hostname -f   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-02-05-setting-selenium-grid/",
	"title": "Setting Selenium Grid",
	"tags": [],
	"description": "",
	"content": " Selenium Grid enables you to spread your tests across multiple machines and multiple browsers, which allows your to run tests in parallel. Also having Hub as central point of communication handles all the driver configuration and runs them automatically.\nDownloading Selenium http://docs.seleniumhq.org/download/\nSetting up Hub Once you have the jarfile downloaded from the Selenium Website,\njava -jar selenium-server-standalone-2.x.x.jar –role hub\nThis starts up a jetty server on default port 4444. Grid Console can be viewed at http://localhost:4444/grid/console\nSetting up Nodes Nodes are the actual machine which performs the tests,\njava –jar selenium-server-standalone-2.x.x.jar –role node –hub http://hubIP:4444/grid/register\nIE, Chrome, Safari \u0026amp; firefox selenium NODE java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot; -browser \u0026quot;browserName=internet explorer, version=11, platform=WINDOWS, maxInstances=10\u0026quot; -browser \u0026quot;browserName=chrome,version=ANY,maxInstances=10,platform=WINDOWS\u0026quot;  IE Node Setup java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=internet explorer,version=11,platform=WINDOWS,maxInstances=10\u0026quot;  Chrome Node Setup java -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5556 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=chrome, version=ANY, maxInstances=10, platform=WINDOWS\u0026quot;  FireFox Node Setup java -jar selenium-server-standalone-2.48.2.jar -port 5557 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/data/2018-01-02-setting-up-storm-in-5-minutes/",
	"title": "Setting up Apache Storm",
	"tags": [],
	"description": "",
	"content": "Apache Storm is free and open source distributed real-time computation system. Storm provides reliable real-time data processing what Hadoop did for batch processing. It provides real-time, robust, user friendly, reliable data processing capability with operational Intelligence.\nThis post is more about setting up your Storm Environment from ground up in less than 5 Minutes. Yes, you heard it right less than 5 Minutes. Without any delay, let\u0026rsquo;s get it running.\n# Installing Zookeeper and Storm brew install zookeeper brew install storm  # Starting Zookeeper brew services start zookeeper # Starting Storm cd /usr/local/bin/ ./storm nimbus ./storm supervisor ./storm ui  Here you go, Storm Cluster is up and you are ready to deploy your Storm Topology\nhttp://localhost:8080/index.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-17-setting-up-jekyll-website/",
	"title": "Setting up Jekyll Website",
	"tags": [],
	"description": "",
	"content": " Pre-Requires  Ruby  # Verify Ruby is Installed ruby --version  Installation gem install jekyll bundler  Create New Site jekyll new myblog  Run Blog Locally bundle exec jekyll serve  Reference  https://github.com/arttuladhar/my-jekyll-blog  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/jenkins_setup_terraform/",
	"title": "Setting up Jenkins using Terraform",
	"tags": [],
	"description": "",
	"content": " Building a Custom Jenkins Image Create Dockerfile with contents:\nFROM jenkins/jenkins:lts USER root RUN apt-get update -y \u0026amp;\u0026amp; apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common RUN curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;)/gpg \u0026gt; /tmp/dkey; apt-key add /tmp/dkey RUN add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;) $(lsb_release -cs) stable\u0026quot; RUN apt-get update -y RUN apt-get install -y docker-ce docker-ce-cli containerd.io RUN curl -O https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip \u0026amp;\u0026amp; unzip terraform_0.11.13_linux_amd64.zip -d /usr/local/bin/ USER ${user}  Build the Image\ndocker build -t jenkins:terraform .  List the Docker Images:\ndocker image ls  Setting up Jenkins Create main module for Terraform, main.tf\n# Jenkins Volume resource \u0026quot;docker_volume\u0026quot; \u0026quot;jenkins_volume\u0026quot; { name = \u0026quot;jenkins_data\u0026quot; } # Start the Jenkins Container resource \u0026quot;docker_container\u0026quot; \u0026quot;jenkins_container\u0026quot; { name = \u0026quot;jenkins\u0026quot; image = \u0026quot;jenkins:terraform\u0026quot; ports { internal = \u0026quot;8080\u0026quot; external = \u0026quot;8080\u0026quot; } volumes { volume_name = \u0026quot;${docker_volume.jenkins_volume.name}\u0026quot; container_path = \u0026quot;/var/jenkins_home\u0026quot; } volumes { host_path = \u0026quot;/var/run/docker.sock\u0026quot; container_path = \u0026quot;/var/run/docker.sock\u0026quot; } }  Initialize Terraform\nterraform init  Plan the deployment\nterraform plan -out=tfplan  Deploy Jenkins\nterraform apply tfplan  Get the Admin password\ndocker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword  Login using Public IP with Port 8080\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_kubernetes/",
	"title": "Setting up Kubernetes and Terraform",
	"tags": [],
	"description": "",
	"content": "Create kube-config.yml\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration networking: podSubnet: 10.244.0.0/16 apiServer: extraArgs: service-node-port-range: 8000-31274  Initialize Kubernetes\nsudo kubeadm init --config kube-config.yml  Copy admin.conf to your home directory\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  Install Flannel:\nsudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  Untaint Kubernetes Master\nkubectl taint nodes --all node-role.kubernetes.io/master-  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_docker/",
	"title": "Setting up Terraform With Docker",
	"tags": [],
	"description": "",
	"content": " Installing Docker on the Swarm Manager and Worker # Update the Operating System sudo yum update -y # Uninstall Old Versions sudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine # Install Docker CE sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # Add Docker Repository sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce # Start Docker and Enable sudo systemctl start docker \u0026amp;\u0026amp; sudo systemctl enable docker # Add `cloud_user` to the `docker` group sudo usermod -aG docker cloud_user docker --version # Configure Swarm Manager Node docker swarm init --advertise-addr [PRIVATE_IP] On the worker node, add the worker to the cluster: docker swarm join --token [TOKEN] [PRIVATE_IP]:2377 # Verify Swarm cluster docker node ls  Installing Terraform # Install Terraform 0.11.13 on the Swarm manager sudo curl -O https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip sudo yum install -y unzip sudo unzip terraform_0.11.13_linux_amd64.zip -d /usr/local/bin/ # Test the Terraform installation terraform version  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/spread/",
	"title": "Spread Syntax",
	"tags": [],
	"description": "",
	"content": " Spread Syntax represented by ... expands an iterable to its individual elements.\nCommon use cases:  Spread in array literal  const head = ['eyes','nose','mouth'] const body = ['stomach','hip','legs'] const headBody = [...head, ...body] console.log(headBody) // [\u0026quot;eyes\u0026quot;, \u0026quot;nose\u0026quot;, \u0026quot;mouth\u0026quot;, \u0026quot;stomach\u0026quot;, \u0026quot;hip\u0026quot;, \u0026quot;legs\u0026quot;]   Spread in Object Literals  const obj1 = { foo: 'bar', x: 42 }; const obj2 = { foo: 'baz', y: 13 }; const clonedObj = { ...obj1 }; console.log(clonedObj) // {foo: \u0026quot;bar\u0026quot;, x: 42} const mergedObj = { ...obj1, ...obj2 }; console.log(mergedObj) // {foo: \u0026quot;baz\u0026quot;, x: 42, y: 13}  "
},
{
	"uri": "http://learn.aayushtuladhar.com/java/spring/",
	"title": "Spring",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/java/spring/2018-09-18-spring-cloud-slueth/",
	"title": "Spring Cloud Slueth",
	"tags": [],
	"description": "",
	"content": " A powerful tool for enhancing logs in any application, but especially in a system built up of multiple services. This is where spring-cloud-starter-sleuth comes into play to help you enhance your logging and traceability across multiple systems. Just including the spring-cloud-starter-sluth in your project.\nFew concepts you need to be familiar with when using Spring Cloud Slueth are concepts of Trace and Spans. Trace can though as single request or job that is triggered in an application. All the various steps in the request, even across application and thread boundaries will have the same traceId. Whereas, Spans can be though of a section of a job request. A single trace can be composed of multiple spans each correlating to a specific step or section of the request.\n[application name, traceId, spanId, export]\n         Application Name Name of the Application, we set in the properties file   traceId Request Id to Single Request   spanId Track Unit of Work   export Indicates whether or not the log was exported to an aggregator like Zipkin    Resources Spring Cloud Sleuth - Baeldung\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-04-10-setting-up-supervisord/",
	"title": "SupervisorD",
	"tags": [],
	"description": "",
	"content": "  Installation Configuration Running Supervisor  Supervisor Configuration Structure  Controller Processes  Reread Configuration and Reload It Controlling Tool Start / Stop Processess   It\u0026rsquo;s been a while I have been using Supervisor to run my application. It\u0026rsquo;s a great little tool for running and monitoring processes on UNIX-like operating systems. It provides you simple simple, centralized interface to all your applications running on a box. Using Web Interface, you can see health and logs of the applications without even logging in in the box.\nInstallation sudo apt-get install -y supervisor  Configuration /etc/supervisor/supervisor.conf\n[include] files = /etc/supervisor/conf.d/*.conf # Enable Web Interface [inet_http_server] port = 9001 username = user # Basic auth username password = pass # Basic auth password  Syntax\n[program:nodehook] - Define the program to monitor. We'll call it \u0026quot;nodehook\u0026quot;. command - This is the command to run that kicks off the monitored process. We use \u0026quot;node\u0026quot; and run the \u0026quot;http.js\u0026quot; file. If you needed to pass any command line arguments or other data, you could do so here. directory - Set a directory for Supervisord to \u0026quot;cd\u0026quot; into for before running the process, useful for cases where the process assumes a directory structure relative to the location of the executed script. autostart - Setting this \u0026quot;true\u0026quot; means the process will start when Supervisord starts (essentially on system boot). autorestart - If this is \u0026quot;true\u0026quot;, the program will be restarted if it exits unexpectedly. startretries - The number of retries to do before the process is considered \u0026quot;failed\u0026quot; stderr_logfile - The file to write any errors output. stdout_logfile - The file to write any regular output. user - The user the process is run as. environment - Environment variables to pass to the process.  Example Configuration\n/etc/supervisor/conf.d/node-chada-chutkila.conf\n[program:node-chadachutkila] command=node /home/art/apps/chada-chutkila/app.js directory=/home/art autostart=true autorestart=true startretries=3 stderr_logfile=/var/log/art/chada-chutkila.err.log stdout_logfile=/var/log/art/chada-chutkila.out.log user=art environment=SECRET_PASSPHRASE='this is secret',SECRET_TWO='another secret'  Running Supervisor sudo service supervisor start\nSupervisor Configuration Structure supervisor/ ├── conf.d │ └── digi-marketplace-node.conf └── supervisord.conf  Controller Processes Reread Configuration and Reload It supervisorctl reread supervisorctl update  Controlling Tool supervisorctl\nStart / Stop Processess supervisorctl start \u0026lt;processName\u0026gt; supervisorctl stop \u0026lt;processName\u0026gt;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/template-literals/",
	"title": "Teplate Literals",
	"tags": [],
	"description": "",
	"content": "Template literals are string literals allowing embedded expressions. You can use multi-line strings and string interpolation features with them.\nconst someText = `string text ${expression} string text`  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": " title: Terraform  00 - Terraform Introduction   01 - HCL Basics   02 - Terraform Modules   Setting up Jenkins using Terraform   Setting up Kubernetes and Terraform   Setting up Terraform With Docker   Terraform State   "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_state/",
	"title": "Terraform State",
	"tags": [],
	"description": "",
	"content": "Create a S3 Bucket in AWS that we will be using to store the Remote State.\nSet the Environment Variables:\nexport AWS_ACCESS_KEY_ID=\u0026quot;[ACCESS_KEY]\u0026quot; export AWS_SECRET_ACCESS_KEY=\u0026quot;[SECRET_KEY]]\u0026quot; export AWS_DEFAULT_REGION=\u0026quot;us-east-1\u0026quot;  Add the Remote Backend Configuration\nterraform { backend \u0026quot;s3\u0026quot; { key = \u0026quot;terraform-aws/terraform.tfstate\u0026quot; } }  Initialize Terraform\nterraform init -backend-config \u0026quot;bucket=[BUCKET_NAME]\u0026quot;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": "  "
},
{
	"uri": "http://learn.aayushtuladhar.com/architecture/twelve-factor-app/",
	"title": "The Twelve Factor App",
	"tags": [],
	"description": "",
	"content": " 12factor.net - Link Introduction Methodology for building software-as-a-service app that:\n Use Declarative formats for setup automation, to minimize time and cost for new developers joining the project. Have a Clean Contract with the underlying operation system, offering Maximum Portability between execution environments Are suitable for Deployment on modern Cloud platforms, obviating the need for servers and system administrators Minimize divergence between deployment and production, enabling Continuous deployment for maximum agility And can Scale up without significant changes to tooling, architecture, or development practices.  The Twelve Factors 1. Codebase  One codebase tracked in revision control, many deploys\n  Multiple apps sharing the same code is a violation of twelve-factor. The solution here is to factor shared code into libraries which can be included through the dependency manager  2. Dependencies  Explicity declare and isolate dependencies\n  A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly via a dependency declaration manifest It uses dependency isolation tool during execution to ensure that no implicit dependency specification is applied uniformly to both production and development Another benefits of explicitly declaring dependency is that it allows new developers to quickly run application locally.  3. Config  Store Config in the environment\n  Apps storing config as constants in the code is violation of twelve-factor, which requires strict separation of config from code.  4. Backing Services  Treat backing services as attached resources\n  A backing service is any service that app consumes over the network as part of its normal operation. Eg, Datastore (MySQL, MongoDB), Messaging (RabbitMQ), SMTP Services and Caching System The code for twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config Each distinct backing service is a resource. The twelve-factor app treats these databases as attached resources, which indicate their loose coupling to the deploy they are attached to.   \n5. Build, Release, Run  Strictly separate build and run stages\n  The Build Stage is a tranform which converts a code repo into an executable bundle known as build. Using a version of the code at a commit specified by the development process, the build stage fetches vendors dependencies and compiles binaries and assets The Release Stage takes the build produced by the build stage and combines it with the deploy\u0026rsquo;s current config. The resulting relrease contains both the build and the config and is ready for immediate execution in the execution environment. The Run Stage runs the application in the execution environment, by launching some set of the app\u0026rsquo;s process against a selected release.   Every release should always have a unique release ID Release cannot be mutated once its created. Any change must create a new release  6. Processes  Execute the app as one or more stateless Processes\n  Twelve-factor process are stateless and share-nothing Sticy sessions are a violation of twelve-factor; Session state data is a good candidate for a datastore that offers time-expiration, such as Memcached  7. Port Binding  Export services via port Binding\n  The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing services. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port. Note also that the port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.  8. Concurrency  Scale out via the process model\n  In the twelve-factor app, processess are a first class citizen. The process model truly shines when it comes time to scale out. The share-nothing, horizontal partionable nature of twelve-factor app process means that adding more concurrency is a simple and reliable operation.  9. Disposability  Maximize robustness with fast startup and graceful shutdown\n  The twelve-factor app\u0026rsquo;s processess are disposable, meaning they can be started or stopped at a moment\u0026rsquo;s notice.  10. Dev / Prod Parity  Keep development, staging and production as similar as possible\n  The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. The twelve-factor developers resists the urge to use different backing services between development and production, even when adapters theoritically abstract away differences in backing services.  11. Logs  Treat logs as event streams\n  Twelve-factor app never concerns itself with routing or storage of its output stream It should not attempt to write to or manage logfiles, instead each running process writes its event stream, unbuffered, to stout  12. Admin Process  Run admin / management tasks as one-off processes\n  Twelve-factor strongly favors languages which provide a REPL shell out of the box, and which make it easy to run one-off scripts. In a local deploy, developers invoke one-off admin processes by a direct shell command inside the app’s checkout directory. In a production deploy, developers can use ssh or other remote command execution mechanism provided by that deploy’s execution environment to run such a process.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/linux/2016-11-28-ubuntu-packages/",
	"title": "Ubuntu Packages",
	"tags": [],
	"description": "",
	"content": " List Packages Installed dpkg -l\nCreate Backup of What Packages Installed dpkg --get-selection \u0026gt; list.txt\nRestore dpkg --clear-selections sudo dpkg --set-selections \u0026lt; list.txt sudo apt-get autoremove sudo apt-get dselect-upgrade  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/security/2016-07-07-ssh-keys/",
	"title": "Understanding SSH Keys",
	"tags": [],
	"description": "",
	"content": "  \nSSH is the most common way of connecting to remove Linux Server. It stands for Secure Shell. It provide a safe and secure way of executing commands, making changes and configuring service remotely. SSH connecting is implemented using client-server model. For a client machine to connect to a remote machine using SSH, SSH daemon must be running on the remote machine.\nClients generally authenticate either using passwords or by SSH Keys. SSH keys provides a more secure way of logging into a virtual private server using SSH. SSH keys are nearly impossible to decipher by brute force alone.\nSSH keys are a matching set of cryptographic keys. Each set contains a public and private key. To authenticate using SSH keys, you place the public key on any server and then unlock it by connecting with the private key.\nGenerating SSH Key Pair ssh-keygen -t rsa\nCopy Public Key ssh-copy-id user@123.45.67.89\nAlternatively,\ncat ~/.ssh/id_rsa.pub | ssh user@123.45.56.78 \u0026quot;mkdir -p ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot;\nNote - Generally when you spin up a server, you can embed your public key in your new server.\nGenerate Public Key from Private Key Option -y outputs the public key\nssh-keygen -y -f private-key \u0026gt; public-key\n# Example ssh-keygen -y -f pin-ost.pem \u0026gt; art.pub  Reference https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-03-15-using-apache-server-benchmarking/",
	"title": "Using Apache Server Benchmarking",
	"tags": [],
	"description": "",
	"content": " Apache Benchmark is a single-threaded command line tool for measuring the performance of a HTTP web server. It gives you an impression of how many requests per second your server is capable of serving.\nInstallation sudo apt-get install apache2-utils.  Usage -p POST Message -H Message Header -T Content Type -c Concurrent Clients -n Number of Requests to Run in the Test  GET REQUEST $ ab -n200 -c100 -H \u0026quot;APP-TOKEN: Q977quNeXjFsNjLNlmC9MK1HuRP+fFKmwDX9KSD6Y=\u0026quot; \\ http://test-api.aayushtuladhar.com:8080/test/start_page=0  POST REQUEST $ ab -p body.txt -n200 -c100 -T application/json \\ https://test-api.aayushtuladhar.com:8080/anotherTest  "
},
{
	"uri": "http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/ec2_roles_instance_profiles/",
	"title": "Using EC2 Roles and Instance Profiles",
	"tags": [],
	"description": "",
	"content": "AWS Identity and Access Management (IAM) roles for Amazon Elastic Compute Cloud (EC2) provide the ability to grant instances temporary credentials. These temporary credentials can then be used by hosted applications to access permissions configured within the role. IAM roles eliminate the need for managing credentials, help mitigate long-term security risks, and simplify permissions management.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/",
	"title": "Using Hugo",
	"tags": [],
	"description": "",
	"content": " All the Hugo Basics "
},
{
	"uri": "http://learn.aayushtuladhar.com/data/data-analytics/2018-05-01-pandas/",
	"title": "Using Pandas",
	"tags": [],
	"description": "",
	"content": " Explore, Visualize, and Predict using Pandas \u0026amp; Jupyter  Explore, Visualize, and Predict using Pandas \u0026amp; Jupyter Pandas Intro Load Data Inspecting Data Summarize Data Reference  Setup\n%matplotlib inline import pandas as pd import matplotlib import numpy as np  pd.__version__, matplotlib.__version__, np.__version__  Pandas Intro The pandas library is very popular among data scientists, quants, Excel junkies, and Python developers because it allows you to perform data ingestion, exporting, transformation, and visualization with ease. But if you are only familiar with Python, pandas may present some challenges. Since pandas is inspired by Numpy, its syntax conventions can be confusing to Python developers.\nPandas has two main datatypes: a Series and a DataFrame\n A Series is like a column from a spreadsheet  s = pd.Series([0, 4, 6, 7]) temps = [30, 40, 60, 90] temp_series = pd.Series(temps) temp_series.sum() # Boolean Arrays hot = pd.Series([False, False, True, True]) temp_series[hot] # Masking mask1 = temp_series \u0026gt; 30 mask2 = temp_series \u0026lt; 60 result = temp_series[mask1 \u0026amp; mask2] dates = pd.date_range('20160101', periods=4) temp3 = pd.Series(temps, index=dates)  A DataFrame is like a spreadsheet\ndf = pd.DataFrame({'name': ['Fred', 'Johh', 'Joe', 'Abe'], 'age': s})  Load Data %ls data/ nyc = pd.read_csv('data/central-park-raw.csv', parse_dates=[0])  Inspecting Data # Columns for DataFrame nyc.columns # Get Datatypes for Columns nyc.dtypes # DataFrame Info nyc.info # Show Only First Bit nyc.head(10)  Summarize Data  df['w'].value_counts() - Count number of rows with unique value of variable df.describe()  Reference  https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf  "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/bigdata/",
	"title": "bigdata",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/bigdata/",
	"title": "bigdata",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/hive/",
	"title": "hive",
	"tags": [],
	"description": "",
	"content": ""
}]