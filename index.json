[
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/00_containers_and_orchestration/",
	"title": "00 - Containers and Orchestration",
	"tags": [],
	"description": "",
	"content": "  Containers Microservices Container Orchestration  Container Orchestrators   Containers Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications.\nMicroservices Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.\nContainer Orchestration Container orchestrators are tools which group systems together to form clusters where containers\u0026rsquo; deployment and management is automated at scale while meeting following requirements.\n Fault-tolerance On-demand scalability Optimal resource usage Auto-discovery to automatically discover and communicate with each other Accessibility from the outside world Seamless updates/rollbacks without any downtime.  Container Orchestrators  Amazon Elastic Container Service Azure Container Instances Azure Service Fabric Kubernetes Marathon Nomad Docker Swarm  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/01_kubernetes-architecture/",
	"title": "01 - Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "  Master Node  Master Node Components Master Node Components: API Server Master Node Components: Scheduler Master Node Componets: Controller Managers Master Node Components: etcd  Worker Node  Worker Node Components Worker Node Component: Container Runtime Worker Node Components: kubelet Worker Node Components: kube-proxy Worker Node Components: Addons  Networking Challenges  Container to Container Communication inside Pods Pod-to-Pod Communication Across Nodes Pod-to-External World Communication   At a very high level, Kubernetes has the following main components\n One ore more master nodes One or more worker nodes Distributed key-value store, such as etcd  Master Node The master node provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster\u0026rsquo;s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API).\nMaster Node Components  API Server Scheduler Controller managers etcd  Master Node Components: API Server All the administrative tasks are coordinated by the kube-apiserver, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster\u0026rsquo;s current state from the etcd, and after a call\u0026rsquo;s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster\u0026rsquo;s data store.\nThe API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules.\nMaster Node Components: Scheduler The role of the kube-scheduler is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object\u0026rsquo;s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object\u0026rsquo;s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with disk==ssd key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc.\nMaster Node Componets: Controller Managers The controller managers are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster\u0026rsquo;s desired state (provided by objects\u0026rsquo; configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state.\nMaster Node Components: etcd etcd is a distributed key-value data store used to persist a Kubernetes cluster\u0026rsquo;s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store.\nOut of all the control plane components, only the API server is able to communicate with the etcd data store.\nWorker Node A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together.\nWorker Node Components A worker node has the following components:\n Container runtime kubelet kube-proxy Addons for DNS, Dashboard, cluster-level monitoring and logging.  Worker Node Component: Container Runtime Although Kubernetes is described as \u0026ldquo;container orchestration engine\u0026rdquo;, it does not have the capability to directly handle containers. In order to run and manage a container\u0026rsquo;s lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Docker is the most widely used container runtime with Kubernetes.\nWorker Node Components: kubelet The kubelet is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod\u0026rsquo;s running containers.\nThe kubelet connects to the container runtime using Container Runtime Interface (CRI). CRI consists of protocol buffers, gRPC API and libraries.\nWorker Node Components: kube-proxy The kube-proxy is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods.\nWorker Node Components: Addons Addons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.\n DNS - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources Dashboard - a general purposed web-based user interface for cluster management Monitoring - collects cluster-level container metrics and saves them to a central data store Logging - collects cluster-level container logs and saves them to a central log store for analysis.  Networking Challenges Container to Container Communication inside Pods Making use of the underlying host operating system\u0026rsquo;s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A network namespace is shared across containers, or with the host operating system.\nWhen a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost.\nPod-to-Pod Communication Across Nodes Kubernetes uses \u0026ldquo;IP-per-Pod\u0026rdquo; model to ensure Pod-to-Pod communication, just as VM are able to communicate with each other. Containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI)\nPod-to-External World Communication Kubernetes enables external accessibility through services, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with kube-proxy, applications become accessible from outside the cluster over a virtual IP.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/02_installing_kubernetes/",
	"title": "02 - Installing Kubernetes",
	"tags": [],
	"description": "",
	"content": "  Local Installation On-Premise Installation Cloud Installation  All-in-One Single-Node Installation In this setup, all the master and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, and it should not be used in production. Minikube is one such example, and we are going to explore it in future chapters.\nSingle-Node etcd, Single-Master and Multi-Worker Installation In this setup, we have a single-master node, which also runs a single-node etcd instance. Multiple worker nodes are connected to the master node.\nSingle-Node etcd, Multi-Master and Multi-Worker Installation In this setup, we have multiple-master nodes configured in HA mode, but we have a single-node etcd instance. Multiple worker nodes are connected to the master nodes.\nMulti-Node etcd, Multi-Master and Multi-Worker Installation In this mode, etcd is configured in clustered HA mode, the master nodes are all configured in HA mode, connecting to multiple worker nodes. This is the most advanced and recommended production setup.\nLocal Installation  Minikube - single-node local Kubernetes cluster Docker Desktop - single-node local Kubernetes cluster for Windows and Mac CDK on LXD - multi-node local cluster with LXD containers.  On-Premise Installation  On-Premise VMs Kubernetes can be installed on VMs created via Vagrant, VMware vSphere, KVM, or another Configuration Management (CM) tool in conjunction with a hypervisor software. There are different tools available to automate the installation, such as Ansible or kubeadm.\n On-Premise Bare Metal Kubernetes can be installed on on-premise bare metal, on top of different operating systems, like RHEL, CoreOS, CentOS, Fedora, Ubuntu, etc. Most of the tools used to install Kubernetes on VMs can be used with bare metal installations as well.\n  Cloud Installation  Hosted Solutions  Google Kubernetes Engine (GKE) Azure Kubernetes Service (AKS) Amazon Elastic Container Service for Kubernetes (EKS) DigitalOcean Kubernetes OpenShift Dedicated   Kubernetes The Hard Way\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/03_minikube/",
	"title": "03 - Minikube",
	"tags": [],
	"description": "",
	"content": "  Installing Minikube  Command Line Interface (CLI) tools and scripts Web-based User Interface (Web UI) from a web browser APIs from CLI or programmatically Kubectl Proxy   Installing Minikube # Install Minikube brew install minikube # Starting Minikube minikube start minikube start --vm-driver=xhyve minikube start --vm-driver=hyperkit minikube status minikube stop  Any healthy running Kubernetes cluster can be accessed via any one of the following methods:\nCommand Line Interface (CLI) tools and scripts kubectl is the Kubernetes Command Line Interface (CLI) client to manage cluster resources and applications. It can be used standalone, or part of scripts and automation tools. Once all required credentials and cluster access points have been configured for kubectl it can be used remotely from anywhere to access a cluster.\nWeb-based User Interface (Web UI) from a web browser APIs from CLI or programmatically # Open Minikube Dashboard minikube dashboard # Serving on different Port kubectl proxy #Dashboard URL http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy/#!/overview?namespace=default  Kubectl Proxy When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates.\nA Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.\nGetting Token\nTOKEN=$(kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\\t' | tr -d \u0026quot; \u0026quot;)  Getting API Server Endpoint\nAPISERVER=$(kubectl config view | grep https | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)  Confirm that the APISERVER stored the same IP as the Kubernetes master IP by issuing the following 2 commands and comparing their outputs:\n$ echo $APISERVER https://192.168.99.100:8443 $ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 ...  Access the API server using the curl command, as shown below:\ncurl $APISERVER --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure  By using the kubectl proxy we are bypassing the authentication for each and every request to the Kubernetes API.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/04_kubernetes_building_blocks/",
	"title": "04 - Kubernetes Building Blocks",
	"tags": [],
	"description": "",
	"content": "  Kubernetes Object Model Pods  Labels  Replication Controller Replica Set Deployments Namespaces  Kubernetes Object Model With each object, we declare our intent or the desired state under the spec section. When creating an object, the object\u0026rsquo;s configuration data section from below the spec field has to be submitted to the Kubernetes API server.\nExample of Deployment object configuration in YAML format.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  Pods A pod is the basic unit that Kubernetes deals with. Containers themselves are not assigned to hosts. Instead, closely related containers are grouped together in a pod. A pod generally represents one or more containers that should be controlled as a single “application”.\nA Pod is the smallest and simplest Kubernetes object. It is the unit of deployment in Kubernetes, which represents a single instance of the application. A Pod is a logical collection of one or more containers, which:\n Are scheduled together on the same host with the Pod Share the same network namespace Have access to mount the same external storage (volumes).  Below is an example of a Pod object\u0026rsquo;s configuration in YAML format:\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  # List all the Pods kubectl get pods # Displays details of Pod kubectl describe pod webserver-74d8bd488f-dwbzz  Labels Labels are key-value pairs attached to Kubernetes objects (e.g. Pods, ReplicaSets). Labels are used to organize and select a subset of objects, based on the requirements in place. Many objects can have the same Label(s). Labels do not provide uniqueness to objects. Controllers use Labels to logically group together decoupled objects, rather than using objects\u0026rsquo; names or IDs.\n# Lists Pods with labels kubectl get pods -L k8s-app,label2 # Select the Pods with a given Label kubectl get pods -l k8s-app=webserver  Replication Controller Although no longer a recommended method, a ReplicationController is a controller that ensures a specified number of replicas of a Pod is running at any given time. If there are more Pods than the desired count, a replication controller would terminate the extra Pods, and, if there are fewer Pods, then the replication controller would create more Pods to match the desired count. Generally, we don\u0026rsquo;t deploy a Pod independently, as it would not be able to re-start itself if terminated in error. The recommended method is to use some type of replication controllers to create and manage Pods.\nThe default controller is a Deployment which configures a ReplicaSet to manage Pods\u0026rsquo; lifecycle.\nReplica Set A ReplicaSet is the next-generation ReplicationController. ReplicaSets support both equality- and set-based selectors, whereas ReplicationControllers only support equality-based Selectors. Currently, this is the only difference.\nWith the help of the ReplicaSet, we can scale the number of Pods running a specific container application image. Scaling can be accomplished manually or through the use of an autoscaler.\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\n # List Replica Sets kubectl get replicasets  Deployments Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node\u0026rsquo;s controller manager, and it ensures that the current state always matches the desired state. It allows for seamless application updates and downgrades through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling.\n# Lists all the Deployments in a given namespace kubectl get deployments # Deleting Deployment (Along with ReplicaSet and Pods) kubectl delete deployments webserver  Deleting a Deployment also deletes the ReplicaSet and the Pods it created.\n Namespaces If multiple users and teams use the same Kubernetes cluster we can partition the cluster into virtual sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/05_authorization_access_control/",
	"title": "05 - Authentication, Authorization, and Admission Control",
	"tags": [],
	"description": "",
	"content": "  Authentication Authorization  Types of RoleBindings Admission Control  Demo - Authentication and Authorization  To access and manage any Kubernetes resource or object in the cluster, we need to access a specific API endpoint on the API server. Each access request goes through the following three stages:\n Authentication - Logs in a user Authorization - Authorizes the API requests added by the logged-in user. Admission Control - Software modules that can modify or reject the requests based on some additional checks, like a pre-set Quota.  Authentication Kubernetes does not have an object called user, nor does it store usernames or other related details in its object store. However, even without that, Kubernetes can use usernames for access control and request logging.\nKubernetes has two kinds of users:\n Normal Users They are managed outside of the Kubernetes cluster via independent services like User/Client Certificates, a file listing usernames/passwords, Google accounts, etc.\n Service Accounts With Service Account users, in-cluster processes communicate with the API server to perform different operations. Most of the Service Account users are created automatically via the API server, but they can also be created manually. The Service Account users are tied to a given Namespace and mount the respective credentials to communicate with the API server as Secrets.\n  For authentication, Kubernetes uses different authentication modules:\n Client Certificates To enable client certificate authentication, we need to reference a file containing one or more certificate authorities by passing the --client-ca-file=SOMEFILE option to the API server. The certificate authorities mentioned in the file would validate the client certificates presented to the API server. A demonstration video covering this topic is also available at the end of this chapter.\n Static Token File We can pass a file containing pre-defined bearer tokens with the --token-auth-file=SOMEFILE option to the API server. Currently, these tokens would last indefinitely, and they cannot be changed without restarting the API server.\n Bootstrap Tokens This feature is currently in beta status and is mostly used for bootstrapping a new Kubernetes cluster.\n Static Password File It is similar to Static Token File. We can pass a file containing basic authentication details with the --basic-auth-file=SOMEFILE option. These credentials would last indefinitely, and passwords cannot be changed without restarting the API server.\n Service Account Tokens This is an automatically enabled authenticator that uses signed bearer tokens to verify the requests. These tokens get attached to Pods using the ServiceAccount Admission Controller, which allows in-cluster processes to talk to the API server.\n OpenID Connect Tokens OpenID Connect helps us connect with OAuth2 providers, such as Azure Active Directory, Salesforce, Google, etc., to offload the authentication to external services.\n Webhook Token Authentication With Webhook-based authentication, verification of bearer tokens can be offloaded to a remote service.\n Authenticating Proxy If we want to program additional authentication logic, we can use an authenticating proxy.\n  Authorization After a successful authentication, users can send the API requests to perform different operations. Then, those API requests get authorized by Kubernetes using various authorization modules.\n Node Authorizer Attribute-Based Access Control (ABAC) Authorizer Webhook Authorizer Role-Based Access Control (RBAC) Authorizer  Role - With Role, we can grant access to resources within a specific Namespace.\nClusterRole - The ClusterRole can be used to grant the same permissions as Role does, but its scope is cluster-wide.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: lfs158 name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]  Types of RoleBindings RoleBinding\nIt allows us to bind users to the same namespace as a Role. We could also refer a ClusterRole in RoleBinding, which would grant permissions to Namespace resources defined in the ClusterRole within the RoleBinding’s Namespace.\nClusterRoleBinding\nIt allows us to grant access to resources at a cluster-level and to all Namespaces.\nTo enable the RBAC authorizer, we would need to start the API server with the \u0026ndash;authorization-mode=RBAC option\n Admission Control Admission control is used to specify granular access control policies, which include allowing privileged containers, checking on resource quota, etc. To use admission controls, we must start the Kubernetes API server with the --enable-admission-plugins, which takes a comma-delimited, ordered list of controller names:\n--enable-admission-plugins=NamespaceLifecycle,ResourceQuota,PodSecurityPolicy,DefaultStorageClass  Demo - Authentication and Authorization  Configuring User by assigning key and certificate Create Context with newly created User Add RBAC Role and rolebindings to namespace  minikube start kubectl config view # Create New Namespace for Demo kubectl create namespace lfs158 mkdir rbac \u0026amp;\u0026amp; cd rbac # Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool openssl genrsa -out student.key 2048 openssl req -new -key student.key -out student.csr -subj \u0026quot;/CN=student/O=learner\u0026quot; cat student.csr | base64 | tr -d '\\n' touch signing-request.yaml apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: \u0026lt;assign encoded value from cat command\u0026gt; usages: - digital signature - key encipherment - client auth kubectl create -f signing-request.yaml kubectl get csr kubectl certificate approve student-csr kubectl get csr # Generating User Certificate kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode \u0026gt; student.crt cat student.crt # Configuring Student User by assigning the key and certificate kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key # Create Student Context with Selected User kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student kubectl config view # Creating a Deployment with Nginx Image kubectl -n lfs158 create deployment nginx --image=nginx:alpine  From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context:\nkubectl --context=student-context get pods  Error from server (Forbidden): pods is forbidden: User \u0026ldquo;student\u0026rdquo; cannot list resource \u0026ldquo;pods\u0026rdquo; in API group \u0026ldquo;\u0026rdquo; in the namespace \u0026ldquo;lfs158\u0026rdquo;\n# Create RBAC Role to allow only get, watch, list actions in lfs158 namespace ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] ~/rbac$ kubectl create -f role.yaml ~/rbac$ kubectl -n lfs158 get roles NAME AGE pod-reader 57s  Create a YAML configuration file for a rolebinding object, which assigns the permissions of the pod-reader role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace:\n# Create RBAC Role Binding to the User ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME AGE pod-read-access 23s  Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context.\n~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-77595c695-f2xmd 1/1 Running 0 7m41s  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/06_services/",
	"title": "06 - Services",
	"tags": [],
	"description": "",
	"content": "  Services Service Object Example kube-proxy Service Discovery Servie Type  Cluster IP NodePort LoadBalancer ExternalIP ExternalName   Services  An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don’t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.\n Using the selectors app==frontend and app==db, we group Pods into two logical sets: one with 3 Pods, and one with a single Pod.\nWe assign a name to the logical grouping, referred to as a Service. In our example, we create two Services, frontend-svc, and db-svc, and they have the app==frontend and the app==db Selectors, respectively.\nServices can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets.\nService Object Example kind: Service apiVersion: v1 metadata: name: frontend-svc spec: selector: app: frontend ports: - protocol: TCP port: 80 targetPort: 5000  The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.\nWhile the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic.\n kube-proxy All worker nodes run a daemon called kube-proxy, which watches the API server on the master node for the addition and removal of Services and endpoints.\nService Discovery As Services are the primary mode of communication in Kubernetes, we need a way to discover them at runtime. Kubernetes supports two methods for discovering Services:\n Environment Variables DNS  Kubernetes has an add-on for DNS, which creates a DNS record for each Service and its format is my-svc.my-namespace.svc.cluster.local.\nServie Type While defining a Service, we can also choose its access scope. We can decide whether the Service:\n Is only accessible within the cluster Is accessible from within the cluster and the external world Maps to an entity which resides either inside or outside the cluster.  Access scope is decided by ServiceType, which can be configured when creating the Service.\n Cluster IP ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address is used for communicating with the Service and is accessible only within the cluster.\nNodePort The NodePort ServiceType is useful when we want to make our Services accessible from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. To access multiple applications from the external world, administrators can configure a reverse proxy - an ingress, and define rules that target Services within the cluster.\nkubectl expose pod \u0026lt;podname\u0026gt; --type=NodePort --port=80  LoadBalancer With the LoadBalancer ServiceType:\n NodePort and ClusterIP are automatically created, and the external load balancer will route to them The Service is exposed at a static port on each worker node The Service is exposed externally using the underlying cloud provider\u0026rsquo;s load balancer feature.  The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, and the Service will work the same way as a NodePort type Service.\n ExternalIP A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform or AWS.\nExternalName ExternalName is a special ServiceType, that has no Selectors and does not define any endpoints. When accessed within the cluster, it returns a CNAME record of an externally configured Service.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/07_deploying_standalone_app/",
	"title": "07 - Deploying Standalone App",
	"tags": [],
	"description": "",
	"content": "  Creating Deployment using YAML File Exposing Application Liveness and Readiness Probes  Liveness Probe Readiness Probe   Creating Deployment using YAML File webserver.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: webserver labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80  kubectl create -f webserver.yaml  This will also create ReplicaSet and Pods as defined in the YAML configuration.\nExposing Application webserver-svc.yaml\napiVersion: v1 kind: Service metadata: name: web-service labels: run: web-service spec: type: NodePort ports: - port: 80 protocol: TCP selector: app: nginx  kubectl create -f webserver-svc.yaml  Liveness and Readiness Probes Liveness Probe Liveness probe checks on an application\u0026rsquo;s health, and if the health check fails, kubelet restarts the affected container automatically.\nLiveness Probes can be set by defining:\n Liveness command  apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5   Liveness HTTP request  livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3   TCP Liveness Probe  livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20  Readiness Probe Readiness Probe can be used to ensure certain conditions are met before application can serve traffic. These conditions include ensuring that the depending service is ready, or acknowledging that a large dataset needs to be loaded, etc. In such cases, we use Readiness Probes and wait for a certain condition to occur.\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/08_kubernetes_volume_management/",
	"title": "08 - Kubernetes Volume Management",
	"tags": [],
	"description": "",
	"content": "  Volumes Volume Types PersistentVolumeClaims Using a Shared hostPath Volume Type  Volumes As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the kubelet will restart it with a clean slate, which means that it will not have any of the old data.\nTo overcome this problem, Kubernetes uses Volumes. A Volume is essentially a directory backed by a storage medium. The storage medium, content and access mode are determined by the Volume Type. In Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\nIn Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\n Volume Types  emptyDir - An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume\u0026rsquo;s life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever.\n hostPath - With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk - With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore - With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk - With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile - With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs - With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs - With nfs, we can mount an NFS share into a Pod. iscsi - With iscsi, we can mount an iSCSI share into a Pod. secret - With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. We will take a look at an example in a later chapter. configMap - With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim - We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. We will cover this in our next section.  PersistentVolumeClaims A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PersistentVolume resources based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.\nUsing a Shared hostPath Volume Type apiVersion: v1 kind: Pod metadata: name: share-pod labels: app: share-pod spec: volumes: - name: \u0026quot;host-volume\u0026quot; hostPath: path: \u0026quot;/home/docker/pod-volume\u0026quot; containers: - image: nginx name: nginx volumeMounts: - mountPath: \u0026quot;/usr/share/nginx/html\u0026quot; name: \u0026quot;host-volume\u0026quot; ports: - containerPort: 80 - image: debian name: debian volumeMounts: - mountPath: /host-vol name: \u0026quot;host-volume\u0026quot; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Hello Kubernetes \u0026gt; /host-vol/index.html; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/09_configmaps_secrets/",
	"title": "09 - ConfigMaps and Secrets",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps  Creating ConfigMaps Using CommandLine Using Configuration File Using Properties File Using ConfigMaps Inside Pods As Environment Variable As Volume  Secrets  Creating Secret Create a Secret from Literal and Display Its Details Create a Secret from YAML File Use Secrets Inside Pods   ConfigMaps ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories.\nCreating ConfigMaps Using CommandLine kubectl create configmap my-config \\ --from-literal=key1=value1 \\ --from-literal=key2=value2  Using Configuration File apiVersion: v1 kind: ConfigMap metadata: name: customer1 data: TEXT1: Customer1_Company TEXT2: Welcomes You COMPANY: Customer1 Company Technology Pct. Ltd.  Using Properties File permission-reset.properties\npermission=read-only allowed=\u0026quot;true\u0026quot; resetCount=3  kubectl create configmap permission-config --from-file=\u0026lt;path/to/\u0026gt;permission-reset.properties  Using ConfigMaps Inside Pods As Environment Variable Inside a Container, we can retrieve the key-value data of an entire ConfigMap or the values of specific ConfigMap keys as environment variables.\ncontainers: - name: myapp-full-container image: myapp envFrom: - configMapRef: name: full-config-map  containers: - name: myapp-specific-container image: myapp env: - name: SPECIFIC_ENV_VAR1 valueFrom: configMapKeyRef: name: config-map-1 key: SPECIFIC_DATA - name: SPECIFIC_ENV_VAR2 valueFrom: configMapKeyRef: name: config-map-2 key: SPECIFIC_INFO  As Volume We can mount a vol-config-map ConfigMap as a Volume inside a Pod. For each key in the ConfigMap, a file gets created in the mount path (where the file is named with the key name) and the content of that file becomes the respective key\u0026rsquo;s value:\ncontainers: - name: myapp-vol-container image: myapp volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: vol-config-map  Secrets Secret object can help by allowing us to encode the sensitive information before sharing it. With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, similar to ConfigMaps; thus, we can control how the information in a Secret is used, reducing the risk for accidental exposures. In Deployments or other resources, the Secret object is referenced, without exposing its content.\nIt is important to keep in mind that the Secret data is stored as plain text inside etcd, therefore administrators must limit access to the API server and etcd. A newer feature allows for Secret data to be encrypted at rest while it is stored in etcd; a feature which needs to be enabled at the API server level.\n Creating Secret Create a Secret from Literal and Display Its Details # Create Secret kubectl create secret generic my-password --from-literal=password=mysqlpassword # Get Secret kubectl get secret my-password # Describe Secret kubectl describe secret my-password  Create a Secret from YAML File echo mysqlpassword | base64 bXlzcWxwYXNzd29yZAo=  mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque data: password: bXlzcWxwYXNzd29yZAo=  Please note that base64 encoding does not mean encryption, and anyone can easily decode our encoded data\n mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque stringData: password: mysqlpassword  # Create Secret from YAML kubectl create -f mypass.yaml  Use Secrets Inside Pods Using Secrets as Environment Variables\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress env: - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: my-password key: password  Using Secrets as Files from a Pod\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress volumeMounts: - name: secret-volume mountPath: \u0026quot;/etc/secret-data\u0026quot; readOnly: true volumes: - name: secret-volume secret: secretName: my-password  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/10_ingress/",
	"title": "10 - Ingress",
	"tags": [],
	"description": "",
	"content": " With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.\n An Ingress is a collection of rules that allow inbound connections to reach the cluster Services.\n To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following:\n TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules  Name-Based Virtual Hosting virtual-host-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: virtual-host-ingress namespace: default spec: rules: - host: blue.example.com http: paths: - backend: serviceName: webserver-blue-svc servicePort: 80 - host: green.example.com http: paths: - backend: serviceName: webserver-green-svc servicePort: 80  fan-out-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: fan-out-ingress namespace: default spec: rules: - host: example.com http: paths: - path: /blue backend: serviceName: webserver-blue-svc servicePort: 80 - path: /green backend: serviceName: webserver-green-svc servicePort: 80  The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller, which we will discuss next.\n # Start the Ingress Controller with Minikube minikube addons enable ingress # Deploy an Ingress Resource kubectl create -f virtual-host-ingress.yaml  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/11_advanced_topics/",
	"title": "11 - Advanced Topics",
	"tags": [],
	"description": "",
	"content": " Annotations With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:\n\u0026quot;annotations\u0026quot;: { \u0026quot;key1\u0026quot; : \u0026quot;value1\u0026quot;, \u0026quot;key2\u0026quot; : \u0026quot;value2\u0026quot; }  Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:\n Store build/release IDs, PR numbers, git branch, etc. Phone/pager numbers of people responsible, or directory entries specifying where such information can be found Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc.  apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019  Jobs and CronJobs A Job creates one or more Pods to perform a given task. The Job object takes the responsibility of Pod failures. It makes sure that the given task is completed successfully. Once the task is complete, all the Pods have terminated automatically. Job configuration options include\n parallelism - to set the number of pods allowed to run in parallel; completions - to set the number of expected completions; activeDeadlineSeconds - to set the duration of the Job; backoffLimit - to set the number of retries before Job is marked as failed; ttlSecondsAfterFinished - to delay the clean up of the finished Jobs.  Quota Management Autoscalling Autoscaling can be implemented in a Kubernetes cluster via controllers which periodically adjust the number of running objects based on single, multiple, or custom metrics. There are various types of autoscalers available in Kubernetes which can be implemented individually or combined for a more robust autoscaling solution:\nHorizontal Pod Autoscaler (HPA) HPA is an algorithm based controller API resource which automatically adjusts the number of replicas in a ReplicaSet, Deployment or Replication Controller based on CPU utilization.\nVertical Pod Autoscaler (VPA) VPA automatically sets Container resource requirements (CPU and memory) in a Pod and dynamically adjusts them in runtime, based on historical utilization data, current resource availability and real-time events.\nCluster Autoscaler Cluster Autoscaler automatically re-sizes the Kubernetes cluster when there are insufficient resources available for new Pods expecting to be scheduled or when there are underutilized nodes in the cluster.\nDaemonSets A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\n running a cluster storage daemon, such as glusterd, ceph, on each node. running a logs collection daemon on every node, such as fluentd or logstash. running a node monitoring daemon on every node, such as Prometheus Node Exporter, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or Instana Agent.  StatefulSets The StatefulSet controller is used for stateful applications which require a unique identity, such as name, network identifications, strict ordering, etc. For example, MySQL cluster, etcd cluster.\nThe StatefulSet controller provides identity and guaranteed ordering of deployment and scaling to Pods. Similar to Deployments, StatefulSets use ReplicaSets as intermediary Pod controllers and support rolling updates and rollbacks.\nKubernetes Federation With Kubernetes Cluster Federation we can manage multiple Kubernetes clusters from a single control plane. We can sync resources across the federated clusters and have cross-cluster discovery. This allows us to perform Deployments across regions, access them using a global DNS record, and achieve High Availability.\nHelm Chart To deploy an application, we use different Kubernetes manifests, such as Deployments, Services, Volume Claims, Ingress, etc. Sometimes, it can be tiresome to deploy them one by one. We can bundle all those manifests after templatizing them into a well-defined format, along with other metadata. Such a bundle is referred to as Chart. These Charts can then be served via repositories, such as those that we have for rpm and deb packages.\nHelm is a package manager (analogous to yum and apt for Linux) for Kubernetes, which can install/update/delete those Charts in the Kubernetes cluster.\nHelm has two components:\n A client called helm, which runs on your user\u0026rsquo;s workstation A server called tiller, which runs inside your Kubernetes cluster.  The client helm connects to the server tiller to manage Charts. Charts submitted for Kubernetes are available here.\nNetwork Policies Monitoring and Logging "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/12_resources/",
	"title": "12 - Resources",
	"tags": [],
	"description": "",
	"content": " Labs CKA Curriculum   "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/",
	"title": "Arrow Functions",
	"tags": [],
	"description": "",
	"content": " Arrow Function Express allows you to write shorter syntax than it\u0026rsquo;s predecessor Function expression. In addition and more exciting is how the new Arrow function bind their context.\n(param1, param2, param3) =\u0026gt; { statements } singleParam =\u0026gt; { statements } () =\u0026gt; { statements }  Example var materials = [ 'Iron', 'Calcium', 'Sodium', 'Magnanese' ] materials.map(material =\u0026gt; material.length)  An arrow function does not newly define its own this when it\u0026rsquo;s being executed.The value of this is always inherited from the enclosing scope.\n// ES5 function CounterES5(){ this.seconds = 0; window.setInterval(function() { this.seconds++; console.log(seconds); }.bind(this), 1000); } //ES6 function CounterES6(){ this.seconds =0; window.setInterval( () =\u0026gt; { this.seconds++; console.log(seconds) },1000 ); }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/",
	"title": "Basic Essentials",
	"tags": [],
	"description": "",
	"content": " Creting Instance (Directly) # Create Compute Instance gcloud compute instances create gcelab2 --machine-type n1-standard-2 \\ --zone us-central1-c  Using Instance Templates / Instance Groups # Create Instance Template gcloud compute instance-templates create nginx-template \\ --metadata-from-file startup-script=startup.sh # Create Target Pool gcloud compute target-pools create nginx-pool # Create Instance Group gcloud compute instance-groups managed create nginx-group \\ --base-instance-name nginx \\ --size 2 \\ --template nginx-template \\ --target-pool nginx-pool # List Instances gcloud compute instances list  Create Filewall # Create Filewall rule to Allow 80 gcloud compute firewall-rules create www-firewall --allow tcp:80  SSH Instance gcloud compute ssh gcelab2 --zone us-central1-c   gcloud is a command-line tool for Google Cloud Platform\n  gsutil is a command-line tool to mange Cloud Storage resources\n Config # List Config gcloud config list # Sets Default Zone to us-central1-a gcloud config set compute/zone us-central1-a  Kubernetes Cluster Engine A cluster consists of at least one cluster master machine and multiple worker machines called nodes. Nodes are Compute Engine virtual machine (VM) instances that run the Kubernetes processes necessary to make them part of the cluster.\n# Create Cluster gcloud container clusters create my-precious-cluster # Updates a kubeconfig file with appropriate credentials to point kubectl at a specific cluster in GKE gcloud container cluster get-credentials my-precious-cluster # Delete Cluster gcloud container clusters delete my-precious-cluster  Network Load Balancer Network load balancing allows you to balance the load of your systems based on incoming IP protocol data, such as address, port, and protocol type. You also get some options that are not available, with HTTP(S) load balancing. For example, you can load balance additional TCP/UDP-based protocols such as SMTP traffic. And if your application is interested in TCP-connection-related characteristics, network load balancing allows your app to inspect the packets, where HTTP(S) load balancing does not.\n# Create Forwarding Rules gcloud compute forwarding-rules create nginx-lab \\ --region us-central1 --port=80 --target-pool nginx-pool # List Forwarding Rules gcloud compute forwarding-rules list  Creating HTTP(s) Load Balancer # Create Health Check gcloud compute http-health-checks create http-basic-check # Defining a HTTP service and map a port name to the relevant port gcloud compute instance-groups manged set-named-ports nginx-group --named-ports http:80 # Creating Backend Service gcloud compute backend-services create nginx-backend \\ --protocol HTTP \\ --http-health-checks http-basic-check \\ --global # Adding instance group to the backend services gcloud compute backend-services add-backend nginx-backend \\ --instance-group nginx-group \\ --instance-group-zone us-central1-a \\ --global # Create a default URL map that directs all incoming requests to all your instances gcloud compute url-maps create web-map --default-service nginx-backend # Create a target HTTP proxy to route requests to URL map gcloud compute target-http-proxies create http-lb-proxy --url-map web-map # Create global forwarding rule to handle and route incoming requests gcloud compute forwarding-rules create http-content-rule \\ --global --target-http-proxy http-lb-proxy --ports 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/",
	"title": "Cloud Storage",
	"tags": [],
	"description": "",
	"content": " Create a Stoage Bucket gsutil mb gs://unique-name  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/",
	"title": "Continuous Delivery with Jenkins in Kubernetes Engine",
	"tags": [],
	"description": "",
	"content": "# Create Kubernetes Cluster gcloud container clusters create jenkins-cd \\ --num-nodes 2 \\ --machine-type n1-standard-2 \\ --scopes \u0026quot;https://www.googleapis.com/auth/projecthosting,cloud-platform\u0026quot; # Update KubeConfig with Cluster credentials gcloud container clusters get-credentials jenkins-cd # Verify Kubernetes can connect to GCP Kubernetes Cluster kubectl cluster-info  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/destructuring/",
	"title": "Destructuring JavaScript Objects",
	"tags": [],
	"description": "",
	"content": "const person = { firstName: 'Aayush', lastName: 'Tuladhar', country: 'Nepal', twitter: '@aayushtuladhar' } /* Problem */ const first = person.firstName; const last = person.lastName; console.log(`Hello ${first} ${last}`); /* Solution */ const { firstName, lastName } = person; console.log(`Hello ${firstName} ${lastName}`); /* ------------------ */ const art = { first: 'ART', last: 'Ratna', links: { social: { twitter: 'https://twitter.com/aayushtuladhar', facebook: 'https://facebook.com/aayush.tuladhar', }, web: { blog: 'https://aayushtuladhar.com' } } }; const { twitter, facebook } = art.links.social; console.log(twitter); console.log(facebook);  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "All the Things DevOps\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/",
	"title": "Digital Learning Notebook",
	"tags": [],
	"description": "",
	"content": " My Digital Learning Notebook As I learn new things, I will be using this notebook to capture important notes regarding various technical aspects of Software Engineering and Technology in general.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Learning the Basics Stuff Starting Server hugo server -D  Documentation Learn\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/reactjs/redux/",
	"title": "Getting Started with Redux",
	"tags": [],
	"description": "",
	"content": " Redux gives you a store, and lets you keep state in it, and get state out, and respond when the state changes. But that’s all it does.\nIt’s actually react-redux that lets you connect pieces of the state to React components.\n The state is the data, and the store is where it’s kept\n Redux Store Redux Reducer Reducer\u0026rsquo;s job is to take the current state and action and return the new state. It has another job, too. It should return the initial state the first time it\u0026rsquo;s called.\nReducer Rule # 1 = Never return undefined from a reducer Reduce Rule # 2 = Reduces must be a pure functions (They can\u0026rsquo;t modify their arguments, and they can\u0026rsquo;t have side effects)\nRedux Actions An action is Redux-speak for a plain object with a property called type. In order to keep things sane and maintainable, we Redux users usually give our actions types that are plain strings, and often uppercased, to signify that they’re meant to be constant values.\nAn action object describes a change you want to make (like “please increment the counter”) or an event that happenend (like “the request to the server failed with this error”).\nIn order to make an action DO something, you need to dispatch it.\nRedux Dispatch The store we created earlier has a built-in function called dispatch. Call it with an action, and Redux will call your reducer with that action (and then replace the state with whatever your reducer returned).\nReferences "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/",
	"title": "Google Cloud Platform",
	"tags": [],
	"description": "",
	"content": "All the Google Cloud Platform Stuff\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": "All the JavaScript Stuff\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. This tutorial provides an overview of different kind of features and functionalities of Kubernetes and teaches how to manage the containerized infrastructure and application deployment.\nFeatures Automatic bin packing\nKubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability.\nSelf-healing\nKubernetes automatically replaces and reschedules containers from failed nodes. It kills and restarts containers unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers.\nHorizontal scaling\nWith Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization.\nService discovery and Load balancing\nContainers receive their own IP addresses from Kubernetes, white it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set.\nAutomated rollouts and rollbacks\nKubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application\u0026rsquo;s health to prevent any downtime.\nSecret and configuration management\nKubernetes manages secrets and configuration details for an application separately from the container image, in order to avoid a re-build of the respective image. Secrets consist of confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub.\nStorage orchestration\nKubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, or network storage systems.\nBatch execution\nKubernetes supports batch execution, long-running jobs, and replaces failed containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_2/",
	"title": "Layouts",
	"tags": [],
	"description": "",
	"content": " Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading  Renders to:\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading Typography I am just being Bold\nI Love my Italics Style\nStrike Through\n I love to give a quotation\n Images ![Minion](https://octodex.github.com/images/minion.png)  Resizing Images ![Minion](https://octodex.github.com/images/minion.png?width=20pc)  Buttons Get Grav  Note / Info/ Tip / Warning A notice disclaimer\n An information disclaimer\n A tip disclaimer\n A warning disclaimer\n Expand   Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/network-concepts/",
	"title": "Network Concepts",
	"tags": [],
	"description": "",
	"content": " Projects Projects are the outermost container and are used to group resources that share the same trust boundary. Many developers map Projects to teams since each Project has its own access policy (IAM) and member list. Projects also serve as a collector of billing and quota details reflecting resource consumption. Projects contain Networks which contain Subnetworks, Firewall rules, and Routes (see below architecture diagrams for illustration).\nNetworks Networks directly connect your resources to each other and to the outside world. Networks, using Firewalls, also house the access policies for incoming and outgoing connections. Networks can be Global (offering horizontal scalability across multiple Regions) or Regional (offering low-latency within a single Region).\nSubnetworks Subnetworks allow you to group related resources (Compute Engine instances) into RFC1918 private address spaces. Subnetworks can only be Regional. A subnetwork can be in auto mode or custom mode.\nAn auto mode network has one subnet per region, each with a predetermined IP range and gateway. These subnets are created automatically when you create the auto mode network, and each subnet has the same name as the overall network.\nA custom mode network has no subnets at creation. In order to create an instance in a custom mode network, you must first create a subnetwork in that region and specify its IP range. A custom mode network can have zero, one, or many subnets per region.\nCreate Virtual Private Cloud (VPC) Network and Instances # Creating VPC Network with auto subnet gcloud compute networks create mynetwork --subnet-mode=auto # Creating VPC Network with Custom Subnet gcloud compute networks create privatenet --subnet-mode=custom # Creating Custom Subnet - privatesubnet gcloud compute networks subnets create privatesubnet --network=privatenet \\ --region=us-central1 --range=10.0.0.0/24 --enable-private-ip-google-access  gcloud compute instances create default-us-vm --zone=us-central1-a --network=default gcloud compute instances create mynet-us-vm --zone=us-central1-a --network=mynetwork gcloud compute instances create mynet-eu-vm --zone=europe-west1-b --network=mynetwork gcloud compute instances create privatenet-bastion --zone=us-central1-c \\ --subnet=privatesubnet --can-ip-forward gcloud compute instances create privatenet-us-vm --zone=us-central1-f \\ --subnet=privatesubnet  When any Project is created, a single Network named default is created for you. The default Network has the following 2 firewall rules defined for network traffic:\n default-deny-all-ingress - Deny all incoming traffic\ndefault-allow-all-egress - Allow all outbound traffic\nFirewall Rules The privilege of creating, modifying, and deleting firewall rules is reserved for the compute.securityAdmin role by IAM.\n gcloud beta compute firewall-rules create mynetwork-allow-icmp --network mynetwork \\ --action ALLOW --direction INGRESS --rules icmp gcloud beta compute firewall-rules create mynetwork-allow-ssh --network mynetwork \\ --action ALLOW --direction INGRESS --rules tcp:22 gcloud beta compute firewall-rules create mynetwork-allow-internal --network \\ mynetwork --action ALLOW --direction INGRESS --rules all \\ --source-ranges 10.128.0.0/9 gcloud beta compute firewall-rules list \\ --filter=\u0026quot;network:mynetwork\u0026quot;  Cloud Routes If you want traffic from specific instances to specific ranges to be routed in a specific way, you can use Google Cloud Routes to set up the destination for this traffic.\nYou can route traffic based on instance tags and destination range, and you can set the next hop to either:\n A specific instance (by instance name or IP) A VPN Tunnel The default internet gateway  If multiple routes exist, the more specific route will be used. If there are multiple of those, the lowest priority value is used. Between Subnetworks, routes are automatically created implicitly at lowest priority value. Those routes cannot be changed or deleted.\nConvert to a NAT gateway # Tag Instance gcloud compute instances add-tags privatenet-us-vm --zone us-central1-f --tags nat-me # Create NAT Route gcloud compute routes create nat-route --network privatenet \\ --destination-range 0.0.0.0/0 --next-hop-instance privatenet-bastion \\ --next-hop-instance-zone us-central1-c --tags nat-me --priority 800  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/",
	"title": "Orchestrating Cloud with Kubernetes",
	"tags": [],
	"description": "",
	"content": " # Creating Kubernetes Cluster gcloud container clusters create io  Quick Demo # Create Deployment kubectl create deployment nginx --image=nginx:1.10.0 # List Pods kubectl get pods # Expose Deployment via a Service using LoadBalancer kubectl expose deployment nginx --port 80 --type LoadBalancer # List Service kubectl get services  Pods Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods\n# Create Pod kubectl create -f pods/monolith.yaml # List Pods kubectl get pods # Describe Pod kubectl describe pods monolith # Port Forward Pod kubectl port-forward monolith 10080:80  Services An abstract way to expose an application running on a set of Pods as a network service.\nNo need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives pods their own IP addresses and a single DNS name for a set of pods, and can load-balance across them.\nPods aren\u0026rsquo;t meant to be persistent. They can be stopped or started for many reasons - like failed liveness or readiness checks - and this leads to a problem\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/template-literals/",
	"title": "Teplate Literals",
	"tags": [],
	"description": "",
	"content": "Template literals are string literals allowing embedded expressions. You can use multi-line strings and string interpolation features with them.\nconst someText = `string text ${expression} string text`  "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/",
	"title": "Using Hugo",
	"tags": [],
	"description": "",
	"content": " All the Hugo Basics "
}]