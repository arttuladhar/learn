[
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/00_containers_and_orchestration/",
	"title": "00 - Containers and Orchestration",
	"tags": [],
	"description": "",
	"content": "  Containers Microservices Container Orchestration  Container Orchestrators   Containers Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications.\nMicroservices Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.\nContainer Orchestration Container orchestrators are tools which group systems together to form clusters where containers\u0026rsquo; deployment and management is automated at scale while meeting following requirements.\n Fault-tolerance On-demand scalability Optimal resource usage Auto-discovery to automatically discover and communicate with each other Accessibility from the outside world Seamless updates/rollbacks without any downtime.  Container Orchestrators  Amazon Elastic Container Service Azure Container Instances Azure Service Fabric Kubernetes Marathon Nomad Docker Swarm  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/01_kubernetes-architecture/",
	"title": "01 - Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "  Master Node  Master Node Components Master Node Components: API Server Master Node Components: Scheduler Master Node Componets: Controller Managers Master Node Components: etcd  Worker Node  Worker Node Components Worker Node Component: Container Runtime Worker Node Components: kubelet Worker Node Components: kube-proxy Worker Node Components: Addons  Networking Challenges  Container to Container Communication inside Pods Pod-to-Pod Communication Across Nodes Pod-to-External World Communication   At a very high level, Kubernetes has the following main components\n One ore more master nodes One or more worker nodes Distributed key-value store, such as etcd  Master Node The master node provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster\u0026rsquo;s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API).\nMaster Node Components  API Server Scheduler Controller managers etcd  Master Node Components: API Server All the administrative tasks are coordinated by the kube-apiserver, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster\u0026rsquo;s current state from the etcd, and after a call\u0026rsquo;s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster\u0026rsquo;s data store.\nThe API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules.\nMaster Node Components: Scheduler The role of the kube-scheduler is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object\u0026rsquo;s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object\u0026rsquo;s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with disk==ssd key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc.\nMaster Node Componets: Controller Managers The controller managers are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster\u0026rsquo;s desired state (provided by objects\u0026rsquo; configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state.\nMaster Node Components: etcd etcd is a distributed key-value data store used to persist a Kubernetes cluster\u0026rsquo;s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store.\nOut of all the control plane components, only the API server is able to communicate with the etcd data store.\nWorker Node A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together.\nWorker Node Components A worker node has the following components:\n Container runtime kubelet kube-proxy Addons for DNS, Dashboard, cluster-level monitoring and logging.  Worker Node Component: Container Runtime Although Kubernetes is described as \u0026ldquo;container orchestration engine\u0026rdquo;, it does not have the capability to directly handle containers. In order to run and manage a container\u0026rsquo;s lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Docker is the most widely used container runtime with Kubernetes.\nWorker Node Components: kubelet The kubelet is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod\u0026rsquo;s running containers.\nThe kubelet connects to the container runtime using Container Runtime Interface (CRI). CRI consists of protocol buffers, gRPC API and libraries.\nWorker Node Components: kube-proxy The kube-proxy is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods.\nWorker Node Components: Addons Addons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.\n DNS - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources Dashboard - a general purposed web-based user interface for cluster management Monitoring - collects cluster-level container metrics and saves them to a central data store Logging - collects cluster-level container logs and saves them to a central log store for analysis.  Networking Challenges Container to Container Communication inside Pods Making use of the underlying host operating system\u0026rsquo;s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A network namespace is shared across containers, or with the host operating system.\nWhen a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost.\nPod-to-Pod Communication Across Nodes Kubernetes uses \u0026ldquo;IP-per-Pod\u0026rdquo; model to ensure Pod-to-Pod communication, just as VM are able to communicate with each other. Containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI)\nPod-to-External World Communication Kubernetes enables external accessibility through services, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with kube-proxy, applications become accessible from outside the cluster over a virtual IP.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/02_installing_kubernetes/",
	"title": "02 - Installing Kubernetes",
	"tags": [],
	"description": "",
	"content": "  Local Installation On-Premise Installation Cloud Installation  All-in-One Single-Node Installation In this setup, all the master and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, and it should not be used in production. Minikube is one such example, and we are going to explore it in future chapters.\nSingle-Node etcd, Single-Master and Multi-Worker Installation In this setup, we have a single-master node, which also runs a single-node etcd instance. Multiple worker nodes are connected to the master node.\nSingle-Node etcd, Multi-Master and Multi-Worker Installation In this setup, we have multiple-master nodes configured in HA mode, but we have a single-node etcd instance. Multiple worker nodes are connected to the master nodes.\nMulti-Node etcd, Multi-Master and Multi-Worker Installation In this mode, etcd is configured in clustered HA mode, the master nodes are all configured in HA mode, connecting to multiple worker nodes. This is the most advanced and recommended production setup.\nLocal Installation  Minikube - single-node local Kubernetes cluster Docker Desktop - single-node local Kubernetes cluster for Windows and Mac CDK on LXD - multi-node local cluster with LXD containers.  On-Premise Installation  On-Premise VMs Kubernetes can be installed on VMs created via Vagrant, VMware vSphere, KVM, or another Configuration Management (CM) tool in conjunction with a hypervisor software. There are different tools available to automate the installation, such as Ansible or kubeadm.\n On-Premise Bare Metal Kubernetes can be installed on on-premise bare metal, on top of different operating systems, like RHEL, CoreOS, CentOS, Fedora, Ubuntu, etc. Most of the tools used to install Kubernetes on VMs can be used with bare metal installations as well.\n  Cloud Installation  Hosted Solutions  Google Kubernetes Engine (GKE) Azure Kubernetes Service (AKS) Amazon Elastic Container Service for Kubernetes (EKS) DigitalOcean Kubernetes OpenShift Dedicated   Kubernetes The Hard Way\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/03_minikube/",
	"title": "03 - Minikube",
	"tags": [],
	"description": "",
	"content": "  Installing Minikube  Command Line Interface (CLI) tools and scripts Web-based User Interface (Web UI) from a web browser APIs from CLI or programmatically Kubectl Proxy   Installing Minikube # Install Minikube brew install minikube # Starting Minikube minikube start minikube start --vm-driver=xhyve minikube start --vm-driver=hyperkit minikube status minikube stop  Any healthy running Kubernetes cluster can be accessed via any one of the following methods:\nCommand Line Interface (CLI) tools and scripts kubectl is the Kubernetes Command Line Interface (CLI) client to manage cluster resources and applications. It can be used standalone, or part of scripts and automation tools. Once all required credentials and cluster access points have been configured for kubectl it can be used remotely from anywhere to access a cluster.\nWeb-based User Interface (Web UI) from a web browser APIs from CLI or programmatically # Open Minikube Dashboard minikube dashboard # Serving on different Port kubectl proxy #Dashboard URL http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy/#!/overview?namespace=default  Kubectl Proxy When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates.\nA Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.\nGetting Token\nTOKEN=$(kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\\t' | tr -d \u0026quot; \u0026quot;)  Getting API Server Endpoint\nAPISERVER=$(kubectl config view | grep https | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)  Confirm that the APISERVER stored the same IP as the Kubernetes master IP by issuing the following 2 commands and comparing their outputs:\n$ echo $APISERVER https://192.168.99.100:8443 $ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 ...  Access the API server using the curl command, as shown below:\ncurl $APISERVER --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure  By using the kubectl proxy we are bypassing the authentication for each and every request to the Kubernetes API.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/04_kubernetes_building_blocks/",
	"title": "04 - Kubernetes Building Blocks",
	"tags": [],
	"description": "",
	"content": "  Kubernetes Object Model Pods  Labels  Replication Controller Replica Set Deployments Namespaces  Kubernetes Object Model With each object, we declare our intent or the desired state under the spec section. When creating an object, the object\u0026rsquo;s configuration data section from below the spec field has to be submitted to the Kubernetes API server.\nExample of Deployment object configuration in YAML format.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  Pods A pod is the basic unit that Kubernetes deals with. Containers themselves are not assigned to hosts. Instead, closely related containers are grouped together in a pod. A pod generally represents one or more containers that should be controlled as a single “application”.\nA Pod is the smallest and simplest Kubernetes object. It is the unit of deployment in Kubernetes, which represents a single instance of the application. A Pod is a logical collection of one or more containers, which:\n Are scheduled together on the same host with the Pod Share the same network namespace Have access to mount the same external storage (volumes).  Below is an example of a Pod object\u0026rsquo;s configuration in YAML format:\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  # List all the Pods kubectl get pods # Displays details of Pod kubectl describe pod webserver-74d8bd488f-dwbzz  Labels Labels are key-value pairs attached to Kubernetes objects (e.g. Pods, ReplicaSets). Labels are used to organize and select a subset of objects, based on the requirements in place. Many objects can have the same Label(s). Labels do not provide uniqueness to objects. Controllers use Labels to logically group together decoupled objects, rather than using objects\u0026rsquo; names or IDs.\n# Lists Pods with labels kubectl get pods -L k8s-app,label2 # Select the Pods with a given Label kubectl get pods -l k8s-app=webserver  Replication Controller Although no longer a recommended method, a ReplicationController is a controller that ensures a specified number of replicas of a Pod is running at any given time. If there are more Pods than the desired count, a replication controller would terminate the extra Pods, and, if there are fewer Pods, then the replication controller would create more Pods to match the desired count. Generally, we don\u0026rsquo;t deploy a Pod independently, as it would not be able to re-start itself if terminated in error. The recommended method is to use some type of replication controllers to create and manage Pods.\nThe default controller is a Deployment which configures a ReplicaSet to manage Pods\u0026rsquo; lifecycle.\nReplica Set A ReplicaSet is the next-generation ReplicationController. ReplicaSets support both equality- and set-based selectors, whereas ReplicationControllers only support equality-based Selectors. Currently, this is the only difference.\nWith the help of the ReplicaSet, we can scale the number of Pods running a specific container application image. Scaling can be accomplished manually or through the use of an autoscaler.\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\n # List Replica Sets kubectl get replicasets  Deployments Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node\u0026rsquo;s controller manager, and it ensures that the current state always matches the desired state. It allows for seamless application updates and downgrades through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling.\n# Lists all the Deployments in a given namespace kubectl get deployments # Deleting Deployment (Along with ReplicaSet and Pods) kubectl delete deployments webserver  Deleting a Deployment also deletes the ReplicaSet and the Pods it created.\n Namespaces If multiple users and teams use the same Kubernetes cluster we can partition the cluster into virtual sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/05_authorization_access_control/",
	"title": "05 - Authentication, Authorization, and Admission Control",
	"tags": [],
	"description": "",
	"content": "  Authentication Authorization  Types of RoleBindings Admission Control  Demo - Authentication and Authorization  To access and manage any Kubernetes resource or object in the cluster, we need to access a specific API endpoint on the API server. Each access request goes through the following three stages:\n Authentication - Logs in a user Authorization - Authorizes the API requests added by the logged-in user. Admission Control - Software modules that can modify or reject the requests based on some additional checks, like a pre-set Quota.  Authentication Kubernetes does not have an object called user, nor does it store usernames or other related details in its object store. However, even without that, Kubernetes can use usernames for access control and request logging.\nKubernetes has two kinds of users:\n Normal Users They are managed outside of the Kubernetes cluster via independent services like User/Client Certificates, a file listing usernames/passwords, Google accounts, etc.\n Service Accounts With Service Account users, in-cluster processes communicate with the API server to perform different operations. Most of the Service Account users are created automatically via the API server, but they can also be created manually. The Service Account users are tied to a given Namespace and mount the respective credentials to communicate with the API server as Secrets.\n  For authentication, Kubernetes uses different authentication modules:\n Client Certificates To enable client certificate authentication, we need to reference a file containing one or more certificate authorities by passing the --client-ca-file=SOMEFILE option to the API server. The certificate authorities mentioned in the file would validate the client certificates presented to the API server. A demonstration video covering this topic is also available at the end of this chapter.\n Static Token File We can pass a file containing pre-defined bearer tokens with the --token-auth-file=SOMEFILE option to the API server. Currently, these tokens would last indefinitely, and they cannot be changed without restarting the API server.\n Bootstrap Tokens This feature is currently in beta status and is mostly used for bootstrapping a new Kubernetes cluster.\n Static Password File It is similar to Static Token File. We can pass a file containing basic authentication details with the --basic-auth-file=SOMEFILE option. These credentials would last indefinitely, and passwords cannot be changed without restarting the API server.\n Service Account Tokens This is an automatically enabled authenticator that uses signed bearer tokens to verify the requests. These tokens get attached to Pods using the ServiceAccount Admission Controller, which allows in-cluster processes to talk to the API server.\n OpenID Connect Tokens OpenID Connect helps us connect with OAuth2 providers, such as Azure Active Directory, Salesforce, Google, etc., to offload the authentication to external services.\n Webhook Token Authentication With Webhook-based authentication, verification of bearer tokens can be offloaded to a remote service.\n Authenticating Proxy If we want to program additional authentication logic, we can use an authenticating proxy.\n  Authorization After a successful authentication, users can send the API requests to perform different operations. Then, those API requests get authorized by Kubernetes using various authorization modules.\n Node Authorizer Attribute-Based Access Control (ABAC) Authorizer Webhook Authorizer Role-Based Access Control (RBAC) Authorizer  Role - With Role, we can grant access to resources within a specific Namespace.\nClusterRole - The ClusterRole can be used to grant the same permissions as Role does, but its scope is cluster-wide.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: lfs158 name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]  Types of RoleBindings RoleBinding\nIt allows us to bind users to the same namespace as a Role. We could also refer a ClusterRole in RoleBinding, which would grant permissions to Namespace resources defined in the ClusterRole within the RoleBinding’s Namespace.\nClusterRoleBinding\nIt allows us to grant access to resources at a cluster-level and to all Namespaces.\nTo enable the RBAC authorizer, we would need to start the API server with the \u0026ndash;authorization-mode=RBAC option\n Admission Control Admission control is used to specify granular access control policies, which include allowing privileged containers, checking on resource quota, etc. To use admission controls, we must start the Kubernetes API server with the --enable-admission-plugins, which takes a comma-delimited, ordered list of controller names:\n--enable-admission-plugins=NamespaceLifecycle,ResourceQuota,PodSecurityPolicy,DefaultStorageClass  Demo - Authentication and Authorization  Configuring User by assigning key and certificate Create Context with newly created User Add RBAC Role and rolebindings to namespace  minikube start kubectl config view # Create New Namespace for Demo kubectl create namespace lfs158 mkdir rbac \u0026amp;\u0026amp; cd rbac # Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool openssl genrsa -out student.key 2048 openssl req -new -key student.key -out student.csr -subj \u0026quot;/CN=student/O=learner\u0026quot; cat student.csr | base64 | tr -d '\\n' touch signing-request.yaml apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: \u0026lt;assign encoded value from cat command\u0026gt; usages: - digital signature - key encipherment - client auth kubectl create -f signing-request.yaml kubectl get csr kubectl certificate approve student-csr kubectl get csr # Generating User Certificate kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode \u0026gt; student.crt cat student.crt # Configuring Student User by assigning the key and certificate kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key # Create Student Context with Selected User kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student kubectl config view # Creating a Deployment with Nginx Image kubectl -n lfs158 create deployment nginx --image=nginx:alpine  From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context:\nkubectl --context=student-context get pods  Error from server (Forbidden): pods is forbidden: User \u0026ldquo;student\u0026rdquo; cannot list resource \u0026ldquo;pods\u0026rdquo; in API group \u0026ldquo;\u0026rdquo; in the namespace \u0026ldquo;lfs158\u0026rdquo;\n# Create RBAC Role to allow only get, watch, list actions in lfs158 namespace ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] ~/rbac$ kubectl create -f role.yaml ~/rbac$ kubectl -n lfs158 get roles NAME AGE pod-reader 57s  Create a YAML configuration file for a rolebinding object, which assigns the permissions of the pod-reader role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace:\n# Create RBAC Role Binding to the User ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME AGE pod-read-access 23s  Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context.\n~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-77595c695-f2xmd 1/1 Running 0 7m41s  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/06_services/",
	"title": "06 - Services",
	"tags": [],
	"description": "",
	"content": "  Services Service Object Example kube-proxy Service Discovery Servie Type  Cluster IP NodePort LoadBalancer ExternalIP ExternalName   Services  An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don’t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.\n Using the selectors app==frontend and app==db, we group Pods into two logical sets: one with 3 Pods, and one with a single Pod.\nWe assign a name to the logical grouping, referred to as a Service. In our example, we create two Services, frontend-svc, and db-svc, and they have the app==frontend and the app==db Selectors, respectively.\nServices can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets.\nService Object Example kind: Service apiVersion: v1 metadata: name: frontend-svc spec: selector: app: frontend ports: - protocol: TCP port: 80 targetPort: 5000  The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.\nWhile the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic.\n kube-proxy All worker nodes run a daemon called kube-proxy, which watches the API server on the master node for the addition and removal of Services and endpoints.\nService Discovery As Services are the primary mode of communication in Kubernetes, we need a way to discover them at runtime. Kubernetes supports two methods for discovering Services:\n Environment Variables DNS  Kubernetes has an add-on for DNS, which creates a DNS record for each Service and its format is my-svc.my-namespace.svc.cluster.local.\nServie Type While defining a Service, we can also choose its access scope. We can decide whether the Service:\n Is only accessible within the cluster Is accessible from within the cluster and the external world Maps to an entity which resides either inside or outside the cluster.  Access scope is decided by ServiceType, which can be configured when creating the Service.\n Cluster IP ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address is used for communicating with the Service and is accessible only within the cluster.\nNodePort The NodePort ServiceType is useful when we want to make our Services accessible from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. To access multiple applications from the external world, administrators can configure a reverse proxy - an ingress, and define rules that target Services within the cluster.\nkubectl expose pod \u0026lt;podname\u0026gt; --type=NodePort --port=80  LoadBalancer With the LoadBalancer ServiceType:\n NodePort and ClusterIP are automatically created, and the external load balancer will route to them The Service is exposed at a static port on each worker node The Service is exposed externally using the underlying cloud provider\u0026rsquo;s load balancer feature.  The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, and the Service will work the same way as a NodePort type Service.\n ExternalIP A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform or AWS.\nExternalName ExternalName is a special ServiceType, that has no Selectors and does not define any endpoints. When accessed within the cluster, it returns a CNAME record of an externally configured Service.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/07_deploying_standalone_app/",
	"title": "07 - Deploying Standalone App",
	"tags": [],
	"description": "",
	"content": "  Creating Deployment using YAML File Exposing Application Liveness and Readiness Probes  Liveness Probe Readiness Probe   Creating Deployment using YAML File webserver.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: webserver labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80  kubectl create -f webserver.yaml  This will also create ReplicaSet and Pods as defined in the YAML configuration.\nExposing Application webserver-svc.yaml\napiVersion: v1 kind: Service metadata: name: web-service labels: run: web-service spec: type: NodePort ports: - port: 80 protocol: TCP selector: app: nginx  kubectl create -f webserver-svc.yaml  Liveness and Readiness Probes Liveness Probe Liveness probe checks on an application\u0026rsquo;s health, and if the health check fails, kubelet restarts the affected container automatically.\nLiveness Probes can be set by defining:\n Liveness command  apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5   Liveness HTTP request  livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3   TCP Liveness Probe  livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20  Readiness Probe Readiness Probe can be used to ensure certain conditions are met before application can serve traffic. These conditions include ensuring that the depending service is ready, or acknowledging that a large dataset needs to be loaded, etc. In such cases, we use Readiness Probes and wait for a certain condition to occur.\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/08_kubernetes_volume_management/",
	"title": "08 - Kubernetes Volume Management",
	"tags": [],
	"description": "",
	"content": "  Volumes Volume Types PersistentVolumeClaims Using a Shared hostPath Volume Type  Volumes As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the kubelet will restart it with a clean slate, which means that it will not have any of the old data.\nTo overcome this problem, Kubernetes uses Volumes. A Volume is essentially a directory backed by a storage medium. The storage medium, content and access mode are determined by the Volume Type. In Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\nIn Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\n Volume Types  emptyDir - An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume\u0026rsquo;s life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever.\n hostPath - With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk - With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore - With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk - With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile - With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs - With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs - With nfs, we can mount an NFS share into a Pod. iscsi - With iscsi, we can mount an iSCSI share into a Pod. secret - With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. We will take a look at an example in a later chapter. configMap - With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim - We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. We will cover this in our next section.  PersistentVolumeClaims A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PersistentVolume resources based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.\nUsing a Shared hostPath Volume Type apiVersion: v1 kind: Pod metadata: name: share-pod labels: app: share-pod spec: volumes: - name: \u0026quot;host-volume\u0026quot; hostPath: path: \u0026quot;/home/docker/pod-volume\u0026quot; containers: - image: nginx name: nginx volumeMounts: - mountPath: \u0026quot;/usr/share/nginx/html\u0026quot; name: \u0026quot;host-volume\u0026quot; ports: - containerPort: 80 - image: debian name: debian volumeMounts: - mountPath: /host-vol name: \u0026quot;host-volume\u0026quot; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Hello Kubernetes \u0026gt; /host-vol/index.html; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/09_configmaps_secrets/",
	"title": "09 - ConfigMaps and Secrets",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps  Creating ConfigMaps Using CommandLine Using Configuration File Using Properties File Using ConfigMaps Inside Pods As Environment Variable As Volume  Secrets  Creating Secret Create a Secret from Literal and Display Its Details Create a Secret from YAML File Use Secrets Inside Pods   ConfigMaps ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories.\nCreating ConfigMaps Using CommandLine kubectl create configmap my-config \\ --from-literal=key1=value1 \\ --from-literal=key2=value2  Using Configuration File apiVersion: v1 kind: ConfigMap metadata: name: customer1 data: TEXT1: Customer1_Company TEXT2: Welcomes You COMPANY: Customer1 Company Technology Pct. Ltd.  Using Properties File permission-reset.properties\npermission=read-only allowed=\u0026quot;true\u0026quot; resetCount=3  kubectl create configmap permission-config --from-file=\u0026lt;path/to/\u0026gt;permission-reset.properties  Using ConfigMaps Inside Pods As Environment Variable Inside a Container, we can retrieve the key-value data of an entire ConfigMap or the values of specific ConfigMap keys as environment variables.\ncontainers: - name: myapp-full-container image: myapp envFrom: - configMapRef: name: full-config-map  containers: - name: myapp-specific-container image: myapp env: - name: SPECIFIC_ENV_VAR1 valueFrom: configMapKeyRef: name: config-map-1 key: SPECIFIC_DATA - name: SPECIFIC_ENV_VAR2 valueFrom: configMapKeyRef: name: config-map-2 key: SPECIFIC_INFO  As Volume We can mount a vol-config-map ConfigMap as a Volume inside a Pod. For each key in the ConfigMap, a file gets created in the mount path (where the file is named with the key name) and the content of that file becomes the respective key\u0026rsquo;s value:\ncontainers: - name: myapp-vol-container image: myapp volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: vol-config-map  Secrets Secret object can help by allowing us to encode the sensitive information before sharing it. With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, similar to ConfigMaps; thus, we can control how the information in a Secret is used, reducing the risk for accidental exposures. In Deployments or other resources, the Secret object is referenced, without exposing its content.\nIt is important to keep in mind that the Secret data is stored as plain text inside etcd, therefore administrators must limit access to the API server and etcd. A newer feature allows for Secret data to be encrypted at rest while it is stored in etcd; a feature which needs to be enabled at the API server level.\n Creating Secret Create a Secret from Literal and Display Its Details # Create Secret kubectl create secret generic my-password --from-literal=password=mysqlpassword # Get Secret kubectl get secret my-password # Describe Secret kubectl describe secret my-password  Create a Secret from YAML File echo mysqlpassword | base64 bXlzcWxwYXNzd29yZAo=  mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque data: password: bXlzcWxwYXNzd29yZAo=  Please note that base64 encoding does not mean encryption, and anyone can easily decode our encoded data\n mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque stringData: password: mysqlpassword  # Create Secret from YAML kubectl create -f mypass.yaml  Use Secrets Inside Pods Using Secrets as Environment Variables\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress env: - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: my-password key: password  Using Secrets as Files from a Pod\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress volumeMounts: - name: secret-volume mountPath: \u0026quot;/etc/secret-data\u0026quot; readOnly: true volumes: - name: secret-volume secret: secretName: my-password  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/10_ingress/",
	"title": "10 - Ingress",
	"tags": [],
	"description": "",
	"content": " With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.\n An Ingress is a collection of rules that allow inbound connections to reach the cluster Services.\n To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following:\n TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules  Name-Based Virtual Hosting virtual-host-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: virtual-host-ingress namespace: default spec: rules: - host: blue.example.com http: paths: - backend: serviceName: webserver-blue-svc servicePort: 80 - host: green.example.com http: paths: - backend: serviceName: webserver-green-svc servicePort: 80  fan-out-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: fan-out-ingress namespace: default spec: rules: - host: example.com http: paths: - path: /blue backend: serviceName: webserver-blue-svc servicePort: 80 - path: /green backend: serviceName: webserver-green-svc servicePort: 80  The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller, which we will discuss next.\n # Start the Ingress Controller with Minikube minikube addons enable ingress # Deploy an Ingress Resource kubectl create -f virtual-host-ingress.yaml  "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/11_advanced_topics/",
	"title": "11 - Advanced Topics",
	"tags": [],
	"description": "",
	"content": " Annotations With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:\n\u0026quot;annotations\u0026quot;: { \u0026quot;key1\u0026quot; : \u0026quot;value1\u0026quot;, \u0026quot;key2\u0026quot; : \u0026quot;value2\u0026quot; }  Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:\n Store build/release IDs, PR numbers, git branch, etc. Phone/pager numbers of people responsible, or directory entries specifying where such information can be found Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc.  apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019  Jobs and CronJobs A Job creates one or more Pods to perform a given task. The Job object takes the responsibility of Pod failures. It makes sure that the given task is completed successfully. Once the task is complete, all the Pods have terminated automatically. Job configuration options include\n parallelism - to set the number of pods allowed to run in parallel; completions - to set the number of expected completions; activeDeadlineSeconds - to set the duration of the Job; backoffLimit - to set the number of retries before Job is marked as failed; ttlSecondsAfterFinished - to delay the clean up of the finished Jobs.  Quota Management Autoscalling Autoscaling can be implemented in a Kubernetes cluster via controllers which periodically adjust the number of running objects based on single, multiple, or custom metrics. There are various types of autoscalers available in Kubernetes which can be implemented individually or combined for a more robust autoscaling solution:\nHorizontal Pod Autoscaler (HPA) HPA is an algorithm based controller API resource which automatically adjusts the number of replicas in a ReplicaSet, Deployment or Replication Controller based on CPU utilization.\nVertical Pod Autoscaler (VPA) VPA automatically sets Container resource requirements (CPU and memory) in a Pod and dynamically adjusts them in runtime, based on historical utilization data, current resource availability and real-time events.\nCluster Autoscaler Cluster Autoscaler automatically re-sizes the Kubernetes cluster when there are insufficient resources available for new Pods expecting to be scheduled or when there are underutilized nodes in the cluster.\nDaemonSets A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\n running a cluster storage daemon, such as glusterd, ceph, on each node. running a logs collection daemon on every node, such as fluentd or logstash. running a node monitoring daemon on every node, such as Prometheus Node Exporter, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or Instana Agent.  StatefulSets The StatefulSet controller is used for stateful applications which require a unique identity, such as name, network identifications, strict ordering, etc. For example, MySQL cluster, etcd cluster.\nThe StatefulSet controller provides identity and guaranteed ordering of deployment and scaling to Pods. Similar to Deployments, StatefulSets use ReplicaSets as intermediary Pod controllers and support rolling updates and rollbacks.\nKubernetes Federation With Kubernetes Cluster Federation we can manage multiple Kubernetes clusters from a single control plane. We can sync resources across the federated clusters and have cross-cluster discovery. This allows us to perform Deployments across regions, access them using a global DNS record, and achieve High Availability.\nHelm Chart To deploy an application, we use different Kubernetes manifests, such as Deployments, Services, Volume Claims, Ingress, etc. Sometimes, it can be tiresome to deploy them one by one. We can bundle all those manifests after templatizing them into a well-defined format, along with other metadata. Such a bundle is referred to as Chart. These Charts can then be served via repositories, such as those that we have for rpm and deb packages.\nHelm is a package manager (analogous to yum and apt for Linux) for Kubernetes, which can install/update/delete those Charts in the Kubernetes cluster.\nHelm has two components:\n A client called helm, which runs on your user\u0026rsquo;s workstation A server called tiller, which runs inside your Kubernetes cluster.  The client helm connects to the server tiller to manage Charts.\nSecurity Contexts and Pod Security Policies At times we need to define specific privileges and access control settings for Pods and Containers. Security Contexts allow us to set Discretionary Access Control for object access permissions, privileged running, capabilities, security labels, etc. However, their effect is limited to the individual Pods and Containers where such context configuration settings are incorporated in the spec section.\nIn order to apply security settings to multiple Pods and Containers cluster-wide, we can define Pod Security Policies. They allow more fine-grained security settings to control the usage of the host namespace, host networking and ports, file system groups, usage of volume types, enforce Container user and group ID, root privilege escalation, etc.\nNetwork Policies Kubernetes was designed to allow all Pods to communicate freely, without restrictions, with all other Pods in cluster Namespaces. In time it became clear that it was not an ideal design, and mechanisms needed to be put in place in order to restrict communication between certain Pods and applications in the cluster Namespace. Network Policies are sets of rules which define how Pods are allowed to talk to other Pods and resources inside and outside the cluster. Pods not covered by any Network Policy will continue to receive unrestricted traffic from any endpoint. Network Policies are very similar to typical Firewalls. They are designed to protect mostly assets located inside the Firewall but can restrict outgoing traffic as well based on sets of rules and policies.\nThe Network Policy API resource specifies podSelectors, *Ingress* and/or *Egress* policyTypes, and rules based on source and destination ipBlocks and ports. Very simplistic default allow or default deny policies can be defined as well. As a good practice, it is recommended to define a default deny policy to block all traffic to and from the Namespace, and then define sets of rules for specific traffic to be allowed in and out of the Namespace. Let\u0026rsquo;s keep in mind that not all the networking solutions available for Kubernetes support Network Policies. Review the Pod-to-Pod Communication section from the Kubernetes Architecture chapter if needed. By default, Network Policies are namespaced API resources, but certain network plugins provide additional features so that Network Policies can be applied cluster-wide.\nMonitoring and Logging In Kubernetes, we have to collect resource usage data by Pods, Services, nodes, etc., to understand the overall resource consumption and to make decisions for scaling a given application. Two popular Kubernetes monitoring solutions are the Kubernetes Metrics Server and Prometheus.\n Metrics Server\nMetrics Server is a cluster-wide aggregator of resource usage data - a relatively new feature in Kubernetes.  PrometheusPrometheus, now part of CNCF (Cloud Native Computing Foundation), can also be used to scrape the resource usage from different Kubernetes components and objects. Using its client libraries, we can also instrument the code of our application.  Another important aspect for troubleshooting and debugging is Logging, in which we collect the logs from different components of a given system. In Kubernetes, we can collect logs from different cluster components, objects, nodes, etc. Unfortunately, however, Kubernetes does not provide cluster-wide logging by default, therefore third party tools are required to centralize and aggregate cluster logs. The most common way to collect the logs is using Elasticsearch, which uses fluentd with custom configuration as an agent on the nodes. fluentd is an open source data collector, which is also part of CNCF.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/12_resources/",
	"title": "12 - Resources",
	"tags": [],
	"description": "",
	"content": " Labs CKA Curriculum   "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/2018-05-30-aws/",
	"title": "Amazon Web Services (AWS)",
	"tags": [],
	"description": "",
	"content": " IAM Identity and Access Management\n Add User  AWS Mobile Hub  A dashboard for easily manage multiple AWS services Provides a graphical abstraction over DynamoDB, Cognito, Pinpoint and other services.  Features  Secure Authentication (Amazon Cognito) Database (Amazon DynamoDB) Storage (Amazon S3) Cloud API (API Gateway) (Amazon Lambda) Hosting \u0026amp; Streaming (S3 \u0026amp; Cloudfront) Analytics and Notifications (Pinpoint)  AWS Mobile CLI  A command line tool for setting up new projects using AWS Mobile Hub  Install AWS Mobile CLI npm install -g awsmoible -cli  Configure the CLI with your AWS credentials awsmobile configure  Setup Backend with your App awsmobile init  Connect your app with configured AWS feautures import Amplify from 'aws-amplify'; import awsmobile from './YOUR-PATH-TO/aws-exports'; Amplify.configure(awsmobile);  Add User Sign In awsmobile user-signin enable awsmobile push awsmobile run  Add NoSQL Database awsmobile database enable --prompt awsmobile push awsmobile run  Console awsmobile console  Hosting and Streaming awsmobile publish  AWS Amplify  A javascript library with a bunch of helper functions for working with AWS Mobile Hub  AWS AppSync  GraphQL as a service  References  https://docs.aws.amazon.com/aws-mobile/latest/developerguide/aws-mobile-cli-reference.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/",
	"title": "Arrow Functions",
	"tags": [],
	"description": "",
	"content": " Arrow Function Express allows you to write shorter syntax than it\u0026rsquo;s predecessor Function expression. In addition and more exciting is how the new Arrow function bind their context.\n(param1, param2, param3) =\u0026gt; { statements } singleParam =\u0026gt; { statements } () =\u0026gt; { statements }  Example var materials = [ \u0026#39;Iron\u0026#39;, \u0026#39;Calcium\u0026#39;, \u0026#39;Sodium\u0026#39;, \u0026#39;Magnanese\u0026#39; ] materials.map(material =\u0026gt; material.length)  An arrow function does not newly define its own this when it\u0026rsquo;s being executed.The value of this is always inherited from the enclosing scope.\n// ES5 function CounterES5(){ this.seconds = 0; window.setInterval(function() { this.seconds++; console.log(seconds); }.bind(this), 1000); } //ES6 function CounterES6(){ this.seconds =0; window.setInterval( () =\u0026gt; { this.seconds++; console.log(seconds) },1000 ); }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-28-ubuntu-packages/",
	"title": "Backup and Restore Ubuntu Packages",
	"tags": [],
	"description": "",
	"content": " List Packages Installed dpkg -l\nCreate Backup of What Packages Installed dpkg --get-selection \u0026gt; list.txt\nRestore dpkg --clear-selections sudo dpkg --set-selections \u0026lt; list.txt sudo apt-get autoremove sudo apt-get dselect-upgrade  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/",
	"title": "Basic Essentials",
	"tags": [],
	"description": "",
	"content": " Creting Instance (Directly) # Create Compute Instance gcloud compute instances create gcelab2 --machine-type n1-standard-2 \\ --zone us-central1-c  Using Instance Templates / Instance Groups # Create Instance Template gcloud compute instance-templates create nginx-template \\ --metadata-from-file startup-script=startup.sh # Create Target Pool gcloud compute target-pools create nginx-pool # Create Instance Group gcloud compute instance-groups managed create nginx-group \\ --base-instance-name nginx \\ --size 2 \\ --template nginx-template \\ --target-pool nginx-pool # List Instances gcloud compute instances list  Create Filewall # Create Filewall rule to Allow 80 gcloud compute firewall-rules create www-firewall --allow tcp:80  SSH Instance gcloud compute ssh gcelab2 --zone us-central1-c   gcloud is a command-line tool for Google Cloud Platform\n  gsutil is a command-line tool to mange Cloud Storage resources\n Config # List Config gcloud config list # Sets Default Zone to us-central1-a gcloud config set compute/zone us-central1-a  Kubernetes Cluster Engine A cluster consists of at least one cluster master machine and multiple worker machines called nodes. Nodes are Compute Engine virtual machine (VM) instances that run the Kubernetes processes necessary to make them part of the cluster.\n# Create Cluster gcloud container clusters create my-precious-cluster # Updates a kubeconfig file with appropriate credentials to point kubectl at a specific cluster in GKE gcloud container cluster get-credentials my-precious-cluster # Delete Cluster gcloud container clusters delete my-precious-cluster  Network Load Balancer Network load balancing allows you to balance the load of your systems based on incoming IP protocol data, such as address, port, and protocol type. You also get some options that are not available, with HTTP(S) load balancing. For example, you can load balance additional TCP/UDP-based protocols such as SMTP traffic. And if your application is interested in TCP-connection-related characteristics, network load balancing allows your app to inspect the packets, where HTTP(S) load balancing does not.\n# Create Forwarding Rules gcloud compute forwarding-rules create nginx-lab \\ --region us-central1 --port=80 --target-pool nginx-pool # List Forwarding Rules gcloud compute forwarding-rules list  Creating HTTP(s) Load Balancer # Create Health Check gcloud compute http-health-checks create http-basic-check # Defining a HTTP service and map a port name to the relevant port gcloud compute instance-groups manged set-named-ports nginx-group --named-ports http:80 # Creating Backend Service gcloud compute backend-services create nginx-backend \\ --protocol HTTP \\ --http-health-checks http-basic-check \\ --global # Adding instance group to the backend services gcloud compute backend-services add-backend nginx-backend \\ --instance-group nginx-group \\ --instance-group-zone us-central1-a \\ --global # Create a default URL map that directs all incoming requests to all your instances gcloud compute url-maps create web-map --default-service nginx-backend # Create a target HTTP proxy to route requests to URL map gcloud compute target-http-proxies create http-lb-proxy --url-map web-map # Create global forwarding rule to handle and route incoming requests gcloud compute forwarding-rules create http-content-rule \\ --global --target-http-proxy http-lb-proxy --ports 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/chef/2016-02-29-berks/",
	"title": "Berkshelf",
	"tags": [],
	"description": "",
	"content": " Berkshelf Reference External Dependent Cookbooks so that it can download those cookbooks rather than manually downloading via knife cookbook command.\nknife cookbook site\nBerkshelf lets you treat your cookbooks the way you treat gem in a Ruby project. When external cookbooks are used, Berkshelf doesn\u0026rsquo;t requite knife cookbook site to install community cookbooks.\nImplementing Berkshelf gem install berkshelf\nInstall Cookbooks via Berks berks install\nUpload berks to Chef Server berks upload \u0026lt;cookbook\u0026gt;\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/bookmarks/",
	"title": "Bookmarks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/chef/",
	"title": "Chef",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/chef/2018-01-30-chef-databags/",
	"title": "Chef Databags",
	"tags": [],
	"description": "",
	"content": " Databags are global variables that is stored as JSON Data and is accessible from Chef Server. A data bag is indexed for searching and can be loaded by recipe or accessed during a search.\nCreating Data Bag (Using Knife) $ knife data bag create DATA_BAG_NAME (DATA_BAG_ITEM) knife data bag create TEST_BAG  Adding File to Data Bag knife data bag from file TEST_BAG test.json  Data Bag Items A data bag is container of related data bag items, where each individual data bag item is a JSON file.\nUsing Data Bags data_bag_item('keystore_file', 'dev')  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/",
	"title": "Cloud Storage",
	"tags": [],
	"description": "",
	"content": " Create a Stoage Bucket gsutil mb gs://unique-name  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/",
	"title": "Continuous Delivery with Jenkins in Kubernetes Engine",
	"tags": [],
	"description": "",
	"content": "# Create Kubernetes Cluster gcloud container clusters create jenkins-cd \\ --num-nodes 2 \\ --machine-type n1-standard-2 \\ --scopes \u0026quot;https://www.googleapis.com/auth/projecthosting,cloud-platform\u0026quot; # Update KubeConfig with Cluster credentials gcloud container clusters get-credentials jenkins-cd # Verify Kubernetes can connect to GCP Kubernetes Cluster kubectl cluster-info  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-01-29-dns-basics/",
	"title": "DNS Basics",
	"tags": [],
	"description": "",
	"content": " DNS (Domain Name System) is essential component of modern internet communication. It allows us to reference computers by human friendly names instead of IP Addresses.\nTerminologies  Domain Name IP Address  DNS Lookup using Dig Dig is a flexible tool for interrogating DNS name servers. It performs DNS lookup and is very helpful to troubleshoot DNS problems.\ndig \u0026lt;serverName\u0026gt; +nostats +nocomments +nocmd  $ dig google.com +nostats ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; google.com +nostats ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 9995 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;google.com.\tIN\tA ;; ANSWER SECTION: google.com.\t142\tIN\tA\t172.217.8.206   The first line tell us the version of dig (9.10.6) command Next, dig shows the header of the response it received from the DNS server The question section tells us about the A Record and IN means this is an internet lookup The answer section tells us that google.com has IP address of 172.217.8.206  References  DNS Concepts  "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/destructuring/",
	"title": "Destructuring JavaScript Objects",
	"tags": [],
	"description": "",
	"content": "const person = { firstName: \u0026#39;Aayush\u0026#39;, lastName: \u0026#39;Tuladhar\u0026#39;, country: \u0026#39;Nepal\u0026#39;, twitter: \u0026#39;@aayushtuladhar\u0026#39; } /* Problem */ const first = person.firstName; const last = person.lastName; console.log(`Hello ${first}${last}`); /* Solution */ const { firstName, lastName } = person; console.log(`Hello ${firstName}${lastName}`); /* ------------------ */ const art = { first: \u0026#39;ART\u0026#39;, last: \u0026#39;Ratna\u0026#39;, links: { social: { twitter: \u0026#39;https://twitter.com/aayushtuladhar\u0026#39;, facebook: \u0026#39;https://facebook.com/aayush.tuladhar\u0026#39;, }, web: { blog: \u0026#39;https://aayushtuladhar.com\u0026#39; } } }; const { twitter, facebook } = art.links.social; console.log(twitter); console.log(facebook);  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "All the Things DevOps\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/",
	"title": "Digital Learning Notebook",
	"tags": [],
	"description": "",
	"content": " My Digital Learning Notebook Welcome to my digital notebook that I am sharing my learnings to capture important notes regarding various aspects of Software Engineering, Web Development and Cloud Engineering and other miscellenious topics.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-17-docker-compose/",
	"title": "Docker Compose",
	"tags": [],
	"description": "",
	"content": " Docker Compose is used to run multiple containers as a single service.\n# docker-compose.yml version: '2' services: web: build: . # build from Dockerfile context: ./Path dockerfile: Dockerfile ports: - \u0026quot;5000:5000\u0026quot; volumes: - .:/code redis: image: redis  Command Line # Start Service docker-compose start # Stop Service docker-compose stop # Pause Service docker-compose pause # UnPause Service docker-compose unpause # List containers docker-compose ps # Create and start containers docker-compose up # Stop and remove containers, networks, images, and volumes docker-compose down  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2018-04-19-drone/",
	"title": "Drone",
	"tags": [],
	"description": "",
	"content": "  Creating Pipeline  Images Cloning Commands Services Plugins  Running Drone Locally Drone Secrets  Repo Level Secrets Org Level Secrets Using Secrets in Pipeline   Drone is a CI/CD platform built on Docker and written in Go.\nCreating Pipeline Drone pipeline are written in .drone.yml file in the root of the repository.\n Pipelines are event based, which can be triggered via push, pull_request, tag and deployment events\npipeline: backend: image: golang commands: - go get - go build - go test frontend: image: node:6 commands: - npm install - npm test notify: image: plugins/slack channel: developers username: drone   Images Drone executes your build inside an ephemeral Docker Image, which means you don\u0026rsquo;t have to setup or install any repository dependencies.\nCloning Drone automatically clones your repository into a local volume that is mounted into each Docker container.\nCommands Drone mounts the workspace into your build container (Defined Image) and executes bash commands inside your build container, using the root of your repository as the working directory.\nServices Drone supports launching service containers as part of the build process. This can be very helpful when your unit tests require database access etc.\nPlugins Drone supports publish, deployments and notification capabilities through external plugins.\nRunning Drone Locally drone exec  Drone Secrets Repo Level Secrets # Create Value Secret (Concealed) drone secret add --conceal \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create Value Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create File Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt; # View Secret for a Repo drone secret ls \u0026lt;repo_path\u0026gt;  Org Level Secrets # Create Org Level Secret (Concealed) drone org secret add --conceal \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level File Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt;  Using Secrets in Pipeline pipeline: docker: image: plugins/docker - username: ${DOCKER_USERNAME} - password: ${DOCKER_PASSWORD} + secrets: [ docker_username, docker_password ]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Learning the Basics Stuff Starting Server hugo server -D  Documentation Learn\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/reactjs/redux/",
	"title": "Getting Started with Redux",
	"tags": [],
	"description": "",
	"content": " Redux gives you a store, and lets you keep state in it, and get state out, and respond when the state changes. But that’s all it does.\nIt’s actually react-redux that lets you connect pieces of the state to React components.\n The state is the data, and the store is where it’s kept\n Redux Store Redux Reducer Reducer\u0026rsquo;s job is to take the current state and action and return the new state. It has another job, too. It should return the initial state the first time it\u0026rsquo;s called.\nReducer Rule # 1 = Never return undefined from a reducer Reduce Rule # 2 = Reduces must be a pure functions (They can\u0026rsquo;t modify their arguments, and they can\u0026rsquo;t have side effects)\nRedux Actions An action is Redux-speak for a plain object with a property called type. In order to keep things sane and maintainable, we Redux users usually give our actions types that are plain strings, and often uppercased, to signify that they’re meant to be constant values.\nAn action object describes a change you want to make (like “please increment the counter”) or an event that happenend (like “the request to the server failed with this error”).\nIn order to make an action DO something, you need to dispatch it.\nRedux Dispatch The store we created earlier has a built-in function called dispatch. Call it with an action, and Redux will call your reducer with that action (and then replace the state with whatever your reducer returned).\nReferences "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/",
	"title": "Google Cloud Platform",
	"tags": [],
	"description": "",
	"content": "All the Google Cloud Platform Stuff\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-07-06-gradle-wrapper/",
	"title": "Gradle Wrapper",
	"tags": [],
	"description": "",
	"content": " Gradle Wrapper The Gradle wrapper allows you to run a Gradle task without requiring that Gradle is installed on your system.\nCreating Gradle Wrapper task wrapper(type: Wrapper) { gradleVersion = '2.10' //we want gradle 2.10 to run this project }  Running Gradle Wrapper gradle wrapper\nFollowing files will be created:\n|-gradle |--- wrapper |--- gradle-wrapper.jar |--- gradle-wrapper.properties |-gradlew |-gradlew.bat  Gradle wrapper are useful when you want to run gradle command without installing gradle\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-09-22-install-docker-centos-7/",
	"title": "Installing Docker CE on Centos 7",
	"tags": [],
	"description": "",
	"content": "  Install Docker Community Edition Verify  Install Docker Community Edition sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast sudo yum install docker-ce sudo systemctl start docker  Verify sudo docker run hello-world  "
},
{
	"uri": "http://learn.aayushtuladhar.com/java/",
	"title": "Java",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": "All the JavaScript Stuff\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-08-20-jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "  Installing Plugins  Pipeline Script   Installing Jenkins docker pull jenkins/jenkins # Persist Jenkins Data within the Docker Container docker volume create jenkins-data docker run --name jenkins-production \\ --detach \\ -p 50000:50000 \\ -p 8080:8080 \\ -v jenkins-data:/var/jenkins_home \\ jenkins/jenkins:2.164.2  Access Jenkins at http://localhost:8080/\nInstalling Plugins  Blueocean plugin  Manage Jenkins \u0026gt; Manage Plugins\nPipeline Script Hello World Pipeline Script\npipeline { agent none environment { APPLICATION_NAME = 'hello-jenkins-pipeline' } stages { stage('build') { steps { echo \u0026quot;Hello World\u0026quot; } } } }  Pipeline script to Build Another Jenkins Job\npipeline { agent none stages{ stage('build'){ steps { echo \u0026quot;Hello World\u0026quot; build 'project2-child' } } } }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-13-jenkins-shared-libs/",
	"title": "Jenkins Shared Library",
	"tags": [],
	"description": "",
	"content": " Jenkins Shared library is the concept of having a common pipeline code in the version control system that can be used by any number of pipeline just by referencing it. In fact, multiple teams can use the same library for their pipelines. Pipeline has support for creating \u0026ldquo;Shared Libraries\u0026rdquo; which can be defined in external source control repositories and loaded into existing Pipelines.\n A shared library is a collection of independent Groovy scripts which you pull into your Jenkinsfile at runtime.\n Getting Started with Shared Library A Shared Library is defined with a name, a source code retrieval method such as by SCM, and optionally a default version. The name should be a short identifier as it will be used in scripts.\nDirectory structure jenkins-shared-library |____vars |____src |____resources  Resources  https://devopscube.com/create-jenkins-shared-library/ https://tomd.xyz/articles/jenkins-shared-library/ https://dev.to/kuperadrian/how-to-setup-a-unit-testable-jenkins-shared-pipeline-library-2e62 https://tomd.xyz/articles/jenkins-shared-library/  "
},
{
	"uri": "http://learn.aayushtuladhar.com/chef/2016-04-05-chef-knife-commands/",
	"title": "Knife Commands",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/introduction-to-kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. This tutorial provides an overview of different kind of features and functionalities of Kubernetes and teaches how to manage the containerized infrastructure and application deployment.\nFeatures Automatic bin packing\nKubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability.\nSelf-healing\nKubernetes automatically replaces and reschedules containers from failed nodes. It kills and restarts containers unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers.\nHorizontal scaling\nWith Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization.\nService discovery and Load balancing\nContainers receive their own IP addresses from Kubernetes, white it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set.\nAutomated rollouts and rollbacks\nKubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application\u0026rsquo;s health to prevent any downtime.\nSecret and configuration management\nKubernetes manages secrets and configuration details for an application separately from the container image, in order to avoid a re-build of the respective image. Secrets consist of confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub.\nStorage orchestration\nKubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, or network storage systems.\nBatch execution\nKubernetes supports batch execution, long-running jobs, and replaces failed containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/java/the-basics/lambda_expressions/",
	"title": "Lambda Expressions",
	"tags": [],
	"description": "",
	"content": " Lambda expressions are not unknown to many of us who have worked on other popular programming languages like Scala. In Java programming language, a Lambda expression (or function) is just an anonymous function, i.e., a function with no name and without being bounded to an identifier. They are written exactly in the place where it’s needed, typically as a parameter to some other function.\nThe most important features of Lambda Expressions is that they execute in the context of their appearance. So, a similar lambda expression can be executed differently in some other context (i.e. logic will be same but results will be different based on different parameters passed to function).\nSynatax // This function takes two parameters and returns their sum //(parameters) -\u0026gt; expression (x, y) -\u0026gt; x + y (int a, int b) -\u0026gt; a * b //(parameters) -\u0026gt; { statements; } (x, y) -\u0026gt; { x+y; } //() -\u0026gt; expression () -\u0026gt; z () -\u0026gt; 100  Functional Interface A functional interface is an interface with a single abstract method (SAM). A class implements any interface by providing implementations for all the methods in it.\n@FunctionalInterface public interface Runnable { public abstract void run(); }  Type in which lambda expressions are converted, are always of functional interface type.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_2/",
	"title": "Layouts",
	"tags": [],
	"description": "",
	"content": " Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading  Renders to:\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading Typography I am just being Bold\nI Love my Italics Style\nStrike Through\n I love to give a quotation\n Images ![Minion](https://octodex.github.com/images/minion.png)  Resizing Images ![Minion](https://octodex.github.com/images/minion.png?width=20pc)  Buttons Get Grav  Note / Info/ Tip / Warning A notice disclaimer\n An information disclaimer\n A tip disclaimer\n A warning disclaimer\n Expand   Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-27-issue-remote-commands/",
	"title": "Linux Command Line Hacks",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-02-26-mqtt/",
	"title": "Mqtt",
	"tags": [],
	"description": "",
	"content": " Introduction  MQTT is a featherweight, ISO complaint PUB-SUB messaging protocol. Designed for low powered devices PRAM consistent: Guaranteed in-order delivery per-publisher Multiple Transport: TCP, TLS, Websockets Flexible: Arbitrary message up to 256 MB Topics can also be used for Key-Value storage  Topic based Pub/Sub  Decouples Publisher and Subscribers  Quality of Service  QoS 0 - \u0026ldquo;Fire and Forget\u0026rdquo; Q0S 1 - \u0026ldquo;At least once\u0026rdquo; QoS 2 - \u0026ldquo;Exactly once; 2 phase commit\u0026rdquo;  Ideal for intermittent connectivity; Sessions may last weeks or months Supports Disconnect \u0026amp; Last Will \u0026amp; Testament message\n MQTT supports Retained messages which are automatically delivered when a client subscribes to a topic.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-10-01-network-command-essentials/",
	"title": "Network Tools / Command Essentials",
	"tags": [],
	"description": "",
	"content": " Must Know Network Tools / Commands NetStat netstat stands for network statistics. This command displays incoming and outgoing network connections as well as other network information. The netstat utility can show you the open connections on your computer, which programs are making which connections, how much data is being transmitted, and other information.\n## List all connections netstat -a ## List TCP or UDP Connections netstat -at //TCP Connections netstat -au //UDP Connections ## List all Ports being Listened to netstat -an | grep \u0026quot;LISTEN \u0026quot;  IpTables ## Open 9001 Port sudo iptables -A INPUT -p tcp --dport 9001 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 9001 -m conntrack --ctstate ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp --dport 3306 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 3306 -m conntrack --ctstate ESTABLISHED -j ACCEPT  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-10-8-owasp/",
	"title": "OWASP Dependency Check",
	"tags": [],
	"description": "",
	"content": " OWASP dependency-check is an open source solution that can be used to scan Java and .NET applications to identify the use of known vulnerable components.\nLink\nAdding OWASP Check to Gradle Projects Adding following fragments to build.gradle\nbuildscript { repositories { mavenCentral() } dependencies { classpath 'org.owasp:dependency-check-gradle:5.2.2' } } plugins { id 'org.owasp.dependencycheck' version '5.2.2' }  Gradle Task\n./gradlew dependencyCheckAggregate  Configuring DependencyCheck dependencyCheck { format='ALL' cveValidForHours=1 outputDirectory = file(\u0026quot;$project.buildDir/reports/dependencycheck\u0026quot;) suppressionFile = 'config/dependencyCheck/suppressions.xml' failBuildOnCVSS = 5 failOnError = true }  Sample Suppressions.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;suppressions xmlns=\u0026quot;https://jeremylong.github.io/DependencyCheck/dependency-suppression.1.1.xsd\u0026quot;\u0026gt; \u0026lt;suppress\u0026gt; \u0026lt;notes\u0026gt;\u0026lt;![CDATA[file name: postgresql-42.2.5.jar]]\u0026gt;\u0026lt;/notes\u0026gt; \u0026lt;gav regex=\u0026quot;true\u0026quot;\u0026gt;^org\\.postgresql:postgresql:.*$\u0026lt;/gav\u0026gt; \u0026lt;cve\u0026gt;CVE-2016-7048\u0026lt;/cve\u0026gt; \u0026lt;/suppress\u0026gt; \u0026lt;/suppressions\u0026gt;  Resources  Dependency Check Gradle  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-10-3-openshift-cheatsheet/",
	"title": "OpenShift Cheatsheet",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-03-24-open-stack-cli/",
	"title": "OpenStack",
	"tags": [],
	"description": "",
	"content": " Getting Started with OpenStack What is OpenStack OpenStack is elastic cloud software that provides software developers with the ability to control the virtual infrastructure on which to deploy their applications. It is a set of software tools for building and managing cloud computing platforms for public and private clouds.\nIt accelerates time-to-market by dramatically reducing application provisioning times, giving companies full control of their software development lifecycle and ultimately giving them a significant competitive advantage. OpenStack also provides application portability by allowing enterprises to freely move between OpenStack clouds without vendor lock-in.\nIntroduction to OpenStack OpenStack lets users deploy virtual machines and other instances that handles different tasks for managing a cloud environment on the fly.\nKey Components of OpenStack  Nova - Primary Computing Engine behind OpenStack. It is used for deploying and managing large numbers of virtual machines and other instances to handle computing tasks.\n Swift - Storage System for Objects and Files.\n   Network Commands ### List floating IP Pools openstack ip floating pool list ### Get FloatingIP from the external Network openstack ip floating create \u0026lt;poolname\u0026gt; openstack ip floating create ext_vlan1767_net ### Create a Neutron Network neutron net-create ART-Land #### View list of Networks neutron net-list #### Create network for project neutron net-create \u0026lt;NetworkName\u0026gt; #### Creating Subnet neutron subnet-create --name ART-Subnet ART-Land 192.168.10.0/24 #### View Subnet neutron subnet show \u0026lt;subnetName\u0026gt; neutron subnet-show ART-Subnet ### Create Router neutron router-create \u0026lt;routername\u0026gt; neutron router-create ART-router ### Add Interface to router neutron router-interface-add \u0026lt;router\u0026gt; \u0026lt;subnet\u0026gt; neutron router-interface-add ART-router ART-Subnet ### Attach Router to External Network neuter router-gateway-set ART-router ext_vlan1767_net # Security Groups and Rules ### Creating Security Group openstack security group \u0026lt;groupname\u0026gt; --description 'Allow SSH and pings' openstack security group create BasicSG --description 'Allow SSH and Pings' ### Adding Rules to Security Groups #### Create Rule to allow SSH openstack security group rule create BasicSG --proto tcp --dst-port 22:22 #### Create Rule to allow Ping from any Source IP openstack security group rule create BasicSG --proto icmp --dst-port -1 ### Create Security Group to Open all TCP internal openstack security group create OpenSG --description 'All TCP internal' #### Add a new security rule to our OpenSG group that will allow all traffic from the private subnet in openstack security group rule create OpenSG --proto tcp --dst-port 1:65535 --src-ip 192.168.10.0/24  Creating Instances openstack server create --flavor smem-4vcpu --image ubuntu-latest --security-group BasicSG --nic net-id=ART-Land --key-name ArtKey ARTBastionHost  Working with Stacks ### Launching a Stacks openstack stack create --template \u0026lt;templateFile\u0026gt; --environment \u0026lt;envFile\u0026gt; \u0026lt;stackName\u0026gt; ### Listing Stacks openstack stack list openstack flavor list openstack image list  Neutron  Provides API to allow your users to create networks, subnets, routers etc.  Network - An isolated L2 segment, analogous to a VLAN in physical networking.\nSubnet - A block of v4 of v6 IP address and associated configuration state.\nPort - A connection point for attaching a single device (NIC) to Neutron network.\nopenstack network list # Create Network openstack network create mynetwork # Create Subnet openstack subnet create --network mynetwork \\ --subnet-range 10.0.0.0/29 --dns-nameserver 8.8.8.8 mynetwork-subnet  Nova Nova sits on top of Hypervisor Nova Flavors\n# List images openstack image list # List flavors openstack flavor list # List networks openstack network list # Boot an instance using flavor and image names (if names are unique) openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyFirstInstance # Boot another instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MySecondInstance # List instances, notice status of instance openstack server list # Show details of instance openstack server show MyFirstInstance # View console log of instance openstack console log show MyFirstInstance # Get the console url of the instance openstack console url show MyFirstInstance  Cinder Creates a volume on a Hypervisor. Allowing instances to attaching and detaching the volume without losing it\u0026rsquo;s persistence.\nCinder is powered by LVM and iSCSI. Cinder is Inspired by Elastic Block Storage.\nopenstack volume create --size 1 --type sata MyFirstVolume openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID  Use \u0026lsquo;myadmin\u0026rsquo; credentials source ~/credentials/myadmin\nVerify Cinder services are functioning and checking in :-) openstack volume service list  Use \u0026lsquo;myuser\u0026rsquo; credentials source ~/credentials/myuser  Create a new volume openstack volume create --size 1 --type sata MyFirstVolume  Boot an instance to attach volume to openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance  List instances, notice status of instance openstack server list  List volumes, notice status of volume openstack volume list  Get volume and instance IDs MYFIRSTVOLUME_ID=`openstack volume show MyFirstVolume | awk '/ id / { print $4 }'` MYVOLUMEINSTANCE_ID=`openstack server show MyVolumeInstance | awk '/ id / { print $4 }'` # Attach volume to instance after instance is active, and volume is available openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # Confirm the volume has been attached openstack volume list # Get the console url of the instance openstack console url show MyVolumeInstance # Login to the instance # username: cirros # password: cubswin:) # From inside the instance # List storage devices sudo fdisk -l # Make filesystem on volume sudo mkfs.ext3 /dev/vdb # Create a mountpoint sudo mkdir /extraspace # Mount volume at mountpoint sudo mount /dev/vdb /extraspace # Create a file on volume sudo touch /extraspace/helloworld.txt sudo ls /extraspace # Unmount volume sudo umount /extraspace # Log out of instance exit # Detach volume from instance openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # List volumes, notice status of volume openstack volume list # Delete instance openstack server delete MyVolumeInstance  # Use 'myuser' credentials source ~/credentials/myuser # Create a new instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyLastInstance --wait # Get the console url of the instance openstack console url show MyLastInstance # Login to the instance # username: cirros # password: cubswin:) # Ping openstack.org ( should fail, as we have nothing routing between the tenant network (private) and provider network (public) ) ping openstack.org # Create a router openstack router create MyRouter # Connect the private-subnet to the router openstack router add subnet MyRouter private-subnet # List interfaces attached to router openstack port list --router MyRouter # Connect router to public network neutron router-gateway-set MyRouter public # Examine details of router openstack router show MyRouter # Get instance ID for MyLastInstance MYLASTINSTANCE_ID=`openstack server show MyLastInstance | awk '/ id / { print $4 }'` # Find port id for instance MYLASTINSTANCE_IP=`openstack server show MyLastInstance | awk '/ addresses / { print $4 }' | cut -d '=' -f 2` MYLASTINSTANCE_PORT_ID=`openstack port list --device-owner compute:None | awk ' /'$MYLASTINSTANCE_IP'/{print $2}'` # Create a floating IP and attach it to instance openstack floating ip create --port $MYLASTINSTANCE_PORT_ID public # Create a new security group openstack security group create remote # Add rules to security group to allow SSH and ping openstack security group rule create --proto icmp --src-ip 0.0.0.0/0 remote openstack security group rule create --proto tcp --dst-port 22 --src-ip 0.0.0.0/0 remote # Apply security group to instance with floating IP openstack server add security group MyLastInstance remote # Ping instance with floating IP ping 172.16.0.12 # Delete all your servers openstack server delete MyLastInstance  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-08-16-openshift/",
	"title": "Openshift",
	"tags": [],
	"description": "",
	"content": "  Architecture Containers and Image  Container Registries  Pods and Services  Pods Services Labels  Builds and Image Streams  Builds Image Stream Image stream tag Image stream image Image stream trigger Templates  References  Architecture OpenShift is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as acurately as possible, with a focus on easy composition of applications by a developer.\nOpenShift Container Platform (OCP) has a microservices based architecture of smaller, decoupled units that work together. It runs on top of Kubernetes cluster, with data about the objects stored in etcd, a realible key-value store.\n REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object.  Containers and Image The basic units of OpenShift Container Platform applications are called Containers. Linux container technologies are lightweight mechanism for isolating running processess so that they are limited to interacting with only their designated resources.\nContainers in OpenShift Container Platform are based on Docker formatted container Images. An image is a binary that includes all of the requirements for running a single container, as well as metadata describring its needs and capabilities.\nContainer Registries A container registry is a service for storing and retreving Docker formatted container images. A registry contains a collection of one or more image repositories. Each image repository contains one of more tagged images. Docker provides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a registry at registry.access.redhat.com for subscribers. OpenShift Container Platform can also supply its own internal registry for managing custom container images.\nPods and Services Pods OpenShift Enterprise leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.\nPods are the rough equivalent of a machine instance (physical or virtual) to a container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.\n# Get All Pods within a Namespace oc get pods # Get Pod information in YAML oc get pod hello-node-1-nz525 -o yaml # Get Pods Details using Label oc describe pods -l app=hello-node  Services A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.\nServices are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. A service uses a label selector to find all the containers running that provide a certain network service on a certain port.\nLabels Labels are used to organize, group, or select API objects. For example, pods are \u0026ldquo;tagged\u0026rdquo; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.\nMost objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.\nlabels: key1: value1 key2: value2  Builds and Image Streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process.\nOpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container registry.\nThe OpenShift build system provides extensible support for build strategies that are based on selectable types specified in the build API. There are three build strategies available:\n Docker build Source-to-Image (S2I) build Custom build  By default, Docker builds and S2I builds are supported.\nImage Stream An OpenShift Container Platform object that contains pointers to any number of Docker-formatted container images identified by tags. You can think of an image stream as equivalent to a Docker repository.\nImage stream tag A named pointer to an image in an image stream. An image stream tag is similar to a Docker image tag. See Image Stream Tag below.\nImage stream image An image that allows you to retrieve a specific Docker image from a particular image stream where it is tagged. An image stream image is an API resource object that pulls together some metadata about a particular image SHA identifier. See Image Stream Images below.\nImage stream trigger A trigger that causes a specific action when an image stream tag changes. For example, importing can cause the value of the tag to change, which causes a trigger to fire when there are Deployments, Builds, or other resources listening for those. See Image Stream Triggers below.\n# Get OpenShift from Project Openshift oc get is -n openshift oc describe is jenkins -n openshift # Get Image Stream Definition in YAML oc get is jenkins -o yaml  Templates A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift. A template can be processed to create anything you have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.\nYou can create a list of objects from a template using the CLI or, if a template has been uploaded to your project or the global template library, using the web console.\n# Get All Templates from a project oc get templates -n openshift # Create Template from YAML file oc create -f \u0026lt;filename\u0026gt;  References  Core Concepts - Builds and Image Streams\n CLI Reference - Basic CLI Operation\n Core Concepts - Architecture\n Core Concepts - Builds and Image Streams\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/",
	"title": "Orchestrating Cloud with Kubernetes",
	"tags": [],
	"description": "",
	"content": " # Creating Kubernetes Cluster gcloud container clusters create io  Quick Demo # Create Deployment kubectl create deployment nginx --image=nginx:1.10.0 # List Pods kubectl get pods # Expose Deployment via a Service using LoadBalancer kubectl expose deployment nginx --port 80 --type LoadBalancer # List Service kubectl get services  Pods Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods\n# Create Pod kubectl create -f pods/monolith.yaml # List Pods kubectl get pods # Describe Pod kubectl describe pods monolith # Port Forward Pod kubectl port-forward monolith 10080:80  Services An abstract way to expose an application running on a set of Pods as a network service.\nNo need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives pods their own IP addresses and a single DNS name for a set of pods, and can load-balance across them.\nPods aren\u0026rsquo;t meant to be persistent. They can be stopped or started for many reasons - like failed liveness or readiness checks - and this leads to a problem\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-26-setting-apache-virtual-host/",
	"title": "Setting Apache Virtual Host",
	"tags": [],
	"description": "",
	"content": " Install Apache WebServer (Pre-Req) sudo apt-get update sudo apt-get install apache2  Apache Virtual Host To run more than one site on a single machine, you need to setup virtual hosts for sites your plan to host on an apache server.\nName Based Virtual Hosts (Most Common)  The server relies on the client to report the hostname as part of the HTTP headers. Using this technique, many different hosts can share the same IP address.  IP Based Virtual Hosts  IP-based virtual hosting is a method to apply different directives based on the IP address and port a request is received on. Most commonly, this is used to serve different websites on different ports or interfaces.  Setting up Virtual Directories sudo mkdir -p /var/www/aayushtuladhar.com/public_html sudo chown -R $USER:$USER /var/www/aayushtuladhar.com/public_html  Creating Virtual Host Files sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/aayushtuladhar.com.conf \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin admin@aayushtuladhar.com ServerName aayushtuladhar.com ServerAlias aayushtuladhar.com *.aayushtuladhar.com DocumentRoot /var/www/aayshtuladhar.com/public_html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;/VirtualHost\u0026gt;   ServerName - Base Domain that should match for the virtual host definition. ServerAlias - Lists names are other names which people can use to see that same web site.  Enable Virtual Host sudo a2ensite example.com.conf\nDisable Default Virtual Host sudo a2dissite 000-default.conf\nRestart Apache sudo service apache2 restart # Ubuntu 15.10 sudo systemctl restart apache2 # Ubuntu 14.10 and Earlier  Reference  http://www.unixmen.com/setup-apache-virtual-hosts-on-ubuntu-15-10/ https://httpd.apache.org/docs/2.4/vhosts/name-based.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-03-01-setting-fqdn/",
	"title": "Setting FQDN",
	"tags": [],
	"description": "",
	"content": "  FQDN  Finding FQDN  Hostname  Finding Hostname  Setting hostname and FQDN  FQDN FQDN stands for Fully Qualified Domain Name. It is a domain name that specifies its exact location in the tree hierarhcy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone.\nExample, somehost.example.com\nFinding FQDN hostname -f  Hostname A hostname is a label that is assigned to a device connected to a computer network and that is used to identify the device. Example, kafka-dev\nFinding Hostname  $ hostname AayushTuladhar-Mac.local  Setting hostname and FQDN  Update /etc/hosts\n127.0.0.1 packer-20140710152503.aayushtuladhar.com packer-20140710152503 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 \u0026lt;ip-addr\u0026gt; kafka-dev.aayushtuladhar.com kafka-dev  Update /etc/sysconfig/network\n[vagrant@kafka-dev ~]$ cat /etc/sysconfig/network NETWORKING=yes HOSTNAME=kafka-dev.aayushtuladhar.com NOZEROCONF=yes  Restart the Hostname Service\n/etc/init.d/network restart  Verify Hostname and FQDN\nhostname hostname -f   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-02-05-setting-selenium-grid/",
	"title": "Setting Selenium Grid",
	"tags": [],
	"description": "",
	"content": " Selenium Grid enables you to spread your tests across multiple machines and multiple browsers, which allows your to run tests in parallel. Also having Hub as central point of communication handles all the driver configuration and runs them automatically.\nDownloading Selenium http://docs.seleniumhq.org/download/\nSetting up Hub Once you have the jarfile downloaded from the Selenium Website,\njava -jar selenium-server-standalone-2.x.x.jar –role hub\nThis starts up a jetty server on default port 4444. Grid Console can be viewed at http://localhost:4444/grid/console\nSetting up Nodes Nodes are the actual machine which performs the tests,\njava –jar selenium-server-standalone-2.x.x.jar –role node –hub http://hubIP:4444/grid/register\nIE, Chrome, Safari \u0026amp; firefox selenium NODE java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot; -browser \u0026quot;browserName=internet explorer, version=11, platform=WINDOWS, maxInstances=10\u0026quot; -browser \u0026quot;browserName=chrome,version=ANY,maxInstances=10,platform=WINDOWS\u0026quot;  IE Node Setup java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=internet explorer,version=11,platform=WINDOWS,maxInstances=10\u0026quot;  Chrome Node Setup java -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5556 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=chrome, version=ANY, maxInstances=10, platform=WINDOWS\u0026quot;  FireFox Node Setup java -jar selenium-server-standalone-2.48.2.jar -port 5557 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-17-setting-up-jekyll-website/",
	"title": "Setting up Jekyll Website",
	"tags": [],
	"description": "",
	"content": " Pre-Requires  Ruby  # Verify Ruby is Installed ruby --version  Installation gem install jekyll bundler  Create New Site jekyll new myblog  Run Blog Locally bundle exec jekyll serve  Reference  https://github.com/arttuladhar/my-jekyll-blog  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2018-09-18-spring-cloud-slueth/",
	"title": "Spring Cloud Slueth",
	"tags": [],
	"description": "",
	"content": " A powerful tool for enhancing logs in any application, but especially in a system built up of multiple services. This is where spring-cloud-starter-sleuth comes into play to help you enhance your logging and traceability across multiple systems. Just including the spring-cloud-starter-sluth in your project.\nFew concepts you need to be familiar with when using Spring Cloud Slueth are concepts of Trace and Spans. Trace can though as single request or job that is triggered in an application. All the various steps in the request, even across application and thread boundaries will have the same traceId. Whereas, Spans can be though of a section of a job request. A single trace can be composed of multiple spans each correlating to a specific step or section of the request.\n[application name, traceId, spanId, export]\n         Application Name Name of the Application, we set in the properties file   traceId Request Id to Single Request   spanId Track Unit of Work   export Indicates whether or not the log was exported to an aggregator like Zipkin    Resources Spring Cloud Sleuth - Baeldung\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-04-10-setting-up-supervisord/",
	"title": "Supervisor",
	"tags": [],
	"description": "",
	"content": "  Installation Configuration Running Supervisor  Supervisor Configuration Structure  Controller Processes  Reread Configuration and Reload It Controlling Tool Start / Stop Processess   It\u0026rsquo;s been a while I have been using Supervisor to run my application. It\u0026rsquo;s a great little tool for running and monitoring processes on UNIX-like operating systems. It provides you simple simple, centralized interface to all your applications running on a box. Using Web Interface, you can see health and logs of the applications without even logging in in the box.\nInstallation sudo apt-get install -y supervisor  Configuration /etc/supervisor/supervisor.conf\n[include] files = /etc/supervisor/conf.d/*.conf # Enable Web Interface [inet_http_server] port = 9001 username = user # Basic auth username password = pass # Basic auth password  Syntax\n[program:nodehook] - Define the program to monitor. We'll call it \u0026quot;nodehook\u0026quot;. command - This is the command to run that kicks off the monitored process. We use \u0026quot;node\u0026quot; and run the \u0026quot;http.js\u0026quot; file. If you needed to pass any command line arguments or other data, you could do so here. directory - Set a directory for Supervisord to \u0026quot;cd\u0026quot; into for before running the process, useful for cases where the process assumes a directory structure relative to the location of the executed script. autostart - Setting this \u0026quot;true\u0026quot; means the process will start when Supervisord starts (essentially on system boot). autorestart - If this is \u0026quot;true\u0026quot;, the program will be restarted if it exits unexpectedly. startretries - The number of retries to do before the process is considered \u0026quot;failed\u0026quot; stderr_logfile - The file to write any errors output. stdout_logfile - The file to write any regular output. user - The user the process is run as. environment - Environment variables to pass to the process.  Example Configuration\n/etc/supervisor/conf.d/node-chada-chutkila.conf\n[program:node-chadachutkila] command=node /home/art/apps/chada-chutkila/app.js directory=/home/art autostart=true autorestart=true startretries=3 stderr_logfile=/var/log/art/chada-chutkila.err.log stdout_logfile=/var/log/art/chada-chutkila.out.log user=art environment=SECRET_PASSPHRASE='this is secret',SECRET_TWO='another secret'  Running Supervisor sudo service supervisor start\nSupervisor Configuration Structure supervisor/ ├── conf.d │ └── digi-marketplace-node.conf └── supervisord.conf  Controller Processes Reread Configuration and Reload It supervisorctl reread supervisorctl update  Controlling Tool supervisorctl\nStart / Stop Processess supervisorctl start \u0026lt;processName\u0026gt; supervisorctl stop \u0026lt;processName\u0026gt;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/template-literals/",
	"title": "Teplate Literals",
	"tags": [],
	"description": "",
	"content": "Template literals are string literals allowing embedded expressions. You can use multi-line strings and string interpolation features with them.\nconst someText = `string text ${expression} string text`  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-01-23-nginx/",
	"title": "The Twelve Factor App",
	"tags": [],
	"description": "",
	"content": " 12factor.net - Link Introduction Methodology for building software-as-a-service app that:\n Use Declarative formats for setup automation, to minimize time and cost for new developers joining the project. Have a Clean Contract with the underlying operation system, offering Maximum Portability between execution environments Are suitable for Deployment on modern Cloud platforms, obviating the need for servers and system administrators Minimize divergence between deployment and production, enabling Continuous deployment for maximum agility And can Scale up without significant changes to tooling, architecture, or development practices.  The Twelve Factors 1. Codebase  One codebase tracked in revision control, many deploys\n  Multiple apps sharing the same code is a violation of twelve-factor. The solution here is to factor shared code into libraries which can be included through the dependency manager  2. Dependencies  Explicity declare and isolate dependencies\n  A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly via a dependency declaration manifest It uses dependency isolation tool during execution to ensure that no implicit dependency specification is applied uniformly to both production and development Another benefits of explicitly declaring dependency is that it allows new developers to quickly run application locally.  3. Config  Store Config in the environment\n  Apps storing config as constants in the code is violation of twelve-factor, which requires strict separation of config from code.  4. Backing Services  Treat backing services as attached resources\n  A backing service is any service that app consumes over the network as part of its normal operation. Eg, Datastore (MySQL, MongoDB), Messaging (RabbitMQ), SMTP Services and Caching System The code for twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config Each distinct backing service is a resource. The twelve-factor app treats these databases as attached resources, which indicate their loose coupling to the deploy they are attached to.   \n5. Build, Release, Run  Strictly separate build and run stages\n  The Build Stage is a tranform which converts a code repo into an executable bundle known as build. Using a version of the code at a commit specified by the development process, the build stage fetches vendors dependencies and compiles binaries and assets The Release Stage takes the build produced by the build stage and combines it with the deploy\u0026rsquo;s current config. The resulting relrease contains both the build and the config and is ready for immediate execution in the execution environment. The Run Stage runs the application in the execution environment, by launching some set of the app\u0026rsquo;s process against a selected release.   Every release should always have a unique release ID Release cannot be mutated once its created. Any change must create a new release  6. Processes  Execute the app as one or more stateless Processes\n  Twelve-factor process are stateless and share-nothing Sticy sessions are a violation of twelve-factor; Session state data is a good candidate for a datastore that offers time-expiration, such as Memcached  7. Port Binding  Export services via port Binding\n  The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing services. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port. Note also that the port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.  8. Concurrency  Scale out via the process model\n  In the twelve-factor app, processess are a first class citizen. The process model truly shines when it comes time to scale out. The share-nothing, horizontal partionable nature of twelve-factor app process means that adding more concurrency is a simple and reliable operation.  9. Disposability  Maximize robustness with fast startup and graceful shutdown\n  The twelve-factor app\u0026rsquo;s processess are disposable, meaning they can be started or stopped at a moment\u0026rsquo;s notice.  10. Dev / Prod Parity  Keep development, staging and production as similar as possible\n  The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. The twelve-factor developers resists the urge to use different backing services between development and production, even when adapters theoritically abstract away differences in backing services.  11. Logs  Treat logs as event streams\n  Twelve-factor app never concerns itself with routing or storage of its output stream It should not attempt to write to or manage logfiles, instead each running process writes its event stream, unbuffered, to stout  12. Admin Process  Run admin / management tasks as one-off processes\n  Twelve-factor strongly favors languages which provide a REPL shell out of the box, and which make it easy to run one-off scripts. In a local deploy, developers invoke one-off admin processes by a direct shell command inside the app’s checkout directory. In a production deploy, developers can use ssh or other remote command execution mechanism provided by that deploy’s execution environment to run such a process.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-07-07-ssh-keys/",
	"title": "Understanding SSH Keys",
	"tags": [],
	"description": "",
	"content": "  \nSSH is the most common way of connecting to remove Linux Server. It stands for Secure Shell. It provide a safe and secure way of executing commands, making changes and configuring service remotely. SSH connecting is implemented using client-server model. For a client machine to connect to a remote machine using SSH, SSH daemon must be running on the remote machine.\nClients generally authenticate either using passwords or by SSH Keys. SSH keys provides a more secure way of logging into a virtual private server using SSH. SSH keys are nearly impossible to decipher by brute force alone.\nSSH keys are a matching set of cryptographic keys. Each set contains a public and private key. To authenticate using SSH keys, you place the public key on any server and then unlock it by connecting with the private key.\nGenerating SSH Key Pair ssh-keygen -t rsa\nCopy Public Key ssh-copy-id user@123.45.67.89\nAlternatively,\ncat ~/.ssh/id_rsa.pub | ssh user@123.45.56.78 \u0026quot;mkdir -p ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot;\nNote - Generally when you spin up a server, you can embed your public key in your new server.\nGenerate Public Key from Private Key Option -y outputs the public key\nssh-keygen -y -f private-key \u0026gt; public-key\n# Example ssh-keygen -y -f pin-ost.pem \u0026gt; art.pub  Reference https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-03-15-using-apache-server-benchmarking/",
	"title": "Using Apache Server Benchmarking",
	"tags": [],
	"description": "",
	"content": " Apache Benchmark is a single-threaded command line tool for measuring the performance of a HTTP web server. It gives you an impression of how many requests per second your server is capable of serving.\nInstallation sudo apt-get install apache2-utils.  Usage -p POST Message -H Message Header -T Content Type -c Concurrent Clients -n Number of Requests to Run in the Test  GET REQUEST $ ab -n200 -c100 -H \u0026quot;APP-TOKEN: Q977quNeXjFsNjLNlmC9MK1HuRP+fFKmwDX9KSD6Y=\u0026quot; \\ http://test-api.aayushtuladhar.com:8080/test/start_page=0  POST REQUEST $ ab -p body.txt -n200 -c100 -T application/json \\ https://test-api.aayushtuladhar.com:8080/anotherTest  "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/",
	"title": "Using Hugo",
	"tags": [],
	"description": "",
	"content": " All the Hugo Basics "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
}]