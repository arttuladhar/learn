[
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/00_containers_and_orchestration/",
	"title": "00 - Containers and Orchestration",
	"tags": [],
	"description": "",
	"content": "  Containers Microservices Container Orchestration  Container Orchestrators   Containers Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications.\nMicroservices Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.\nContainer Orchestration Container orchestrators are tools which group systems together to form clusters where containers\u0026rsquo; deployment and management is automated at scale while meeting following requirements.\n Fault-tolerance On-demand scalability Optimal resource usage Auto-discovery to automatically discover and communicate with each other Accessibility from the outside world Seamless updates/rollbacks without any downtime.  Container Orchestrators  Amazon Elastic Container Service Azure Container Instances Azure Service Fabric Kubernetes Marathon Nomad Docker Swarm  "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/aws_certified_solutions_architect/01-aws-fundamentals/",
	"title": "01 - AWS Fundamentals",
	"tags": [],
	"description": "",
	"content": "  Architecture 101  Access Management Basics AWS Account  Authentication Authorization Billing  AWS Physical and Networking Layer Well-Architected Framework Elasticity Introduction to S3 - (Simple Storage Service)   Architecture 101 Access Management Basics AWS Account AWS accounts are more than just a way to log in and access AWS services — they are a crucial AWS feature that AWS solutions architects can use to implement secure and high-performance systems.\nAWS Account is used to perform following functionalities:\n Authentication Authorization Billing  Authentication AWS account are isolated. they are created initially with a single root user. This user, via it\u0026rsquo;s username / password / API Keys, is the only identity that can use the account. If account credentials are leaked, the impact is limited to that account.\nAuthorization Authorization is controlled on a per-account basis. The root user starts with full control of the account and its resources. Additional identities can be created or external identities can be granted access. Unless defined otherwise, no identity except the account root user has access to resources.\nBilling Every AWS account has it\u0026rsquo;s own isolation billing information. This is initially in the form of an attached credit card, but established acccounts can be converted to user traditional, term-based invoicing. By default, you are only billed for resoure in your account. Billing or security exploits are limited to a single account.\nAWS Physical and Networking Layer Regions contain multiple Availability Zones (AZs), which are separated and isolated networks. AZs in the same region are connected with reduandant high-speed, low-latency network connections.\nA failure in one AZ generally won\u0026rsquo;t impact another AZ.\n Most AWS services run within AZs. Some series operate from one AZ, while other replicate between AZs. Some services allow you to choose the AZ to use, and some don\u0026rsquo;t.\nEdge locations are small pockets of AWS compute, storage, and networking close to major populations and are generally used for edge computing and content delivery.\n Well-Architected Framework Well-Architected Framework is a set of Best Practices, Principles, Concepts to help you build effective AWS solutions. It introduces general design principles to help you build efficient systems.\nPillars of Well Architected Framework\n Security Operational Excellence Reliability Performance Efficiency Cost Optimization  AWS Well-Architected\nAWS-Well Architected Framework - PDF\nElasticity Traditional legacy system use vertial scalling. An attempt is made to forecast demand and purchase servers ideally before the demand passes current capacity. Purchase too early and capacity is wasted. Purchase too late and performance is impacted.\nWhen horizontal scalling is used (more smaller servers), capacity can be maintained closer to demand. There is less waste because servers are smaller and there\u0026rsquo;s less risk of performance impact as each increase is less expensive, so it generally require less approval.\nElasticity, or Elastic Scalling is where automation and horizontal scalling are used in conjunction to match capacity with demand. Demand is rarely so linear - it can increase or decrease, often in a rapid and sudden way. An efficient platform should scale OUT and IN, matching demand on that system.\nIntroduction to S3 - (Simple Storage Service) Simple Storage Service (S3) is a global object storage platfrom that can be used to store objects in the form of text files, photos, audio, movies, large binaries or other object types.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/hcl_basics/",
	"title": "01 - HCL Basics",
	"tags": [],
	"description": "",
	"content": "  Terraform CLI Commands Terraform Syntax Resources Console and Outputs Variables  Passing Variable  DataSources Terraform Workspaces NullResources and Local-exec  Terraform CLI Commands    Command Description     init Initializes a new or existing Terraform configuration   validate Validates the Terraform files   plan Generates ans shows an execution plan   apply Builds or Change Infrastructure   output Reads an output from a state file   show Inspects Terraform State or Plan   providers Print a tree of the providers using the configuration   destory Destorys Terraform-managed infrastructure    Terraform Syntax  Single line comments start with # Multi line comments are wrapped with /* and */ Values are assigned with the syntax key=value Strings are double quoted Strings can interpolate other values using the syntax ${}  Resources Resosurces are the Objects manged by Terraform such as VM or S3 Buckets. Declaring a Resource tells Terraform that it should CREATE and MANAGE the resource described. If the resource already exists it must be imported into Terraform\u0026rsquo;s State.\nSyntax\nresource \u0026lt;Resource_Type\u0026gt; \u0026lt;Resource_Name\u0026gt; { // Meta Parameters }  Example\n# Example # Download the latest Ghost image resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;ghost:latest\u0026quot; } # Start the Container resource \u0026quot;docker_container\u0026quot; \u0026quot;container_id\u0026quot; { name = \u0026quot;ghost_blog\u0026quot; image = \u0026quot;${docker_image.image_id.latest}\u0026quot; ports { internal = \u0026quot;2368\u0026quot; external = \u0026quot;80\u0026quot; } }  Console and Outputs Outputs are printed by the CLI after apply. These can reveal calculated values.\n#Output the IP Address of the Container output \u0026quot;ip_address\u0026quot; { value = \u0026quot;${docker_container.container_id.ip_address}\u0026quot; description = \u0026quot;The IP for the container.\u0026quot; } #Output the Name of the Container output \u0026quot;container_name\u0026quot; { value = \u0026quot;${docker_container.container_id.name}\u0026quot; description = \u0026quot;The name of the container.\u0026quot; }  Variables Input variables serve as parameters for a Terraform file. A variable block configures a single input variable for a Terraform module. Each block declares a single variable.\nSyntax\nvariable [NAME] { [OPTION] = \u0026quot;[VALUE]\u0026quot; }  Variables can be used using interpolation syntax \u0026quot;${var.name}\u0026quot;\nExample\nvariable \u0026quot;image_name\u0026quot; { description = \u0026quot;Image for container.\u0026quot; default = \u0026quot;ghost:latest\u0026quot; } resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;${var.image_name}\u0026quot; }  Passing Variable Variables can be specified on the command line with -var bucket_name=my-bucket or in file using terraform.tfvars\nterraform apply -var 'foo=bar' terraform apply -var 'container_name=ghost_blog' -var 'ext_port=8080'  We can set the variabl using Environment Variables\nexport TF_VAR_container_name=ghost_blog export TF_VAR_ext_port=8080  DataSources Data sources allow data to be fetched or computed for use elsewhere in Terraform configuration. Use of data sources allows a Terraform configuration to make use of information defined outside of Terraform, or defined by another separate Terraform configuration.\nEach provider may offer data sources alongside its set of resource types.\nTerraform Workspaces Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data such as Terraform State are stored. Terraform supports multiple workspace to isolate different terraform plans and their respective state files.\n# Creates Dev Workspace terraform workspace new dev # List Workspaces terraform workspace list # Switch to default Workspace terraform workspace select default  NullResources and Local-exec # Download the latest Ghost Image resource \u0026quot;docker_image\u0026quot; \u0026quot;image_id\u0026quot; { name = \u0026quot;${lookup(var.image_name, var.env)}\u0026quot; } # Start the Container resource \u0026quot;docker_container\u0026quot; \u0026quot;container_id\u0026quot; { name = \u0026quot;${lookup(var.container_name, var.env)}\u0026quot; image = \u0026quot;${docker_image.image_id.latest}\u0026quot; ports { internal = \u0026quot;${var.int_port}\u0026quot; external = \u0026quot;${lookup(var.ext_port, var.env)}\u0026quot; } } # Using Null Resource and Local Exec to Write Container Name and Container Address to TextFile resource \u0026quot;null_resource\u0026quot; \u0026quot;null_id\u0026quot; { provisioner \u0026quot;local-exec\u0026quot; { command = \u0026quot;echo ${docker_container.container_id.name}:${docker_container.container_id.ip_address} \u0026gt;\u0026gt; container.txt\u0026quot; } }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/01_kubernetes-architecture/",
	"title": "01 - Kubernetes Architecture",
	"tags": [],
	"description": "",
	"content": "  Master Node  Master Node Components Master Node Components: API Server Master Node Components: Scheduler Master Node Componets: Controller Managers Master Node Components: etcd  Worker Node  Worker Node Components Worker Node Component: Container Runtime Worker Node Components: kubelet Worker Node Components: kube-proxy Worker Node Components: Addons  Networking Challenges  Container to Container Communication inside Pods Pod-to-Pod Communication Across Nodes Pod-to-External World Communication   At a very high level, Kubernetes has the following main components\n One ore more master nodes One or more worker nodes Distributed key-value store, such as etcd  Master Node The master node provides a running environment for the control plane responsible for managing the state of a Kubernetes cluster, and it is the brain behind all operations inside the cluster. The control plane components are agents with very distinct roles in the cluster\u0026rsquo;s management. In order to communicate with the Kubernetes cluster, users send requests to the master node via a Command Line Interface (CLI) tool, a Web User-Interface (Web UI) Dashboard, or Application Programming Interface (API).\nMaster Node Components  API Server Scheduler Controller managers etcd  Master Node Components: API Server All the administrative tasks are coordinated by the kube-apiserver, a central control plane component running on the master node. The API server intercepts RESTful calls from users, operators and external agents, then validates and processes them. During processing the API server reads the Kubernetes cluster\u0026rsquo;s current state from the etcd, and after a call\u0026rsquo;s execution, the resulting state of the Kubernetes cluster is saved in the distributed key-value data store for persistence. The API server is the only master plane component to talk to the etcd data store, both to read and to save Kubernetes cluster state information from/to it - acting as a middle-man interface for any other control plane agent requiring to access the cluster\u0026rsquo;s data store.\nThe API server is highly configurable and customizable. It also supports the addition of custom API servers, when the primary API server becomes a proxy to all secondary custom API servers and routes all incoming RESTful calls to them based on custom defined rules.\nMaster Node Components: Scheduler The role of the kube-scheduler is to assign new objects, such as pods, to nodes. During the scheduling process, decisions are made based on current Kubernetes cluster state and new object\u0026rsquo;s requirements. The scheduler obtains from etcd, via the API server, resource usage data for each worker node in the cluster. The scheduler also receives from the API server the new object\u0026rsquo;s requirements which are part of its configuration data. Requirements may include constraints that users and operators set, such as scheduling work on a node labeled with disk==ssd key/value pair. The scheduler also takes into account Quality of Service (QoS) requirements, data locality, affinity, anti-affinity, taints, toleration, etc.\nMaster Node Componets: Controller Managers The controller managers are control plane components on the master node running controllers to regulate the state of the Kubernetes cluster. Controllers are watch-loops continuously running and comparing the cluster\u0026rsquo;s desired state (provided by objects\u0026rsquo; configuration data) with its current state (obtained from etcd data store via the API server). In case of a mismatch corrective action is taken in the cluster until its current state matches the desired state.\nMaster Node Components: etcd etcd is a distributed key-value data store used to persist a Kubernetes cluster\u0026rsquo;s state. New data is written to the data store only by appending to it, data is never replaced in the data store. Obsolete data is compacted periodically to minimize the size of the data store.\nOut of all the control plane components, only the API server is able to communicate with the etcd data store.\nWorker Node A worker node provides a running environment for client applications. Though containerized microservices, these applications are encapsulated in Pods, controlled by the cluster control plane agents running on the master node. Pods are scheduled on worker nodes, where they find required compute, memory and storage resources to run, and networking to talk to each other and the outside world. A Pod is the smallest scheduling unit in Kubernetes. It is a logical collection of one or more containers scheduled together.\nWorker Node Components A worker node has the following components:\n Container runtime kubelet kube-proxy Addons for DNS, Dashboard, cluster-level monitoring and logging.  Worker Node Component: Container Runtime Although Kubernetes is described as \u0026ldquo;container orchestration engine\u0026rdquo;, it does not have the capability to directly handle containers. In order to run and manage a container\u0026rsquo;s lifecycle, Kubernetes requires a container runtime on the node where a Pod and its containers are to be scheduled. Docker is the most widely used container runtime with Kubernetes.\nWorker Node Components: kubelet The kubelet is an agent running on each node and communicates with the control plane components from the master node. It receives Pod definitions, primarily from the API server, and interacts with the container runtime on the node to run containers associated with the Pod. It also monitors the health of the Pod\u0026rsquo;s running containers.\nThe kubelet connects to the container runtime using Container Runtime Interface (CRI). CRI consists of protocol buffers, gRPC API and libraries.\nWorker Node Components: kube-proxy The kube-proxy is the network agent which runs on each node responsible for dynamic updates and maintenance of all networking rules on the node. It abstracts the details of Pods networking and forwards connection requests to Pods.\nWorker Node Components: Addons Addons are cluster features and functionality not yet available in Kubernetes, therefore implemented through 3rd-party pods and services.\n DNS - cluster DNS is a DNS server required to assign DNS records to Kubernetes objects and resources Dashboard - a general purposed web-based user interface for cluster management Monitoring - collects cluster-level container metrics and saves them to a central data store Logging - collects cluster-level container logs and saves them to a central log store for analysis.  Networking Challenges Container to Container Communication inside Pods Making use of the underlying host operating system\u0026rsquo;s kernel features, a container runtime creates an isolated network space for each container it starts. On Linux, that isolated network space is referred to as a network namespace. A network namespace is shared across containers, or with the host operating system.\nWhen a Pod is started, a network namespace is created inside the Pod, and all containers running inside the Pod will share that network namespace so that they can talk to each other via localhost.\nPod-to-Pod Communication Across Nodes Kubernetes uses \u0026ldquo;IP-per-Pod\u0026rdquo; model to ensure Pod-to-Pod communication, just as VM are able to communicate with each other. Containers are integrated with the overall Kubernetes networking model through the use of the Container Network Interface (CNI)\nPod-to-External World Communication Kubernetes enables external accessibility through services, complex constructs which encapsulate networking rules definitions on cluster nodes. By exposing services to the external world with kube-proxy, applications become accessible from outside the cluster over a virtual IP.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/01_setting_up_k8_cluster/",
	"title": "01 - Setting up Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": " Using Ubuntu Distribution (Ubuntu Xenial LTS 16.04) as Base Image for the Virtual Machine. We will be building a Kubernetes Cluster\nSetup Docker and Kubernetes Repositories curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF  Install Docker, Kubelet, KubeAdm and KubeCtl sudo apt-get update sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu kubelet=1.17.0-00 kubeadm=1.17.0-00 kubectl=1.17.0-00 sudo apt-mark hold docker-ce kubelet kubeadm kubectl  Enable IPTable Bridge Call echo \u0026quot;net.bridge.bridge-nf-call-iptables=1\u0026quot; | sudo tee -a /etc/sysctl.conf sudo sysctl -p  Initailize Cluster on Kube Master # Initialize Kube Cluster sudo kubeadm init --pod-network-cidr=10.244.0.0/16 # Setup Local KubeConfig mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  Install Flannel Networking kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml  Join Master Node sudo kubeadm join 172.31.47.84:6443 --token c06upw.nhz4qqqifxmqdzaf --discovery-token-ca-cert-hash sha256:970ac46eb8847181b2ff5cf911836ef73c3ac103508739f3679e4933a1795775  You can use kubeadm token create --print-join-command to get the join command.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/02_core_concepts/",
	"title": "02 - Core Concepts",
	"tags": [],
	"description": "",
	"content": "  Kubernetes API Primitives Pods Namespaces Basic Container Configuration  Kubernetes API Primitives Kubernetes API Primitives are also called Kubernetes Objects. These are data objects that represent the state of the cluster. Example of Kubernetes Objects:\n Pod Node Service Service Account  The kubectl api-resource command will list the object types currently available to the cluster.\nEvery object has a spec and status:\n Spec - You provide the spec. This defines the desired state of the object Status - This is provided by the Kubernetes cluster and contains information about the current state of the object.  Kubernetes objects are often represented in the yaml format, so you can create an object by providing the cluster with yaml, defining the object and it\u0026rsquo;s spec.\nYou can get information about an object\u0026rsquo;s spec and status using the kubectl describe command:\nkubectl describe $object_type $object_name  Pods Pods are the basic building blocks of any application running in Kubernetes.\nA Pod consists of one or more containers and a set of resources shared by those containers. All containers managed by Kubenernetes cluster are part of a pod.\nA basic pod in yaml format:\napiVersion: v1 kind: Pod metadata: name: my-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello K8s! \u0026amp;\u0026amp; sleep 3600']  Save the pod\u0026rsquo;s yaml definition to a file my-pod.yml, then create the pod using\nkubectl create -f my-pod.yml  Similarly, you can change existing objects using:\nkubectl apply -f my-pod.yaml # Or kubectl edit pod my-pod  # Delete Pod kubectl delete pod my-pod  Namespaces Namespaces provide a way to keep your objects organized within the cluster. Every object belongs to a namespace. When no namespace is specified, the cluster will assume the default namespace.\nWhen creating an object, you can assign it to a namespace by specifying a namespace in the metadata:\nmypod-ckad-ns.yml\napiVersion: v1 kind: Pod metadata: name: my-pod namespace: my-ckad labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 60s']  When working with objects using kubectl use the -n flag to specify the namespace. For example,\nkubectl get pods -n my-ckad  Basic Container Configuration You can specify the command that will be used to run a container in the Pod spec. Thiw will override any built-in default command specified by the container image.\napiVersion: v1 kind: Pod metadata: name: my-command-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['echo'] args: ['Hello World'] restartPolicy: Never  Ports are another important part of container configuration. If you need a poart that the container is listening on to be exposed to the cluster, you can specify a containerPort\nmy-containerport-pod.yml\napiVersion: v1 kind: Pod metadata: name: my-containerport-pod labels: app: myapp spec: containers: - name: myapp-container image: nginx ports: - containerPort: 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/aws_certified_solutions_architect/02-identity_access_control/",
	"title": "02 - Identity and Access Control",
	"tags": [],
	"description": "",
	"content": "  Identity and Access Control  IAM Policy IAM Policy Notes IAM Users IAM Groups IAM Access Keys  Multi-Account Management and Organizations  AWS Organizations Role Switching Between Accounts   Identity and Access Control Identity and Access Management, known as IAM, is one of the key services within AWS. It controls access to the AWS API endpoints that are used by the console UI, command line tools, and any applications wanting to utilize AWS. Identity and Access Management (IAM) is the primary service that handles authentication and authorization within AWS environments.\nIAM controls access to AWS service via policies that can be attached to users, groups and roles. Users are given long-term credentials to access AWS resource (username and password or access keys).\nRoles allow for short-term access to resources when assumed, using temporary access credentials.\nIAM Policy IAM policies are JSON documents that either allow or deny access to combinations of actions and resources. An IAM policy (policy document) is known as an identity policy when attached to an identity or a resource policy when attached to a resource. They have no effect until they are attached to something.\nA policy document is a list of statements.\nEach statement matches a request to AWS. Requests are matched based on their Action(or actions), which are the API calls or operations being attempted and the Resource (or resources) the request is against. A given statement results in an Allow or Deny for the request.\nIAM Policy Notes  If a request isn\u0026rsquo;t explicitly allowed, it\u0026rsquo;s implicity (default) denied. If a request is explicitly denied, it overrides everything else. If a request is explicitly allowed, it\u0026rsquo;s allowed unless denied by an explicit deny. Remember: DENY -\u0026gt; ALLOW -\u0026gt; DENY Only attached policies have any impact When evaluating policies, all applicable policies are merged:  All identity (user, group, role) and any resource policies  Managed policies allow the same policy to impact many identities. Inline policies allow exceptions to be applied to identities. AWS-managed policies are low overhead but lack flexibility. Customer-managed policies are flexible but require administration Inline and managed policies can apply to users, groups and roles.  Use Managed Policies to control the base level permissions and for customization use in-line permissions as needed.\n IAM Users IAM users are a type of IAM identity suitable for long-term access for a known entity (human, service, application)\nPrincipals authenticate to IAM users either with a username and password or using access keys.\n Hard limit of 5,000  IAM Groups IAM groups allow for large-scale management of IAM users. This way, policies can be applied to groups and impact collections of similar users.\nGroup is not truly an identity in IAM because it cannot be identified as a Principal in a permission policy. It is simply a way to attach policies to multiple users at one time.\n IAM Access Keys Access keys consist of access key IDs and secret access keys. Access keys are the long-term credentials used to authenticate to AWS for anything but the console UI. This lesson walks through the architecture and discusses some key exam-relevant points.\nMulti-Account Management and Organizations AWS Organizations AWS Organizations is useful for businesses that need to manage multiple accounts. It provides the following features:\n Consolidated billing Service control policies (SCPs) Account creation Simplified role switching  Role Switching Between Accounts "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/02_installing_kubernetes/",
	"title": "02 - Installing Kubernetes",
	"tags": [],
	"description": "",
	"content": "  Local Installation On-Premise Installation Cloud Installation  All-in-One Single-Node Installation In this setup, all the master and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, and it should not be used in production. Minikube is one such example, and we are going to explore it in future chapters.\nSingle-Node etcd, Single-Master and Multi-Worker Installation In this setup, we have a single-master node, which also runs a single-node etcd instance. Multiple worker nodes are connected to the master node.\nSingle-Node etcd, Multi-Master and Multi-Worker Installation In this setup, we have multiple-master nodes configured in HA mode, but we have a single-node etcd instance. Multiple worker nodes are connected to the master nodes.\nMulti-Node etcd, Multi-Master and Multi-Worker Installation In this mode, etcd is configured in clustered HA mode, the master nodes are all configured in HA mode, connecting to multiple worker nodes. This is the most advanced and recommended production setup.\nLocal Installation  Minikube - single-node local Kubernetes cluster Docker Desktop - single-node local Kubernetes cluster for Windows and Mac CDK on LXD - multi-node local cluster with LXD containers.  On-Premise Installation  On-Premise VMs Kubernetes can be installed on VMs created via Vagrant, VMware vSphere, KVM, or another Configuration Management (CM) tool in conjunction with a hypervisor software. There are different tools available to automate the installation, such as Ansible or kubeadm.\n On-Premise Bare Metal Kubernetes can be installed on on-premise bare metal, on top of different operating systems, like RHEL, CoreOS, CentOS, Fedora, Ubuntu, etc. Most of the tools used to install Kubernetes on VMs can be used with bare metal installations as well.\n  Cloud Installation  Hosted Solutions  Google Kubernetes Engine (GKE) Azure Kubernetes Service (AKS) Amazon Elastic Container Service for Kubernetes (EKS) DigitalOcean Kubernetes OpenShift Dedicated   Kubernetes The Hard Way\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/modules/",
	"title": "02 - Terraform Modules",
	"tags": [],
	"description": "",
	"content": " Module is a container for multiple resources that are going to be used together.\nMain goal of module is logical grouping of resources to it\u0026rsquo;s cohesive unit that can be reused and shared across different systems. Modules can also be shared across multiple teams or via public registry such as GitHub or Terraform Cloud registry.\nUsing Terraform Modules # Download the image module \u0026quot;image\u0026quot; { source = \u0026quot;./image\u0026quot; image_name = \u0026quot;${var.image_name}\u0026quot; } # Start the container module \u0026quot;container\u0026quot; { source = \u0026quot;./container\u0026quot; image = \u0026quot;${module.image.image_out}\u0026quot; container_name = \u0026quot;${var.container_name}\u0026quot; int_port = \u0026quot;${var.int_port}\u0026quot; ext_port = \u0026quot;${var.ext_port}\u0026quot; }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/aws_certified_solutions_architect/03-compute/",
	"title": "03 - Compute",
	"tags": [],
	"description": "",
	"content": "  Elastic Cloud Compute (EC2) Elastic Block Storage (EBS)  EBS Types  EBS Snapshots Security Groups Instance Metadata AMI Bootstrap Private Instance and Public Instance EC2 Instance Roles EBS Volume and Snapshot Encryption EBS Optimized, Enhanced Networking, and Placement Group  EBS optimization Enhanced networking Cluster, partition, and spread placement groups  EC2 Billing Models Dedicated Hosts  Elastic Cloud Compute (EC2) EC2 is one of the most widely used services within AWS. As an Infrastructure as a Service (IaaS) product, it\u0026rsquo;s responsible for providing long-running compute as a service.\nEC2 instances are grouped into families, which are designed for a specific broad type workload. The type determines a certain set of features, and size decide the level of workload they can cope with.\nThe current EC2 families are:\n Genral Purpose Compute Optimized Memory Optimized Storage Optimized Accelerated Computing  Instance Types include:\n T2 and T3: Low cost instance types that provide burst capability M5: for general workoads C4: Provides more capable CPU X1 and R4: Optimize large amounts of Fast Memory I3: Delivers fast IO P2, G3 and F1: Deliver GPU and FPGA  Instance sizes include Nano, Small, Medium, Large, X.Large, 2X.Large and Larger\nSpecial Cases\n \u0026ldquo;a\u0026rdquo;: Use AMD CPU \u0026ldquo;A\u0026rdquo;: Arm based \u0026ldquo;n\u0026rdquo;: Higher speed networking \u0026ldquo;d\u0026rdquo;: NVMe storage  Elastic Block Storage (EBS) Elastic Block Storage is a storage service that creates and managed volumes based on four underlying storage types. Volumes are persistent, can be attached and removed from EC2 instances, and are replicated within a single AZ.\nTo protect against AZ failure, EBS snapshots (to S3) can be used. Data is replicated across AZs in the region and (optionally) internationally.\n  EBS supports a maximum per-instance throughput of 1750 MiB/s and 80,000 IOPS.  EBS Types General Purpose (gp2): (SSD)\n Default for most workloads 3 IOPS/GiB (100 IOPS - 16,000 IOPS) Burst up to 3,000 IOPS (credit based) 1 GiB - 16 TiB size, max throughput p/vol of 250 MiB/s  EBS Snapshots EBS Snapshots are a point-in-time backup of an EBS volume stored in S3. The initial snapshot is a full copy of the volume. Future snapshots only store the data changed since the last snapshot.\nSnapshots can be used to create new volumes and are a great way to move or copy instances between AZs. When creating a snapshot of the root/boot volume of an instance of busy volume, it\u0026rsquo;s recommended the instance is powered off, or disks are \u0026ldquo;flushed\u0026rdquo;\nSnapshots can be copied between regions, shared and automated using Data Lifecycle Manager (DLM).\nSecurity Groups Security Groups are software firewalls that can be attached to network interface and (by association) products in AWS. Security groups each have inbound rules and outbound rules. A rule allows traffic to and from a source (IP, network, named AWS entity) and protocol. Security Group belongs to a VPC\nEach Elastic Network Interface (ENI) can have upto 5 security groups.\n Security group have a hidden implicit/default deny rule but cannot explicitly deny traffic.\nThey are stateful - meaning for any traffic allowed in/out, the return traffic is automatically allowed. Security groups can reference AWS resource, other security groups, and even themselves.\nInstance Metadata Instance metadata can be used to access information about an instance from the instance. It allows applications running within EC2 to have visibility into their environment. Instance metadata is data relating to the instance that can be accessed from within the instance itself using a utility capable of accessing HTTP and using the URL\nhttp://169.254.169.254/latest/meta-data  Instance metadata is a way that scripts and application running on EC2 can get visibility of data they would normally need API calls for.\nThe metadata can provide the current external IPv4 address for the instance, which isn\u0026rsquo;t configured on the instance itself but provided by the internet gateway in the VPC. It provides the Availability Zone the instance was lanched in and the security group applied to the instance. IN the case of spot instances, it also provides the approximate time the instance will terminate.\nRemember the IP address to access metadata\n AMI AMIs (Amazon Machine Images) are used to build instance. They store snapshots of EBS volumes, permissions, and a block device mapping, which configures how the instance OS see the attached volumes. AMIs can be shared, free or paid and can be copied to other AWS regions.\n Configure Instance - Source instance and attached EBS volumes are configured with any required software and configuration.\n Create Image - Snapshots are created from volumes. AMI references snapshots, permission, and block device mapping.\n Launch Instance - With approriate launch permissions, instances can be created from an AMI. EBS volumes are created using snapshots as the source, and an EC2 instance is created uinsg the block device mapping to reference its new volumes.\n  Downside of using AMI is you can\u0026rsquo;t do dynamic configuration\n Bootstrap Bootstrapping is a process where instructions are executed on an instance during its launch process. Bootstraping is used to configure the instance, perform software installation, and add application configuraiton.\nIn EC2, user data can be used to run shell scripts or run cloud-init directives.\nPrivate Instance and Public Instance  Private Instance - Private IP allocated when launching instances. Unchanged during stop/start. Released when terminated. Public Instance - Same private address as private instance. A public IPv4 address is allocated when the machine starts and deallocated when it stops. By default all public IPv4 addresses are dynamic Elastic IP - Elastic IPs are static. When allocated, they replace the normal public IP, which is deallocated.  EC2 Instance Roles EC2 instance roles are IAM roles that can be \u0026ldquo;assumed\u0026rdquo; by EC2 using an itermediary called an instance profile. An instance profile is either created automatically when using the console UI or manually when using the CLI. It\u0026rsquo;s a container for the role that is associated with an EC2 instance.\nThe instance profile allows application on the EC2 instance to access the credentials fromt he role using the instance metadata.\nEBS Volume and Snapshot Encryption Volume encryption uses EC2 host hardware to encrypt data at rest and in transit between EBS and EC2 instances. Encryption generates a data encryption key (DEK) from a customer master key (CMK) in each region. A unique DEK encrypts each volume. Snapshots of that volume are encrypted with the same DEK, as are any volumes created from that snapshot.\nEncrypted DEKs stored with volume are decrypted by KMS using a CMK and given to the EC2 host.\nPlaintext DEKs stored in EC2 memory and used to encrypt and decrypt data. The EC2 instance and OS see plaintext data as normal - no performance impact.\nEBS Optimized, Enhanced Networking, and Placement Group EBS optimization EBS-optimized mode, which was historically optional is now the default, adds optimizations and dedicated communcation paths for storage and traditional data networking. This allows consistent utilization of both - and is one required feature to support higher performance storage.\nEnhanced networking Traditionally, virtual networking meant a virtual host (EC2 host) arranging access for n virtual machines to access one physical network card - this multitasking is done in software and is typically slow.\nEnhanced networking uses SR-IOV, which allows a single physical network card to appear as multiple physical devices. Each instance can be given one of these (fake) physical devices. This results in faster transfer rate, lower CPU usage, and lower consistent latency. EC2 delivers this via the Elastic Network Adapter (ENA) or Intel 82599 Virtual Function (VF) interface.\nCluster, partition, and spread placement groups  Cluster Placement Group Cluster placement groups place instances physically near each other in a single AZ. Every instance can talk to each other instance at the same time at full speed. Works with enhanced networking for peak performance.  Use Cluster Placement Group for maximum performance\n Partition Placement Group  Use Partition Placement Group if you have large infrastructe platform and provides visibility on placement.\n Spread Placement Group  Use Spread Placement Group for maximum availability.\nEC2 Billing Models On-Demand Billing (Default)\nSpot Instances\nSpot instance allow consumption of spare AWS capacity for a given instance type and size in a specific AZ. Instances are provided for as long as your bid price is above the spot price, and you ony ever pay the spot price. If your bid is exceeded, instances are terminated with a two-minute warning.\nSpot fleets are a container for \u0026ldquo;capacity needs\u0026rdquo;. You can specify pools of instances of certain types/sizes aiming for a given \u0026ldquo;capacity\u0026rdquo;. A minimum percentage of on-demand can be set to ensure the fleet is always active.\nSpot instances are perfect for non-critical workloads, burst workloads or consistent non-critical jobs that can tolerate interruptions without impacting functionality.\nSpot is not suitable for long-running workloads that require stability and cannot tolerate interruptions.\nSpot Pricing can save you more than 90% than on-demand application.\n  Reserved Instances  Reserved instance lock in a reduced rate for one or three years. Zonal reserved instance include a capacity reservation. Your commitment incurs costs even if instance aren\u0026rsquo;t launched. Reserved purchases are used for long-running, understood, and consistent workloads.\nWhen to Use Reserved Purchases*\n Base / Consistent Load Known and Understood Growth Critical Systems / Components  When to Use Spot Instances / Fleets\n Burst-y workloads Cost-critial, when can cope with interruptions  When to Use On-Demand\n Default or unknown demand Anything in between reserved/spot Short-term workloads that cannot tolerate interruptions  Dedicated Hosts EC2 dedicated hosts are a feature of EC2, giving you complete control over physical instance placement and dedicated hardware free from other customer interaction.\nServerless Compute Lambda Essentials Lambda is a FaaS product. Function are code, which run in a runtime. Functions are invoked by events, perform actions for upto 15 minutes and terminate. Functions are also stateless - each run is clean.\nAPI Gateway API Gateway is a managed API endpoint service. It can be used to create, publish, monitor and secure APIs \u0026ldquo;As a Service\u0026rdquo;. API Gateway can use other AWS services for compute (FaaS/IaaS) as well as to store and recall data.\nStep Functions Step Functions is a serverless visual workflow servcie that provides state machines. A state machine can orchestrate other AWS services with simple logic, branching, and parallel execution, and it maintains a state. Workflow steps are known as states, and they can perform work via tasks. Step Functions allows for long-running serverless workflows. A state machine can be defined by using Amazon State Language (ASL).\nContainer Based Compute ECS ECS is a managed container engine. It allows Docker containers to be deployed and managed within AWS environments. ECS can use infrastructure clsutes based on EC2 or Fargate where AWS manages the backing infrastructure.\nWith EC2 launch type utilizes your own EC2 instances. AWS Fargate is a managed service, so tasks are auto placed.\nCluster - A logical collection of ECS resources - either ECS EC2 instances or a logical representation of managed Fargate infrastructure\nTask Definition - Defines your application. Similar to Dockerfile but for running containers in ECS. Task definition can contain multiple containers.\nContainer Definition - Inside a task definition, a container definition defines the individual containers a task uses. It controls the CPU and memory each container has, in addition to port mappings for the container\nTask - A single running copy of any containers defined by a task definition. One working copy of an application\nServices - Allows task definitions to be scaled by adding additional tasks. Define minimum and maximum values.\nRegistry - Storage for container images. (eg. ECS Container Registry or Dockerhub). Used to download image to create containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/03_configuration/",
	"title": "03 - Configuration",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps SecurityContexts Resource Requirements Secrets Service Accounts   ConfigMaps A ConfigMap is a Kubernetes Object that stores configuration data in a key-value format. This configuration data can then be used to configure software running in a container, by referencing the ConfigMap in the Pod spec.\nmyconfigmap.yml\napiVersion: v1 kind: ConfigMap metadata: name: my-config-map data: myKey: myValue anotherKey: anotherValue  Passing ConfigMap data to a pod\u0026rsquo;s container as an environment variable:\nmyconfigmap-pod.yml\napiVersion: v1 kind: Pod metadata: name: my-configmap-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo $(MY_VAR) \u0026amp;\u0026amp; SLEEP 3600\u0026quot;] env: - name: MY_VAR valueFrom: configMapKeyRef: name: my-config-map key: myKey  It\u0026rsquo;s also possible to pass ConfigMap data to container, in the form of file using a mount volume.\napiVersion: v1 kind: Pod metadata: name: my-configmap-volume-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo $(cat /etc/config/myKey) \u0026amp;\u0026amp; sleep 3600\u0026quot;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: my-config-map  SecurityContexts A Pod\u0026rsquo;s Security Context defines privilege and access control settings for a pod. If a container needs special operating system-level permissions, we can provide them using the securityContext.\nThe securityContext is defined as part of a Pod\u0026rsquo;s spec.\nThis spec will cause the container to run as the OS user with an ID of 1000, and the containers will run as the group with and ID of 2000.\napiVersion: v1 kind: Pod metadat: name: my-securitycontext-pod spec: securityContext: runAsUser: 2000 fsGroup: 3000 containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;cat /message/message.txt \u0026amp;\u0026amp; sleep 3600s\u0026quot;]  Resource Requirements Kubernetes allows us to specify the resource requirements of a container in the pod spec. A container\u0026rsquo;s memory and CPU requirements are defined in terms of resource requests and limits.\nResoruce request: The amount of resource necessary to run a container. A pod will only be a run on a node that has enough available resources to run the pod\u0026rsquo;s containeers.\nResource limit: A maximum value for the resource usage of a container\nResource requests and limits are defined as part of a pod\u0026rsquo;s spec.\napiVersion: v1 kind: Pod metadata: name: my-resource-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot;  Memory is measured in bytes. 128Mi means 128 Mebibytes CPU is measured in \u0026ldquo;cores\u0026rdquo;. 500m means 500 milliCPU or 0.5 CPU cores\nSecrets Secrets are pieces of sensitive information store in the Kubernetes cluster, such as passwords, tokens and keys. If a container needs a sensitive piece of information, such as password, it is more secure to store it as a secret than storing it in a pod spec or in the container itself.\nmy-secret.yml\napiVersion: v1 kind: Secret metadata: name: my-secret stringData: myKey: myPassword  apiVersion: v1 kind: Pod metadata: name: my-secret-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! $(MY_PASSWORD) \u0026amp;\u0026amp; sleep 3600\u0026quot;] env: - name: MY_PASSWORD valueFrom: secretKeyRef: name: my-secret key: myKey  Service Accounts ServiceAccounts allow containers running in pods to access the Kubernetes API. Some applications may need to interact with the cluster itself, and service accounts provide a way to let them to do it securely, with properly limited permissions.\nYou can determine the ServiceAccount that a pod will use by specifying a serviceAccountName in the pod spec:\n# Createing Service Account kubectl create serviceaccount my-serviceaccount  apiVersion: v1 kind: Pod metadata: name: my-serviceaccount-pod spec: serviceAccountName: my-serviceaccount containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/03_minikube/",
	"title": "03 - Minikube",
	"tags": [],
	"description": "",
	"content": "  Installing Minikube  Command Line Interface (CLI) tools and scripts Web-based User Interface (Web UI) from a web browser APIs from CLI or programmatically Kubectl Proxy   Installing Minikube # Install Minikube brew install minikube # Starting Minikube minikube start minikube start --vm-driver=xhyve minikube start --vm-driver=hyperkit minikube status minikube stop  Any healthy running Kubernetes cluster can be accessed via any one of the following methods:\nCommand Line Interface (CLI) tools and scripts kubectl is the Kubernetes Command Line Interface (CLI) client to manage cluster resources and applications. It can be used standalone, or part of scripts and automation tools. Once all required credentials and cluster access points have been configured for kubectl it can be used remotely from anywhere to access a cluster.\nWeb-based User Interface (Web UI) from a web browser APIs from CLI or programmatically # Open Minikube Dashboard minikube dashboard # Serving on different Port kubectl proxy #Dashboard URL http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy/#!/overview?namespace=default  Kubectl Proxy When not using the kubectl proxy, we need to authenticate to the API server when sending API requests. We can authenticate by providing a Bearer Token when issuing a curl, or by providing a set of keys and certificates.\nA Bearer Token is an access token which is generated by the authentication server (the API server on the master node) and given back to the client. Using that token, the client can connect back to the Kubernetes API server without providing further authentication details, and then, access resources.\nGetting Token\nTOKEN=$(kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\\t' | tr -d \u0026quot; \u0026quot;)  Getting API Server Endpoint\nAPISERVER=$(kubectl config view | grep https | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;)  Confirm that the APISERVER stored the same IP as the Kubernetes master IP by issuing the following 2 commands and comparing their outputs:\n$ echo $APISERVER https://192.168.99.100:8443 $ kubectl cluster-info Kubernetes master is running at https://192.168.99.100:8443 ...  Access the API server using the curl command, as shown below:\ncurl $APISERVER --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure  By using the kubectl proxy we are bypassing the authentication for each and every request to the Kubernetes API.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/04_kubernetes_building_blocks/",
	"title": "04 - Kubernetes Building Blocks",
	"tags": [],
	"description": "",
	"content": "  Kubernetes Object Model Pods  Labels  Replication Controller Replica Set Deployments Namespaces  Kubernetes Object Model With each object, we declare our intent or the desired state under the spec section. When creating an object, the object\u0026rsquo;s configuration data section from below the spec field has to be submitted to the Kubernetes API server.\nExample of Deployment object configuration in YAML format.\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  Pods A pod is the basic unit that Kubernetes deals with. Containers themselves are not assigned to hosts. Instead, closely related containers are grouped together in a pod. A pod generally represents one or more containers that should be controlled as a single “application”.\nA Pod is the smallest and simplest Kubernetes object. It is the unit of deployment in Kubernetes, which represents a single instance of the application. A Pod is a logical collection of one or more containers, which:\n Are scheduled together on the same host with the Pod Share the same network namespace Have access to mount the same external storage (volumes).  Below is an example of a Pod object\u0026rsquo;s configuration in YAML format:\napiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: containers: - name: nginx image: nginx:1.15.11 ports: - containerPort: 80  # List all the Pods kubectl get pods # Displays details of Pod kubectl describe pod webserver-74d8bd488f-dwbzz  Labels Labels are key-value pairs attached to Kubernetes objects (e.g. Pods, ReplicaSets). Labels are used to organize and select a subset of objects, based on the requirements in place. Many objects can have the same Label(s). Labels do not provide uniqueness to objects. Controllers use Labels to logically group together decoupled objects, rather than using objects\u0026rsquo; names or IDs.\n# Lists Pods with labels kubectl get pods -L k8s-app,label2 # Select the Pods with a given Label kubectl get pods -l k8s-app=webserver  Replication Controller Although no longer a recommended method, a ReplicationController is a controller that ensures a specified number of replicas of a Pod is running at any given time. If there are more Pods than the desired count, a replication controller would terminate the extra Pods, and, if there are fewer Pods, then the replication controller would create more Pods to match the desired count. Generally, we don\u0026rsquo;t deploy a Pod independently, as it would not be able to re-start itself if terminated in error. The recommended method is to use some type of replication controllers to create and manage Pods.\nThe default controller is a Deployment which configures a ReplicaSet to manage Pods\u0026rsquo; lifecycle.\nReplica Set A ReplicaSet is the next-generation ReplicationController. ReplicaSets support both equality- and set-based selectors, whereas ReplicationControllers only support equality-based Selectors. Currently, this is the only difference.\nWith the help of the ReplicaSet, we can scale the number of Pods running a specific container application image. Scaling can be accomplished manually or through the use of an autoscaler.\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\n # List Replica Sets kubectl get replicasets  Deployments Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node\u0026rsquo;s controller manager, and it ensures that the current state always matches the desired state. It allows for seamless application updates and downgrades through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling.\n# Lists all the Deployments in a given namespace kubectl get deployments # Deleting Deployment (Along with ReplicaSet and Pods) kubectl delete deployments webserver  Deleting a Deployment also deletes the ReplicaSet and the Pods it created.\n Namespaces If multiple users and teams use the same Kubernetes cluster we can partition the cluster into virtual sub-clusters using Namespaces. The names of the resources/objects created inside a Namespace are unique, but not across Namespaces in the cluster.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/04_multi_container_pods/",
	"title": "04 - Multi Container Pods",
	"tags": [],
	"description": "",
	"content": " Multi-container pods are simply pods with more than one container that all work together as a single unit.\nIt is often a good idea to keep containers separate by keeping them in their own seperate pods, but there are several cases where multi-continer pods can be beneficial.\nYou can create multi-continer pods by listing multiple containers in the pod definition.\napiVersion: v1 kind: Pod metadata: name: multi-container-pod spec: containers: - name: nginx image: nginx:1.15.8 ports: - containerPort: 80 - name: busybox-sidecar image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;while true; do echo \u0026quot;Hello From SideCar\u0026quot;; sleep 3600s; done;\u0026quot;]  How can containers interact with one another in a pod ?\n Shared Network - All listening ports are accessible to other containers in the pod, even if they are not exposed outside the pod. Shared Storage Volumes - Containers can intereact with each other by reading and modifying files in a shared storage volume that is mounted with both containers. Shared Process Namespace - With process namespace sharing enabled, containers in the same pod can interact with and signal one another\u0026rsquo;s process.  Design Patterns for Multi-Container Pods  Sidecar Pod - The sidecar pattern uses a sidecar container that enhances or adds functionality of the main container in some way. This could be something like, Eg, a sidecar that syncs files from a Git repository to the file system in a web server container. Ambassador Pod - The ambassador pattern uses an ambassador container to accept network traffic and pass it on to the main controller. One example might be an ambassador that listens on a custom port, and forwards traffic to the main container on its hard-coded port. Adapter Pod - The adapter pattern uses an adapter container to change the output of the main container in some way. An example could be an adapter that formats and decorates log output from the main container.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/aws_certified_solutions_architect/04-networking/",
	"title": "04 - Networking",
	"tags": [],
	"description": "",
	"content": "  Networking Fundamentals  IP Addressing Subnetting Firewall Proxy Servers  Virtual Private Cloud (VPC)  Virtual Private Cloud (VPC): VPC Peering VPC Endpoints  DNS 101  Networking Fundamentals The Open Syste Interconnection (OSI) Model is a standard used by networking manufacturers globally. It was created and published in 1984; it splits all network communications into seven layers. Each layer servers the layer that\u0026rsquo;s above it plus the layer beneath it which adds additional capabilities. Data between two devices travels down the stack on the device\u0026rsquo;s A-side (wrapped at each layer) and gets transmitted before moving up the stack at the device B-side (where the wrapping gets stripped aways at every stage). The data wrapping process is called encapsulation.\nAt Layer 1 (Physical), networks use a shared medium where devices can transmit signals and listen. Layer 1 showcases how data gets received and transmitted while taking into consideration the medium, voltages, and RF details.\nLayer 2 (Data Link) adds MAC addresses that can be used for named communication between two devices on a local network. Aditionally, layer 2 adds control over the media, avoiding cross-talk, this allows a back-off time and retransmission. L2 communications use L1 to broadcast and listen. L2 runs on top of L1.\nThe Network Layer (L3) allows for unique device-to-device communcation over interconnected networks. L3 devices can pass packets over tens or even hundreds of L2 networks. The packets remain largely unchanged during this journey - travelling within different L2 frames as they pass over various networks.\nL4 (Transport) adds TCP and UDP. TCP is designated for reliable transport, and UDP is aimed at speed. TCP uses segments to ensure data is received in the consistent order and adds error checking and \u0026ldquo;ports\u0026rdquo; allowing different stream of communications to the same host.\nL5 (Session) adds the concept of sessions, so that request and reply communication streams are viewed as a single \u0026ldquo;session\u0026rdquo; of communication between client and server.\nL6 (Presentation) adds data converstion, encryption, compression,and standard that L7 can use. L7(Application) is where protocols (such as HTTP, SSH, and FTP) are added.\nIP Addressing IPv4 addresses are how two devices can communicate at layer 4 and above of the OSI seven-layer model. IP address (IPs) are actually 32-bit binary values but are represented in dotted-decimal notation to make them easier for humans to read and understand.\nIPs are split into a network part and host part. The netmask (eg. 255.255.255.0) or prefix (e.g. /24) shows where this split occurs.\n   IP 10 0 0 0     Binary 10000000 00000000 00000000 00000000   Subnet Mask 255 255 255 0   Prefix /24 11111111 11111111 11111111 X    Subnetting Subnetting is a process of breaking a network down into smaller subnetworks. You might be allocated a public range for your business or decide on a private range for a VPC. Subnetting allows you to break it into smaller allocations for use in smaller network.\nFirewall A firewall is a device that historically sits at the border between different networks and monitors traffic flow between them. A firewall is capable of reading packet data and either allowing or denying traffic based on that data.\nFirewall establish a barrier between networks of different security levels and historically have been the first line of defense against perimeter attacks.\nProxy Servers A proxy server acts as a gateway between you and the internet. It\u0026rsquo;s an intermediary server separating end users from the websites they browse. Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy.\nA proxy server is a type of gateway that sits between a private and public network. Proxy servers can also choose to pass on traffic or not based on network layer appliances eg, username or element of corporate identity.\nVirtual Private Cloud (VPC) Virtual Private Cloud (VPC):  A private network within AWS. It\u0026rsquo;s your private data center inside the AWS platform. Can be configured to be public/private or a mixture Regional (can\u0026rsquo;t span regions), highly available, and can be connected to your data center and corporate networks Isolated from other VPCs by default VPC and subnet: Max /16 (65,536 IPs) and minimum /28 (16 IPs) VPC subnet can\u0026rsquo;t span AZs (1:1 Mapping) Certain IPs are reserved in subnets  Regional Default VPC:\n Required for some services, used as a default for most Pre-configured with all required network/security Configured using /16 CIDR Block (172.31.0.0/16) A /20 Public subnet in each AZ, allocating a public P by default Attached internet gateway with a \u0026ldquo;main\u0026rdquo; route table sending all IPv4 traffic to the internet gateway using a 0.0.0.0/0 route A default DHCP option set attached SG: Default - all from itself, all outbound NACL: Default - allow all inbound and outbound  Custom VPC:\n Can be designed and configured in any valid way You need to allocate IP ranges, create subnets, and provision gateways and networking, as well as design and implement security When you need multiple tiers of a more complex set of networking Best practise is to not use default for most production things  VPC Routing\nRoutes\nVPC Peering  Allows direct communication between VPCs Services can communicate using private IPs from VPC to VPC VPC peers can span AWS accounts and even regions (with some limitations) Data is encrypted and transits via the AWS global backbone. VPC peers are used to link two VPCs at layer 3: company mergers, shared services, company and vendor, auditing.  Important Limits and Considerations:\n VPC CIDR blocks cannot overlap VPC peers connect two VPCs - routing is not transitive Routes are required at both sites (remote CIDR -\u0026gt; peer connection) NACLs and SGs can be used to control access SGs can be referenced but not cross-region IPv6 support is not available cross-region DNS reoslution to private IPs can be enabled, but it\u0026rsquo;s a setting needed at both sides.  Steps to Enable VPC Peering\n Create VPC Peering Object Add Route FROM VPC Subnet and TO VPC Peering Object. Update Security Group to Unblock the Inbound and Outbound Rules Ensure there aren\u0026rsquo;t any connection Blocking Rules in Network ACLs  VPCs doesn\u0026rsquo;t support Transitive Routing\n VPC Endpoints VPC Endpoints are gateway objects created within a VPC. They can be used to connect to AWS public servers without the need for the VPC to have an attached internet gateway and be public.\nVPC Endpoint Types:\n Gateway endpoints: Can be used for DynamoDB and S3 Interface endpoints: Can be used for everything else (e.g. SNS, SQS)  When to Use a VPC Endpoint:\n If the entire VPC is private with no IGW If a specific instance has no public IP/NATGW and needs to access public services To access resources restricted to specific VPCs or endpoints (private S3 bucket)   Limitations and Considerations: * Gateway endpoints are used via route table entries - they are gateway devices. Prefix lists for a service are used in the destintation field with the gateway as the target. * Gateway endpoints can be restricted via policies * Gateway endpoints are HA across AZs in a region * Interface endpoints are interfaces in a specific subnet. For HA, you need to add multiple interfaces - one per AZ * Interface endpoints are controlled via SGs on that interface. NACLs also impact traffic. * Interface endpoints add or replace the DNS for the service - no route table updates are required. * Code changes to use the endpoint DNS, or enable private DNS to override the default service DNS.\nEgress-only internet gateways provide IPv6 instances with outgoing access to the public internet using IPv6 but prevent the instances from being accessed from the internet.\nNAT isn\u0026rsquo;t required with IPv6, and so NATGW\u0026rsquo;s are compatible with IPv6. Egress-only gateways provide the outgoing-only access of a NATGW but do so without adjusting any IP addresses.\n DNS 101 "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/05_authorization_access_control/",
	"title": "05 - Authentication, Authorization, and Admission Control",
	"tags": [],
	"description": "",
	"content": "  Authentication Authorization  Types of RoleBindings Admission Control  Demo - Authentication and Authorization  To access and manage any Kubernetes resource or object in the cluster, we need to access a specific API endpoint on the API server. Each access request goes through the following three stages:\n Authentication - Logs in a user Authorization - Authorizes the API requests added by the logged-in user. Admission Control - Software modules that can modify or reject the requests based on some additional checks, like a pre-set Quota.  Authentication Kubernetes does not have an object called user, nor does it store usernames or other related details in its object store. However, even without that, Kubernetes can use usernames for access control and request logging.\nKubernetes has two kinds of users:\n Normal Users They are managed outside of the Kubernetes cluster via independent services like User/Client Certificates, a file listing usernames/passwords, Google accounts, etc.\n Service Accounts With Service Account users, in-cluster processes communicate with the API server to perform different operations. Most of the Service Account users are created automatically via the API server, but they can also be created manually. The Service Account users are tied to a given Namespace and mount the respective credentials to communicate with the API server as Secrets.\n  For authentication, Kubernetes uses different authentication modules:\n Client Certificates To enable client certificate authentication, we need to reference a file containing one or more certificate authorities by passing the --client-ca-file=SOMEFILE option to the API server. The certificate authorities mentioned in the file would validate the client certificates presented to the API server. A demonstration video covering this topic is also available at the end of this chapter.\n Static Token File We can pass a file containing pre-defined bearer tokens with the --token-auth-file=SOMEFILE option to the API server. Currently, these tokens would last indefinitely, and they cannot be changed without restarting the API server.\n Bootstrap Tokens This feature is currently in beta status and is mostly used for bootstrapping a new Kubernetes cluster.\n Static Password File It is similar to Static Token File. We can pass a file containing basic authentication details with the --basic-auth-file=SOMEFILE option. These credentials would last indefinitely, and passwords cannot be changed without restarting the API server.\n Service Account Tokens This is an automatically enabled authenticator that uses signed bearer tokens to verify the requests. These tokens get attached to Pods using the ServiceAccount Admission Controller, which allows in-cluster processes to talk to the API server.\n OpenID Connect Tokens OpenID Connect helps us connect with OAuth2 providers, such as Azure Active Directory, Salesforce, Google, etc., to offload the authentication to external services.\n Webhook Token Authentication With Webhook-based authentication, verification of bearer tokens can be offloaded to a remote service.\n Authenticating Proxy If we want to program additional authentication logic, we can use an authenticating proxy.\n  Authorization After a successful authentication, users can send the API requests to perform different operations. Then, those API requests get authorized by Kubernetes using various authorization modules.\n Node Authorizer Attribute-Based Access Control (ABAC) Authorizer Webhook Authorizer Role-Based Access Control (RBAC) Authorizer  Role - With Role, we can grant access to resources within a specific Namespace.\nClusterRole - The ClusterRole can be used to grant the same permissions as Role does, but its scope is cluster-wide.\nkind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: lfs158 name: pod-reader rules: - apiGroups: [\u0026quot;\u0026quot;] # \u0026quot;\u0026quot; indicates the core API group resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]  Types of RoleBindings RoleBinding\nIt allows us to bind users to the same namespace as a Role. We could also refer a ClusterRole in RoleBinding, which would grant permissions to Namespace resources defined in the ClusterRole within the RoleBinding’s Namespace.\nClusterRoleBinding\nIt allows us to grant access to resources at a cluster-level and to all Namespaces.\nTo enable the RBAC authorizer, we would need to start the API server with the \u0026ndash;authorization-mode=RBAC option\n Admission Control Admission control is used to specify granular access control policies, which include allowing privileged containers, checking on resource quota, etc. To use admission controls, we must start the Kubernetes API server with the --enable-admission-plugins, which takes a comma-delimited, ordered list of controller names:\n--enable-admission-plugins=NamespaceLifecycle,ResourceQuota,PodSecurityPolicy,DefaultStorageClass  Demo - Authentication and Authorization  Configuring User by assigning key and certificate Create Context with newly created User Add RBAC Role and rolebindings to namespace  minikube start kubectl config view # Create New Namespace for Demo kubectl create namespace lfs158 mkdir rbac \u0026amp;\u0026amp; cd rbac # Create a private key for the student user with openssl tool, then create a certificate signing request for the student user with openssl tool openssl genrsa -out student.key 2048 openssl req -new -key student.key -out student.csr -subj \u0026quot;/CN=student/O=learner\u0026quot; cat student.csr | base64 | tr -d '\\n' touch signing-request.yaml apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: student-csr spec: groups: - system:authenticated request: \u0026lt;assign encoded value from cat command\u0026gt; usages: - digital signature - key encipherment - client auth kubectl create -f signing-request.yaml kubectl get csr kubectl certificate approve student-csr kubectl get csr # Generating User Certificate kubectl get csr student-csr -o jsonpath='{.status.certificate}' | base64 --decode \u0026gt; student.crt cat student.crt # Configuring Student User by assigning the key and certificate kubectl config set-credentials student --client-certificate=student.crt --client-key=student.key # Create Student Context with Selected User kubectl config set-context student-context --cluster=minikube --namespace=lfs158 --user=student kubectl config view # Creating a Deployment with Nginx Image kubectl -n lfs158 create deployment nginx --image=nginx:alpine  From the new context student-context try to list pods. The attempt fails because the student user has no permissions configured for the student-context:\nkubectl --context=student-context get pods  Error from server (Forbidden): pods is forbidden: User \u0026ldquo;student\u0026rdquo; cannot list resource \u0026ldquo;pods\u0026rdquo; in API group \u0026ldquo;\u0026rdquo; in the namespace \u0026ldquo;lfs158\u0026rdquo;\n# Create RBAC Role to allow only get, watch, list actions in lfs158 namespace ~/rbac$ vim role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: lfs158 rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] ~/rbac$ kubectl create -f role.yaml ~/rbac$ kubectl -n lfs158 get roles NAME AGE pod-reader 57s  Create a YAML configuration file for a rolebinding object, which assigns the permissions of the pod-reader role to the student user. Then create the rolebinding object and list it from the default minikube context, but from the lfs158 namespace:\n# Create RBAC Role Binding to the User ~/rbac$ vim rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-read-access namespace: lfs158 subjects: - kind: User name: student apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io ~/rbac$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/pod-read-access created ~/rbac$ kubectl -n lfs158 get rolebindings NAME AGE pod-read-access 23s  Now that we have assigned permissions to the student user, we can successfully list pods from the new context student-context.\n~/rbac$ kubectl --context=student-context get pods NAME READY STATUS RESTARTS AGE nginx-77595c695-f2xmd 1/1 Running 0 7m41s  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/05_observability/",
	"title": "05 - Observability",
	"tags": [],
	"description": "",
	"content": "  Liveness and Readiness Probes  Liveness probe Readiness Probe  Container Logging Installing Metrics Server Monitoring Applications  Liveness and Readiness Probes Probes - Allow you to customize how Kubernetes determines the status of your containers\nLiveness probe Indicates whether the container is running properly, and governs whether the cluster will automatically stop or restart the container. Liveness probes can be created by including them in the container spec. This probe runs a command to test the container\u0026rsquo;s liveness.\napiVersion: v1 kind: Pod metadata: name: my-liveness-pod spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', \u0026quot;echo Hello, Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026quot;] livenessProbe: exec: command: - echo - testing initialDelaySeconds: 5 periodSeconds: 5  Readiness Probe Indicates whether the container is ready to service requests, and governs whether request will be forwarded to the pod.\nLiveness and readiness probes can determine container status by doing things like running a command or making an http request\napiVersion: v1 kind: Pod metadata: name: my-readiness-pod spec: containers: - name: myapp-container image: nginx readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 5 periodSeconds: 5  Container Logging A container\u0026rsquo;s normal console output goes into the container log. You can access container logs using the kubectl logs command. For example, create a pod that generates some output every second:\napiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: [/bin/sh, -c, 'i=0; while true; do echo \u0026quot;$i: $(date)\u0026quot;; i=$((i+1)); sleep 1; done']  If a pod has more than one container, you must specify which container to get logs from using the -c flag.\nkubectl logs my-pod -c my-container  Installing Metrics Server The Kubernetes metrics server provides an API which allows you to access data about your pods, and nodes, such as CPU and memory usage.\ncd ~/ git clone https://github.com/linuxacademy/metrics-server kubectl apply -f ~/metrics-server/deploy/1.8+/  Verify\nkubectl get --raw /apis/metrics.k8s.io/  Monitoring Applications With a working metrics server, you can use the kubectl top to gather information about resource usage within the cluster.\nkubectl top pods kubectl top pod resource-consumer-big kubectl top pods -n kube-system kubectl top nodes  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/06_pod-design/",
	"title": "06 - Pod Design",
	"tags": [],
	"description": "",
	"content": "  Labels, Selectors, and Annotations Deployments Rolling Updates and Rollbacks  Rolling Updates and Rollbacks  Jobs and CronJobs  Labels, Selectors, and Annotations Labels are key-value pairs attached to Kubernetes objects. They are used for identifying various attributes of objects which can in turn be used to select and group various subsets of those objects.\nWe can attach labels to objects by listing them in the metadata.labels section of an object descriptor.\napiVersion: v1 kind: Pod metadata: name: my-production-label-pod labels: app: my-app environment: production spec: containers: - name: nginx image: nginx  kubectl get pods --show-labels  Selectors are used for identifying and selecting a specific group of objects using their labels. One way to use selectors is to use them with kubectl get to retrieve a specific list of objects. We can specify a selector using the -l flag.\n# Equality Based Selector kubectl get pods -l app=my-app # InEquality Based Selector kubectl get pods -l env!=prod # Set Based Selector kubectl get pods -l 'env in (dev, qa)' # Chain Multiple Selectors kubectl get pods -l app=myapp, env=prod  Annotations are similar to labels in that they can be used to store custom metadata about objects.\nHowever, unline labels, annotations cannot be used to select or group objects in Kubernetes. External tools can read, write and interact with anootations.\n We can attach annotations to objects using the metadata.annotations section of the object descriptors:\napiVersion: v1 kind: Pod metadata: name: my-annotation-pod annotations: owner: bob@homeoffice.com git-commit: bd7833kjgll20xj3 spec: containers: - name: nginx image: nginx  Like labels, existing annotations can also be viewed using kubectl describe\nDeployments Deployments provide a way to declaratively manage a dynamic set of replica pods. They provide powerful functionality such as scalling and rolling updates.\nA deployment define a desired state for the replica pods. The cluster will constantly work to maintain the desired state, creating, removing, and modifying the replica pods accordingly.\nA deployment is a Kubernetes object that can be created using a descriptor:\nnginx-deployment.yml\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  Note the following: * spec.replicas - The number of replica pods * spec.template - A template pod descriptor which defines the pods which will be created * spec.selector - The deployment will manage all pods whose label match this selector\nkubectl get deployments kubectl get deployment \u0026lt;deployment_name\u0026gt; kubectl describe deployment \u0026lt;deployment_name\u0026gt; kubectl edit deployment \u0026lt;deployment_name\u0026gt; kubectl delete deployment \u0026lt;deployment_name\u0026gt;  Rolling Updates and Rollbacks Rolling updates provide a way to update a deployment to a new container version by gradually updating replicas so that there is no downtime.\nrolling-deployment-nginx.yml\napiVersion: apps/v1 kind: Deployment metadata: name: rolling-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.1 ports: - containerPort: 80  # Set New Image for the Deployment kubectl set image deployment/rolling-deployment nginx=nginx:1.7.9 --record # Perform Rollout kubectl rollout history deployment/rolling-deployment  The \u0026ndash;record flag records information about the update so that it can be rolled back later.\n Rolling Updates and Rollbacks Rollbacks allow us to revert to a previous state. For example, if a rolling update breaks something, we can quickly recover by using a rollback.\n# Get Rollout History for Deployment kubectl rollout history deployment/rolling-deployment  The \u0026ndash;revision flag will give more information on a specific revision number\n # Perform Rollback to previous revision kubectl rollout undo deployment/rolling-deployment kubectl rollout undo deployment/rolling-deployment --to-revision=1  You can also control how rolling update are performed by setting maxSurge and maxUnavailable in the deployment spec:\napiVersion: apps/v1 kind: Deployment metadata: name: rolling-deployment spec: strategy: rollingUpdate: maxSurge: 3 maxUnavailable: 2 replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.1 ports: - containerPort: 80  Jobs and CronJobs Jobs can be used to reliably execute a workload until it completes. The job will create one or more pods. When the job is finished, the container(s) will exit and pods(s) will enter the Completed state.\nbatch-job.yml\napiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\u0026quot;perl\u0026quot;, \u0026quot;-Mbignum=bpi\u0026quot;, \u0026quot;-wle\u0026quot;, \u0026quot;print bpi(2000)\u0026quot;] restartPolicy: Never backoffLimit: 4  hello-cron-job.yml\napiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \u0026quot;*/1 * * * *\u0026quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/06_services/",
	"title": "06 - Services",
	"tags": [],
	"description": "",
	"content": "  Services Service Object Example kube-proxy Service Discovery Servie Type  Cluster IP NodePort LoadBalancer ExternalIP ExternalName   Services  An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you don’t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.\n Using the selectors app==frontend and app==db, we group Pods into two logical sets: one with 3 Pods, and one with a single Pod.\nWe assign a name to the logical grouping, referred to as a Service. In our example, we create two Services, frontend-svc, and db-svc, and they have the app==frontend and the app==db Selectors, respectively.\nServices can expose single Pods, ReplicaSets, Deployments, DaemonSets, and StatefulSets.\nService Object Example kind: Service apiVersion: v1 metadata: name: frontend-svc spec: selector: app: frontend ports: - protocol: TCP port: 80 targetPort: 5000  The user/client now connects to a Service via its ClusterIP, which forwards traffic to one of the Pods attached to it. A Service provides load balancing by default while selecting the Pods for traffic forwarding.\nWhile the Service forwards traffic to Pods, we can select the targetPort on the Pod which receives the traffic. If the targetPort is not defined explicitly, then traffic will be forwarded to Pods on the port on which the Service receives traffic.\n kube-proxy All worker nodes run a daemon called kube-proxy, which watches the API server on the master node for the addition and removal of Services and endpoints.\nService Discovery As Services are the primary mode of communication in Kubernetes, we need a way to discover them at runtime. Kubernetes supports two methods for discovering Services:\n Environment Variables DNS  Kubernetes has an add-on for DNS, which creates a DNS record for each Service and its format is my-svc.my-namespace.svc.cluster.local.\nServie Type While defining a Service, we can also choose its access scope. We can decide whether the Service:\n Is only accessible within the cluster Is accessible from within the cluster and the external world Maps to an entity which resides either inside or outside the cluster.  Access scope is decided by ServiceType, which can be configured when creating the Service.\n Cluster IP ClusterIP is the default ServiceType. A Service receives a Virtual IP address, known as its ClusterIP. This Virtual IP address is used for communicating with the Service and is accessible only within the cluster.\nNodePort The NodePort ServiceType is useful when we want to make our Services accessible from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. To access multiple applications from the external world, administrators can configure a reverse proxy - an ingress, and define rules that target Services within the cluster.\nkubectl expose pod \u0026lt;podname\u0026gt; --type=NodePort --port=80  LoadBalancer With the LoadBalancer ServiceType:\n NodePort and ClusterIP are automatically created, and the external load balancer will route to them The Service is exposed at a static port on each worker node The Service is exposed externally using the underlying cloud provider\u0026rsquo;s load balancer feature.  The LoadBalancer ServiceType will only work if the underlying infrastructure supports the automatic creation of Load Balancers and have the respective support in Kubernetes, as is the case with the Google Cloud Platform and AWS. If no such feature is configured, the LoadBalancer IP address field is not populated, and the Service will work the same way as a NodePort type Service.\n ExternalIP A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform or AWS.\nExternalName ExternalName is a special ServiceType, that has no Selectors and does not define any endpoints. When accessed within the cluster, it returns a CNAME record of an externally configured Service.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/07_deploying_standalone_app/",
	"title": "07 - Deploying Standalone App",
	"tags": [],
	"description": "",
	"content": "  Creating Deployment using YAML File Exposing Application Liveness and Readiness Probes  Liveness Probe Readiness Probe   Creating Deployment using YAML File webserver.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: webserver labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80  kubectl create -f webserver.yaml  This will also create ReplicaSet and Pods as defined in the YAML configuration.\nExposing Application webserver-svc.yaml\napiVersion: v1 kind: Service metadata: name: web-service labels: run: web-service spec: type: NodePort ports: - port: 80 protocol: TCP selector: app: nginx  kubectl create -f webserver-svc.yaml  Liveness and Readiness Probes Liveness Probe Liveness probe checks on an application\u0026rsquo;s health, and if the health check fails, kubelet restarts the affected container automatically.\nLiveness Probes can be set by defining:\n Liveness command  apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5   Liveness HTTP request  livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3   TCP Liveness Probe  livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20  Readiness Probe Readiness Probe can be used to ensure certain conditions are met before application can serve traffic. These conditions include ensuring that the depending service is ready, or acknowledging that a large dataset needs to be loaded, etc. In such cases, we use Readiness Probes and wait for a certain condition to occur.\nreadinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/08_kubernetes_volume_management/",
	"title": "08 - Kubernetes Volume Management",
	"tags": [],
	"description": "",
	"content": "  Volumes Volume Types PersistentVolumeClaims Using a Shared hostPath Volume Type  Volumes As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the kubelet will restart it with a clean slate, which means that it will not have any of the old data.\nTo overcome this problem, Kubernetes uses Volumes. A Volume is essentially a directory backed by a storage medium. The storage medium, content and access mode are determined by the Volume Type. In Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\nIn Kubernetes, a Volume is attached to a Pod and can be shared among the containers of that Pod. The Volume has the same life span as the Pod, and it outlives the containers of the Pod - this allows data to be preserved across container restarts.\n Volume Types  emptyDir - An empty Volume is created for the Pod as soon as it is scheduled on the worker node. The Volume\u0026rsquo;s life is tightly coupled with the Pod. If the Pod is terminated, the content of emptyDir is deleted forever.\n hostPath - With the hostPath Volume Type, we can share a directory from the host to the Pod. If the Pod is terminated, the content of the Volume is still available on the host. gcePersistentDisk - With the gcePersistentDisk Volume Type, we can mount a Google Compute Engine (GCE) persistent disk into a Pod. awsElasticBlockStore - With the awsElasticBlockStore Volume Type, we can mount an AWS EBS Volume into a Pod. azureDisk - With azureDisk we can mount a Microsoft Azure Data Disk into a Pod. azureFile - With azureFile we can mount a Microsoft Azure File Volume into a Pod. cephfs - With cephfs, an existing CephFS volume can be mounted into a Pod. When a Pod terminates, the volume is unmounted and the contents of the volume are preserved. nfs - With nfs, we can mount an NFS share into a Pod. iscsi - With iscsi, we can mount an iSCSI share into a Pod. secret - With the secret Volume Type, we can pass sensitive information, such as passwords, to Pods. We will take a look at an example in a later chapter. configMap - With configMap objects, we can provide configuration data, or shell commands and arguments into a Pod. persistentVolumeClaim - We can attach a PersistentVolume to a Pod using a persistentVolumeClaim. We will cover this in our next section.  PersistentVolumeClaims A PersistentVolumeClaim (PVC) is a request for storage by a user. Users request for PersistentVolume resources based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim.\nUsing a Shared hostPath Volume Type apiVersion: v1 kind: Pod metadata: name: share-pod labels: app: share-pod spec: volumes: - name: \u0026quot;host-volume\u0026quot; hostPath: path: \u0026quot;/home/docker/pod-volume\u0026quot; containers: - image: nginx name: nginx volumeMounts: - mountPath: \u0026quot;/usr/share/nginx/html\u0026quot; name: \u0026quot;host-volume\u0026quot; ports: - containerPort: 80 - image: debian name: debian volumeMounts: - mountPath: /host-vol name: \u0026quot;host-volume\u0026quot; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Hello Kubernetes \u0026gt; /host-vol/index.html; sleep 3600\u0026quot;]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/09_configmaps_secrets/",
	"title": "09 - ConfigMaps and Secrets",
	"tags": [],
	"description": "",
	"content": "  ConfigMaps  Creating ConfigMaps Using CommandLine Using Configuration File Using Properties File Using ConfigMaps Inside Pods As Environment Variable As Volume  Secrets  Creating Secret Create a Secret from Literal and Display Its Details Create a Secret from YAML File Use Secrets Inside Pods   ConfigMaps ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories.\nCreating ConfigMaps Using CommandLine kubectl create configmap my-config \\ --from-literal=key1=value1 \\ --from-literal=key2=value2  Using Configuration File apiVersion: v1 kind: ConfigMap metadata: name: customer1 data: TEXT1: Customer1_Company TEXT2: Welcomes You COMPANY: Customer1 Company Technology Pct. Ltd.  Using Properties File permission-reset.properties\npermission=read-only allowed=\u0026quot;true\u0026quot; resetCount=3  kubectl create configmap permission-config --from-file=\u0026lt;path/to/\u0026gt;permission-reset.properties  Using ConfigMaps Inside Pods As Environment Variable Inside a Container, we can retrieve the key-value data of an entire ConfigMap or the values of specific ConfigMap keys as environment variables.\ncontainers: - name: myapp-full-container image: myapp envFrom: - configMapRef: name: full-config-map  containers: - name: myapp-specific-container image: myapp env: - name: SPECIFIC_ENV_VAR1 valueFrom: configMapKeyRef: name: config-map-1 key: SPECIFIC_DATA - name: SPECIFIC_ENV_VAR2 valueFrom: configMapKeyRef: name: config-map-2 key: SPECIFIC_INFO  As Volume We can mount a vol-config-map ConfigMap as a Volume inside a Pod. For each key in the ConfigMap, a file gets created in the mount path (where the file is named with the key name) and the content of that file becomes the respective key\u0026rsquo;s value:\ncontainers: - name: myapp-vol-container image: myapp volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: vol-config-map  Secrets Secret object can help by allowing us to encode the sensitive information before sharing it. With Secrets, we can share sensitive information like passwords, tokens, or keys in the form of key-value pairs, similar to ConfigMaps; thus, we can control how the information in a Secret is used, reducing the risk for accidental exposures. In Deployments or other resources, the Secret object is referenced, without exposing its content.\nIt is important to keep in mind that the Secret data is stored as plain text inside etcd, therefore administrators must limit access to the API server and etcd. A newer feature allows for Secret data to be encrypted at rest while it is stored in etcd; a feature which needs to be enabled at the API server level.\n Creating Secret Create a Secret from Literal and Display Its Details # Create Secret kubectl create secret generic my-password --from-literal=password=mysqlpassword # Get Secret kubectl get secret my-password # Describe Secret kubectl describe secret my-password  Create a Secret from YAML File echo mysqlpassword | base64 bXlzcWxwYXNzd29yZAo=  mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque data: password: bXlzcWxwYXNzd29yZAo=  Please note that base64 encoding does not mean encryption, and anyone can easily decode our encoded data\n mypass.yaml\napiVersion: v1 kind: Secret metadata: name: my-password type: Opaque stringData: password: mysqlpassword  # Create Secret from YAML kubectl create -f mypass.yaml  Use Secrets Inside Pods Using Secrets as Environment Variables\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress env: - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: my-password key: password  Using Secrets as Files from a Pod\nspec: containers: - image: wordpress:4.7.3-apache name: wordpress volumeMounts: - name: secret-volume mountPath: \u0026quot;/etc/secret-data\u0026quot; readOnly: true volumes: - name: secret-volume secret: secretName: my-password  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/10_ingress/",
	"title": "10 - Ingress",
	"tags": [],
	"description": "",
	"content": " With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.\n An Ingress is a collection of rules that allow inbound connections to reach the cluster Services.\n To allow the inbound connection to reach the cluster Services, Ingress configures a Layer 7 HTTP/HTTPS load balancer for Services and provides the following:\n TLS (Transport Layer Security) Name-based virtual hosting Fanout routing Loadbalancing Custom rules  Name-Based Virtual Hosting virtual-host-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: virtual-host-ingress namespace: default spec: rules: - host: blue.example.com http: paths: - backend: serviceName: webserver-blue-svc servicePort: 80 - host: green.example.com http: paths: - backend: serviceName: webserver-green-svc servicePort: 80  fan-out-ingress.yaml\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: fan-out-ingress namespace: default spec: rules: - host: example.com http: paths: - path: /blue backend: serviceName: webserver-blue-svc servicePort: 80 - path: /green backend: serviceName: webserver-green-svc servicePort: 80  The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller, which we will discuss next.\n # Start the Ingress Controller with Minikube minikube addons enable ingress # Deploy an Ingress Resource kubectl create -f virtual-host-ingress.yaml  "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/11_advanced_topics/",
	"title": "11 - Advanced Topics",
	"tags": [],
	"description": "",
	"content": " Annotations With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:\n\u0026quot;annotations\u0026quot;: { \u0026quot;key1\u0026quot; : \u0026quot;value1\u0026quot;, \u0026quot;key2\u0026quot; : \u0026quot;value2\u0026quot; }  Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:\n Store build/release IDs, PR numbers, git branch, etc. Phone/pager numbers of people responsible, or directory entries specifying where such information can be found Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc.  apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webserver annotations: description: Deployment based PoC dates 2nd May'2019  Jobs and CronJobs A Job creates one or more Pods to perform a given task. The Job object takes the responsibility of Pod failures. It makes sure that the given task is completed successfully. Once the task is complete, all the Pods have terminated automatically. Job configuration options include\n parallelism - to set the number of pods allowed to run in parallel; completions - to set the number of expected completions; activeDeadlineSeconds - to set the duration of the Job; backoffLimit - to set the number of retries before Job is marked as failed; ttlSecondsAfterFinished - to delay the clean up of the finished Jobs.  Quota Management Autoscalling Autoscaling can be implemented in a Kubernetes cluster via controllers which periodically adjust the number of running objects based on single, multiple, or custom metrics. There are various types of autoscalers available in Kubernetes which can be implemented individually or combined for a more robust autoscaling solution:\nHorizontal Pod Autoscaler (HPA) HPA is an algorithm based controller API resource which automatically adjusts the number of replicas in a ReplicaSet, Deployment or Replication Controller based on CPU utilization.\nVertical Pod Autoscaler (VPA) VPA automatically sets Container resource requirements (CPU and memory) in a Pod and dynamically adjusts them in runtime, based on historical utilization data, current resource availability and real-time events.\nCluster Autoscaler Cluster Autoscaler automatically re-sizes the Kubernetes cluster when there are insufficient resources available for new Pods expecting to be scheduled or when there are underutilized nodes in the cluster.\nDaemonSets A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\n running a cluster storage daemon, such as glusterd, ceph, on each node. running a logs collection daemon on every node, such as fluentd or logstash. running a node monitoring daemon on every node, such as Prometheus Node Exporter, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or Instana Agent.  StatefulSets The StatefulSet controller is used for stateful applications which require a unique identity, such as name, network identifications, strict ordering, etc. For example, MySQL cluster, etcd cluster.\nThe StatefulSet controller provides identity and guaranteed ordering of deployment and scaling to Pods. Similar to Deployments, StatefulSets use ReplicaSets as intermediary Pod controllers and support rolling updates and rollbacks.\nKubernetes Federation With Kubernetes Cluster Federation we can manage multiple Kubernetes clusters from a single control plane. We can sync resources across the federated clusters and have cross-cluster discovery. This allows us to perform Deployments across regions, access them using a global DNS record, and achieve High Availability.\nHelm Chart To deploy an application, we use different Kubernetes manifests, such as Deployments, Services, Volume Claims, Ingress, etc. Sometimes, it can be tiresome to deploy them one by one. We can bundle all those manifests after templatizing them into a well-defined format, along with other metadata. Such a bundle is referred to as Chart. These Charts can then be served via repositories, such as those that we have for rpm and deb packages.\nHelm is a package manager (analogous to yum and apt for Linux) for Kubernetes, which can install/update/delete those Charts in the Kubernetes cluster.\nHelm has two components:\n A client called helm, which runs on your user\u0026rsquo;s workstation A server called tiller, which runs inside your Kubernetes cluster.  The client helm connects to the server tiller to manage Charts.\nSecurity Contexts and Pod Security Policies At times we need to define specific privileges and access control settings for Pods and Containers. Security Contexts allow us to set Discretionary Access Control for object access permissions, privileged running, capabilities, security labels, etc. However, their effect is limited to the individual Pods and Containers where such context configuration settings are incorporated in the spec section.\nIn order to apply security settings to multiple Pods and Containers cluster-wide, we can define Pod Security Policies. They allow more fine-grained security settings to control the usage of the host namespace, host networking and ports, file system groups, usage of volume types, enforce Container user and group ID, root privilege escalation, etc.\nNetwork Policies Kubernetes was designed to allow all Pods to communicate freely, without restrictions, with all other Pods in cluster Namespaces. In time it became clear that it was not an ideal design, and mechanisms needed to be put in place in order to restrict communication between certain Pods and applications in the cluster Namespace. Network Policies are sets of rules which define how Pods are allowed to talk to other Pods and resources inside and outside the cluster. Pods not covered by any Network Policy will continue to receive unrestricted traffic from any endpoint. Network Policies are very similar to typical Firewalls. They are designed to protect mostly assets located inside the Firewall but can restrict outgoing traffic as well based on sets of rules and policies.\nThe Network Policy API resource specifies podSelectors, *Ingress* and/or *Egress* policyTypes, and rules based on source and destination ipBlocks and ports. Very simplistic default allow or default deny policies can be defined as well. As a good practice, it is recommended to define a default deny policy to block all traffic to and from the Namespace, and then define sets of rules for specific traffic to be allowed in and out of the Namespace. Let\u0026rsquo;s keep in mind that not all the networking solutions available for Kubernetes support Network Policies. Review the Pod-to-Pod Communication section from the Kubernetes Architecture chapter if needed. By default, Network Policies are namespaced API resources, but certain network plugins provide additional features so that Network Policies can be applied cluster-wide.\nMonitoring and Logging In Kubernetes, we have to collect resource usage data by Pods, Services, nodes, etc., to understand the overall resource consumption and to make decisions for scaling a given application. Two popular Kubernetes monitoring solutions are the Kubernetes Metrics Server and Prometheus.\n Metrics Server\nMetrics Server is a cluster-wide aggregator of resource usage data - a relatively new feature in Kubernetes.  PrometheusPrometheus, now part of CNCF (Cloud Native Computing Foundation), can also be used to scrape the resource usage from different Kubernetes components and objects. Using its client libraries, we can also instrument the code of our application.  Another important aspect for troubleshooting and debugging is Logging, in which we collect the logs from different components of a given system. In Kubernetes, we can collect logs from different cluster components, objects, nodes, etc. Unfortunately, however, Kubernetes does not provide cluster-wide logging by default, therefore third party tools are required to centralize and aggregate cluster logs. The most common way to collect the logs is using Elasticsearch, which uses fluentd with custom configuration as an agent on the nodes. fluentd is an open source data collector, which is also part of CNCF.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/12_resources/",
	"title": "12 - Resources",
	"tags": [],
	"description": "",
	"content": " Labs CKA Curriculum   "
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/aws/aws_certified_solutions_architect/",
	"title": "AWS Certified Solutions Architect",
	"tags": [],
	"description": "",
	"content": "  01 - AWS Fundamentals   02 - Identity and Access Control   03 - Compute   04 - Networking   "
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/",
	"title": "Arrow Functions",
	"tags": [],
	"description": "",
	"content": " Arrow Function Express allows you to write shorter syntax than it\u0026rsquo;s predecessor Function expression. In addition and more exciting is how the new Arrow function bind their context.\n(param1, param2, param3) =\u0026gt; { statements } singleParam =\u0026gt; { statements } () =\u0026gt; { statements }  Example var materials = [ \u0026#39;Iron\u0026#39;, \u0026#39;Calcium\u0026#39;, \u0026#39;Sodium\u0026#39;, \u0026#39;Magnanese\u0026#39; ] materials.map(material =\u0026gt; material.length)  An arrow function does not newly define its own this when it\u0026rsquo;s being executed.The value of this is always inherited from the enclosing scope.\n// ES5 function CounterES5(){ this.seconds = 0; window.setInterval(function() { this.seconds++; console.log(seconds); }.bind(this), 1000); } //ES6 function CounterES6(){ this.seconds =0; window.setInterval( () =\u0026gt; { this.seconds++; console.log(seconds) },1000 ); }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-28-ubuntu-packages/",
	"title": "Backup and Restore Ubuntu Packages",
	"tags": [],
	"description": "",
	"content": " List Packages Installed dpkg -l\nCreate Backup of What Packages Installed dpkg --get-selection \u0026gt; list.txt\nRestore dpkg --clear-selections sudo dpkg --set-selections \u0026lt; list.txt sudo apt-get autoremove sudo apt-get dselect-upgrade  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/",
	"title": "Basic Essentials",
	"tags": [],
	"description": "",
	"content": " Creting Instance (Directly) # Create Compute Instance gcloud compute instances create gcelab2 --machine-type n1-standard-2 \\ --zone us-central1-c  Using Instance Templates / Instance Groups # Create Instance Template gcloud compute instance-templates create nginx-template \\ --metadata-from-file startup-script=startup.sh # Create Target Pool gcloud compute target-pools create nginx-pool # Create Instance Group gcloud compute instance-groups managed create nginx-group \\ --base-instance-name nginx \\ --size 2 \\ --template nginx-template \\ --target-pool nginx-pool # List Instances gcloud compute instances list  Create Filewall # Create Filewall rule to Allow 80 gcloud compute firewall-rules create www-firewall --allow tcp:80  SSH Instance gcloud compute ssh gcelab2 --zone us-central1-c   gcloud is a command-line tool for Google Cloud Platform\n  gsutil is a command-line tool to mange Cloud Storage resources\n Config # List Config gcloud config list # Sets Default Zone to us-central1-a gcloud config set compute/zone us-central1-a  Kubernetes Cluster Engine A cluster consists of at least one cluster master machine and multiple worker machines called nodes. Nodes are Compute Engine virtual machine (VM) instances that run the Kubernetes processes necessary to make them part of the cluster.\n# Create Cluster gcloud container clusters create my-precious-cluster # Updates a kubeconfig file with appropriate credentials to point kubectl at a specific cluster in GKE gcloud container cluster get-credentials my-precious-cluster # Delete Cluster gcloud container clusters delete my-precious-cluster  Network Load Balancer Network load balancing allows you to balance the load of your systems based on incoming IP protocol data, such as address, port, and protocol type. You also get some options that are not available, with HTTP(S) load balancing. For example, you can load balance additional TCP/UDP-based protocols such as SMTP traffic. And if your application is interested in TCP-connection-related characteristics, network load balancing allows your app to inspect the packets, where HTTP(S) load balancing does not.\n# Create Forwarding Rules gcloud compute forwarding-rules create nginx-lab \\ --region us-central1 --port=80 --target-pool nginx-pool # List Forwarding Rules gcloud compute forwarding-rules list  Creating HTTP(s) Load Balancer # Create Health Check gcloud compute http-health-checks create http-basic-check # Defining a HTTP service and map a port name to the relevant port gcloud compute instance-groups manged set-named-ports nginx-group --named-ports http:80 # Creating Backend Service gcloud compute backend-services create nginx-backend \\ --protocol HTTP \\ --http-health-checks http-basic-check \\ --global # Adding instance group to the backend services gcloud compute backend-services add-backend nginx-backend \\ --instance-group nginx-group \\ --instance-group-zone us-central1-a \\ --global # Create a default URL map that directs all incoming requests to all your instances gcloud compute url-maps create web-map --default-service nginx-backend # Create a target HTTP proxy to route requests to URL map gcloud compute target-http-proxies create http-lb-proxy --url-map web-map # Create global forwarding rule to handle and route incoming requests gcloud compute forwarding-rules create http-content-rule \\ --global --target-http-proxy http-lb-proxy --ports 80  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2016-02-29-berks/",
	"title": "Berkshelf",
	"tags": [],
	"description": "",
	"content": " Berkshelf Reference External Dependent Cookbooks so that it can download those cookbooks rather than manually downloading via knife cookbook command.\nknife cookbook site\nBerkshelf lets you treat your cookbooks the way you treat gem in a Ruby project. When external cookbooks are used, Berkshelf doesn\u0026rsquo;t requite knife cookbook site to install community cookbooks.\nImplementing Berkshelf gem install berkshelf\nInstall Cookbooks via Berks berks install\nUpload berks to Chef Server berks upload \u0026lt;cookbook\u0026gt;\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/bigdata/",
	"title": "Big Data",
	"tags": [],
	"description": "",
	"content": "  Hive Query Optimizations   Hive Tables [Best Practices]   Jupyter NoteBook - Shortcuts   Setting up Apache Storm   "
},
{
	"uri": "http://learn.aayushtuladhar.com/bookmarks/",
	"title": "Bookmarks",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/ckad/",
	"title": "CKAD Notes",
	"tags": [],
	"description": "",
	"content": " Reference  Linux Academy - CKAD Study Guide  "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/",
	"title": "Chef",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2018-01-30-chef-databags/",
	"title": "Chef Databags",
	"tags": [],
	"description": "",
	"content": " Databags are global variables that is stored as JSON Data and is accessible from Chef Server. A data bag is indexed for searching and can be loaded by recipe or accessed during a search.\nCreating Data Bag (Using Knife) $ knife data bag create DATA_BAG_NAME (DATA_BAG_ITEM) knife data bag create TEST_BAG  Adding File to Data Bag knife data bag from file TEST_BAG test.json  Data Bag Items A data bag is container of related data bag items, where each individual data bag item is a JSON file.\nUsing Data Bags data_bag_item('keystore_file', 'dev')  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/cloud-devops-engineer/",
	"title": "Cloud DevOps Engineer",
	"tags": [],
	"description": "",
	"content": " Networking Communication Model Three components of networking\n Medium - How are you connected ? Addressing - How do you locate / identify another party ? Content - What information are you sharing ?  Addressing\nIPv4 - 32 Bit address (192.168.112.20) ~ 4 Billion\nIPv6\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/",
	"title": "Cloud Storage",
	"tags": [],
	"description": "",
	"content": " Create a Stoage Bucket gsutil mb gs://unique-name  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/",
	"title": "Continuous Delivery with Jenkins in Kubernetes Engine",
	"tags": [],
	"description": "",
	"content": "# Create Kubernetes Cluster gcloud container clusters create jenkins-cd \\ --num-nodes 2 \\ --machine-type n1-standard-2 \\ --scopes \u0026quot;https://www.googleapis.com/auth/projecthosting,cloud-platform\u0026quot; # Update KubeConfig with Cluster credentials gcloud container clusters get-credentials jenkins-cd # Verify Kubernetes can connect to GCP Kubernetes Cluster kubectl cluster-info  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-01-29-dns-basics/",
	"title": "DNS Basics",
	"tags": [],
	"description": "",
	"content": " DNS (Domain Name System) is essential component of modern internet communication. It allows us to reference computers by human friendly names instead of IP Addresses.\nTerminologies  Domain Name IP Address  DNS Lookup using Dig Dig is a flexible tool for interrogating DNS name servers. It performs DNS lookup and is very helpful to troubleshoot DNS problems.\ndig \u0026lt;serverName\u0026gt; +nostats +nocomments +nocmd  $ dig google.com +nostats ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; google.com +nostats ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 9995 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;google.com.\tIN\tA ;; ANSWER SECTION: google.com.\t142\tIN\tA\t172.217.8.206   The first line tell us the version of dig (9.10.6) command Next, dig shows the header of the response it received from the DNS server The question section tells us about the A Record and IN means this is an internet lookup The answer section tells us that google.com has IP address of 172.217.8.206  References  DNS Concepts  "
},
{
	"uri": "http://learn.aayushtuladhar.com/data-analytics/",
	"title": "Data Analytics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/es-6/destructuring/",
	"title": "Destructuring JavaScript Objects",
	"tags": [],
	"description": "",
	"content": "const person = { firstName: \u0026#39;Aayush\u0026#39;, lastName: \u0026#39;Tuladhar\u0026#39;, country: \u0026#39;Nepal\u0026#39;, twitter: \u0026#39;@aayushtuladhar\u0026#39; } /* Problem */ const first = person.firstName; const last = person.lastName; console.log(`Hello ${first}${last}`); /* Solution */ const { firstName, lastName } = person; console.log(`Hello ${firstName}${lastName}`); /* ------------------ */ const art = { first: \u0026#39;ART\u0026#39;, last: \u0026#39;Ratna\u0026#39;, links: { social: { twitter: \u0026#39;https://twitter.com/aayushtuladhar\u0026#39;, facebook: \u0026#39;https://facebook.com/aayush.tuladhar\u0026#39;, }, web: { blog: \u0026#39;https://aayushtuladhar.com\u0026#39; } } }; const { twitter, facebook } = art.links.social; console.log(twitter); console.log(facebook);  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "",
	"content": "All the Things DevOps\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/",
	"title": "Digital Learning Notebook",
	"tags": [],
	"description": "",
	"content": " My Digital Learning Notebook Welcome to my digital notebook that I am sharing my learnings to capture important notes regarding various aspects of Software Engineering, Web Development and Cloud Engineering and other miscellenious topics.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-17-docker-compose/",
	"title": "Docker Compose",
	"tags": [],
	"description": "",
	"content": " Docker Compose is used to run multiple containers as a single service.\n# docker-compose.yml version: '2' services: web: build: . # build from Dockerfile context: ./Path dockerfile: Dockerfile ports: - \u0026quot;5000:5000\u0026quot; volumes: - .:/code redis: image: redis  Command Line # Start Service docker-compose start # Stop Service docker-compose stop # Pause Service docker-compose pause # UnPause Service docker-compose unpause # List containers docker-compose ps # Create and start containers docker-compose up # Stop and remove containers, networks, images, and volumes docker-compose down  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2018-04-19-drone/",
	"title": "Drone",
	"tags": [],
	"description": "",
	"content": "  Creating Pipeline  Images Cloning Commands Services Plugins  Running Drone Locally Drone Secrets  Repo Level Secrets Org Level Secrets Using Secrets in Pipeline   Drone is a CI/CD platform built on Docker and written in Go.\nCreating Pipeline Drone pipeline are written in .drone.yml file in the root of the repository.\n Pipelines are event based, which can be triggered via push, pull_request, tag and deployment events\npipeline: backend: image: golang commands: - go get - go build - go test frontend: image: node:6 commands: - npm install - npm test notify: image: plugins/slack channel: developers username: drone   Images Drone executes your build inside an ephemeral Docker Image, which means you don\u0026rsquo;t have to setup or install any repository dependencies.\nCloning Drone automatically clones your repository into a local volume that is mounted into each Docker container.\nCommands Drone mounts the workspace into your build container (Defined Image) and executes bash commands inside your build container, using the root of your repository as the working directory.\nServices Drone supports launching service containers as part of the build process. This can be very helpful when your unit tests require database access etc.\nPlugins Drone supports publish, deployments and notification capabilities through external plugins.\nRunning Drone Locally drone exec  Drone Secrets Repo Level Secrets # Create Value Secret (Concealed) drone secret add --conceal \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create Value Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretValue\u0026gt; # Create File Secret drone secret add \u0026lt;repo_path\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt; # View Secret for a Repo drone secret ls \u0026lt;repo_path\u0026gt;  Org Level Secrets # Create Org Level Secret (Concealed) drone org secret add --conceal \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; \u0026lt;secretPass\u0026gt; # Create Org Level File Secret drone org secret add \u0026lt;org\u0026gt; \u0026lt;secretName\u0026gt; @\u0026lt;filePath\u0026gt;  Using Secrets in Pipeline pipeline: docker: image: plugins/docker - username: ${DOCKER_USERNAME} - password: ${DOCKER_PASSWORD} + secrets: [ docker_username, docker_password ]  "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Learning the Basics Stuff Starting Server hugo server -D  Documentation Learn\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/reactjs/redux/",
	"title": "Getting Started with Redux",
	"tags": [],
	"description": "",
	"content": " Redux gives you a store, and lets you keep state in it, and get state out, and respond when the state changes. But that’s all it does.\nIt’s actually react-redux that lets you connect pieces of the state to React components.\n The state is the data, and the store is where it’s kept\n Redux Store Redux Reducer Reducer\u0026rsquo;s job is to take the current state and action and return the new state. It has another job, too. It should return the initial state the first time it\u0026rsquo;s called.\nReducer Rule # 1 = Never return undefined from a reducer Reduce Rule # 2 = Reduces must be a pure functions (They can\u0026rsquo;t modify their arguments, and they can\u0026rsquo;t have side effects)\nRedux Actions An action is Redux-speak for a plain object with a property called type. In order to keep things sane and maintainable, we Redux users usually give our actions types that are plain strings, and often uppercased, to signify that they’re meant to be constant values.\nAn action object describes a change you want to make (like “please increment the counter”) or an event that happenend (like “the request to the server failed with this error”).\nIn order to make an action DO something, you need to dispatch it.\nRedux Dispatch The store we created earlier has a built-in function called dispatch. Call it with an action, and Redux will call your reducer with that action (and then replace the state with whatever your reducer returned).\nReferences "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/",
	"title": "Google Cloud Platform",
	"tags": [],
	"description": "",
	"content": "  Basic Essentials   Cloud Storage   Continuous Delivery with Jenkins in Kubernetes Engine   Orchestrating Cloud with Kubernetes   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-07-06-gradle-wrapper/",
	"title": "Gradle Wrapper",
	"tags": [],
	"description": "",
	"content": " Gradle Wrapper The Gradle wrapper allows you to run a Gradle task without requiring that Gradle is installed on your system.\nCreating Gradle Wrapper task wrapper(type: Wrapper) { gradleVersion = '2.10' //we want gradle 2.10 to run this project }  Running Gradle Wrapper gradle wrapper\nFollowing files will be created:\n|-gradle |--- wrapper |--- gradle-wrapper.jar |--- gradle-wrapper.properties |-gradlew |-gradlew.bat  Gradle wrapper are useful when you want to run gradle command without installing gradle\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/bigdata/2017-05-18-hive-query-optimizations/",
	"title": "Hive Query Optimizations",
	"tags": ["hive", "bigdata"],
	"description": "",
	"content": " Changing Engine for Hive Queries In general, Tez provides increased performance over Map Reduce specially where JOINS are required resulting in intermediate output being written to disk.\nset hive.execution.engine=tez  Using Partitions Partitions separate date in Hive tables by HDFS directories. If data is distributed based on partitions on certain fields, Using those fields to access data allows speeding up Hive queries. In other words, querying without partitions equates to full table scan.\nSplits and Number of Files Considering splits and number of files created while doing joins helps to analyze query performance.\nPerformance Considerations for using JOINS Joins are important aspects of the SQL queries. Avoid using correlated queries and in-line tables. Create temporary tables and try to use inner join wherever possible. You can also use the Hive WITH clause instead of temporary tables\nAlso you should avoid any queries that leads to CARTESIAN JOINS.\nAvoid loading Too Many Small Files Hadoop works well with large files and it applies to Hive as well. You should avoid ingestion process that produces large number of small files. When producing outside Hive, use Text file format. Once you have data into Hive tables, then you can convert that to ORC or Parquet file format.\nChoose Appropriate File Format for the Data Choosing right file format will improve the Hive performance. Hive supports ORCfile, a new table storage format that sports fantastic speed improvements through techniques like predicate push-down, compression etc. ORC file increases esponse times for your HIVE queries.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/bigdata/2018-01-02-hive-tables-best-practices/",
	"title": "Hive Tables [Best Practices]",
	"tags": ["hive", "bigdata"],
	"description": "",
	"content": " Hive Table Storage Formats Avoid using TEXT format, Sequence file format or complex storage format such as JSON. Ideally, RCFile (Row Columnar File) or Parquet files are best suited. If you are building data warehouse on Hive, for better performance use Parquet file format.\nCREATE TABLE IF NOT EXISTS test_table ( col1 int, col2 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS PARQUET;  Compression Techniques Try to split compression algorithms provided by Hadoop \u0026amp; Hive like Snappy. Also you should avoid Gzip because it is not splittable and is CPU intensive.\nCREATE TABLE IF NOT EXISTS test_table ( col1 int, col2 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS PARQUET TBLPROPERTIES (\u0026quot;parquet.compress\u0026quot;=\u0026quot;SNAPPY\u0026quot;) \u0026quot;;  set hive.exec.compress.output=true; set hive.exec.compress.intermediate=true;  Partition Tables Partition will significantly reduce the table scan time. You should partition large Hive tables if you are collecting time series data. Hive Partition will store data in subdirectories like year/month/day. When you query the data, instead of scanning entire table, Hive will go to particular subdirectory and get you required data.\nBucketing Tables Hadoop Hive bucket concept is dividing Hive partition into number of equal clusters or buckets. Hive Buckets distribute the data load into user defined set of clusters. Hive distribute on hash code of key mentioned in query. Bucketing is useful when it is difficult to create partition on a column as it would be having huge variety of data in that column on which we want to run queries.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-09-22-install-docker-centos-7/",
	"title": "Installing Docker CE on Centos 7",
	"tags": [],
	"description": "",
	"content": "  Install Docker Community Edition Verify  Install Docker Community Edition sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast sudo yum install docker-ce sudo systemctl start docker  Verify sudo docker run hello-world  "
},
{
	"uri": "http://learn.aayushtuladhar.com/java/",
	"title": "Java",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/",
	"title": "JavaScript",
	"tags": [],
	"description": "",
	"content": "All the JavaScript Stuff\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-08-20-jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "  Installing Plugins  Pipeline Script   Installing Jenkins docker pull jenkins/jenkins # Persist Jenkins Data within the Docker Container docker volume create jenkins-data docker run --name jenkins-production \\ --detach \\ -p 50000:50000 \\ -p 8080:8080 \\ -v jenkins-data:/var/jenkins_home \\ jenkins/jenkins:2.164.2  Access Jenkins at http://localhost:8080/\nInstalling Plugins  Blueocean plugin  Manage Jenkins \u0026gt; Manage Plugins\nPipeline Script Hello World Pipeline Script\npipeline { agent none environment { APPLICATION_NAME = 'hello-jenkins-pipeline' } stages { stage('build') { steps { echo \u0026quot;Hello World\u0026quot; } } } }  Pipeline script to Build Another Jenkins Job\npipeline { agent none stages{ stage('build'){ steps { echo \u0026quot;Hello World\u0026quot; build 'project2-child' } } } }  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-13-jenkins-shared-libs/",
	"title": "Jenkins Shared Library",
	"tags": [],
	"description": "",
	"content": " Jenkins Shared library is the concept of having a common pipeline code in the version control system that can be used by any number of pipeline just by referencing it. In fact, multiple teams can use the same library for their pipelines. Pipeline has support for creating \u0026ldquo;Shared Libraries\u0026rdquo; which can be defined in external source control repositories and loaded into existing Pipelines.\n A shared library is a collection of independent Groovy scripts which you pull into your Jenkinsfile at runtime.\n Getting Started with Shared Library A Shared Library is defined with a name, a source code retrieval method such as by SCM, and optionally a default version. The name should be a short identifier as it will be used in scripts.\nDirectory structure jenkins-shared-library |____vars |____src |____resources  Resources  https://devopscube.com/create-jenkins-shared-library/ https://tomd.xyz/articles/jenkins-shared-library/ https://dev.to/kuperadrian/how-to-setup-a-unit-testable-jenkins-shared-pipeline-library-2e62 https://tomd.xyz/articles/jenkins-shared-library/  "
},
{
	"uri": "http://learn.aayushtuladhar.com/bigdata/2018-05-10-jupyter-notebooks/",
	"title": "Jupyter NoteBook - Shortcuts",
	"tags": [],
	"description": "",
	"content": " Jupyter Shortcuts Command Mode gives to the ability to create, copy, paste, move, and execute cells. A few keys to know: To enter Command Mode (control + m)\n   Keywork Description     h Bring up help (ESC to dismiss)   b Create cell below   a Create cell above   c Copy cell   v Paste cell below   Enter Go into Edit Mode   m Change cell type to Markdown   y Change cell type to code   ii Interrupt kernel   oo Restart kernel    "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/chef/2016-04-05-chef-knife-commands/",
	"title": "Knife Commands",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"content": " Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. This tutorial provides an overview of different kind of features and functionalities of Kubernetes and teaches how to manage the containerized infrastructure and application deployment.\nFeatures Automatic bin packing\nKubernetes automatically schedules containers based on resource needs and constraints, to maximize utilization without sacrificing availability.\nSelf-healing\nKubernetes automatically replaces and reschedules containers from failed nodes. It kills and restarts containers unresponsive to health checks, based on existing rules/policy. It also prevents traffic from being routed to unresponsive containers.\nHorizontal scaling\nWith Kubernetes applications are scaled manually or automatically based on CPU or custom metrics utilization.\nService discovery and Load balancing\nContainers receive their own IP addresses from Kubernetes, white it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set.\nAutomated rollouts and rollbacks\nKubernetes seamlessly rolls out and rolls back application updates and configuration changes, constantly monitoring the application\u0026rsquo;s health to prevent any downtime.\nSecret and configuration management\nKubernetes manages secrets and configuration details for an application separately from the container image, in order to avoid a re-build of the respective image. Secrets consist of confidential information passed to the application without revealing the sensitive content to the stack configuration, like on GitHub.\nStorage orchestration\nKubernetes automatically mounts software-defined storage (SDS) solutions to containers from local storage, external cloud providers, or network storage systems.\nBatch execution\nKubernetes supports batch execution, long-running jobs, and replaces failed containers.\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/java/the-basics/lambda_expressions/",
	"title": "Lambda Expressions",
	"tags": [],
	"description": "",
	"content": " Lambda expressions are not unknown to many of us who have worked on other popular programming languages like Scala. In Java programming language, a Lambda expression (or function) is just an anonymous function, i.e., a function with no name and without being bounded to an identifier. They are written exactly in the place where it’s needed, typically as a parameter to some other function.\nThe most important features of Lambda Expressions is that they execute in the context of their appearance. So, a similar lambda expression can be executed differently in some other context (i.e. logic will be same but results will be different based on different parameters passed to function).\nSynatax // This function takes two parameters and returns their sum //(parameters) -\u0026gt; expression (x, y) -\u0026gt; x + y (int a, int b) -\u0026gt; a * b //(parameters) -\u0026gt; { statements; } (x, y) -\u0026gt; { x+y; } //() -\u0026gt; expression () -\u0026gt; z () -\u0026gt; 100  Functional Interface A functional interface is an interface with a single abstract method (SAM). A class implements any interface by providing implementations for all the methods in it.\n@FunctionalInterface public interface Runnable { public abstract void run(); }  Type in which lambda expressions are converted, are always of functional interface type.\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/basic_2/",
	"title": "Layouts",
	"tags": [],
	"description": "",
	"content": " Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading  Renders to:\nh1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading Typography I am just being Bold\nI Love my Italics Style\nStrike Through\n I love to give a quotation\n Images ![Minion](https://octodex.github.com/images/minion.png)  Resizing Images ![Minion](https://octodex.github.com/images/minion.png?width=20pc)  Buttons Get Grav  Note / Info/ Tip / Warning A notice disclaimer\n An information disclaimer\n A tip disclaimer\n A warning disclaimer\n Expand   Expand me...   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-27-issue-remote-commands/",
	"title": "Linux Command Line Hacks",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-02-26-mqtt/",
	"title": "Mqtt",
	"tags": [],
	"description": "",
	"content": " Introduction  MQTT is a featherweight, ISO complaint PUB-SUB messaging protocol. Designed for low powered devices PRAM consistent: Guaranteed in-order delivery per-publisher Multiple Transport: TCP, TLS, Websockets Flexible: Arbitrary message up to 256 MB Topics can also be used for Key-Value storage  Topic based Pub/Sub  Decouples Publisher and Subscribers  Quality of Service  QoS 0 - \u0026ldquo;Fire and Forget\u0026rdquo; Q0S 1 - \u0026ldquo;At least once\u0026rdquo; QoS 2 - \u0026ldquo;Exactly once; 2 phase commit\u0026rdquo;  Ideal for intermittent connectivity; Sessions may last weeks or months Supports Disconnect \u0026amp; Last Will \u0026amp; Testament message\n MQTT supports Retained messages which are automatically delivered when a client subscribes to a topic.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-10-01-network-command-essentials/",
	"title": "Network Tools / Command Essentials",
	"tags": [],
	"description": "",
	"content": " Must Know Network Tools / Commands NetStat netstat stands for network statistics. This command displays incoming and outgoing network connections as well as other network information. The netstat utility can show you the open connections on your computer, which programs are making which connections, how much data is being transmitted, and other information.\n## List all connections netstat -a ## List TCP or UDP Connections netstat -at //TCP Connections netstat -au //UDP Connections ## List all Ports being Listened to netstat -an | grep \u0026quot;LISTEN \u0026quot;  IpTables ## Open 9001 Port sudo iptables -A INPUT -p tcp --dport 9001 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 9001 -m conntrack --ctstate ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp --dport 3306 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 3306 -m conntrack --ctstate ESTABLISHED -j ACCEPT  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-10-8-owasp/",
	"title": "OWASP Dependency Check",
	"tags": [],
	"description": "",
	"content": " OWASP dependency-check is an open source solution that can be used to scan Java and .NET applications to identify the use of known vulnerable components.\nLink\nAdding OWASP Check to Gradle Projects Adding following fragments to build.gradle\nbuildscript { repositories { mavenCentral() } dependencies { classpath 'org.owasp:dependency-check-gradle:5.2.2' } } plugins { id 'org.owasp.dependencycheck' version '5.2.2' }  Gradle Task\n./gradlew dependencyCheckAggregate  Configuring DependencyCheck dependencyCheck { format='ALL' cveValidForHours=1 outputDirectory = file(\u0026quot;$project.buildDir/reports/dependencycheck\u0026quot;) suppressionFile = 'config/dependencyCheck/suppressions.xml' failBuildOnCVSS = 5 failOnError = true }  Sample Suppressions.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;suppressions xmlns=\u0026quot;https://jeremylong.github.io/DependencyCheck/dependency-suppression.1.1.xsd\u0026quot;\u0026gt; \u0026lt;suppress\u0026gt; \u0026lt;notes\u0026gt;\u0026lt;![CDATA[file name: postgresql-42.2.5.jar]]\u0026gt;\u0026lt;/notes\u0026gt; \u0026lt;gav regex=\u0026quot;true\u0026quot;\u0026gt;^org\\.postgresql:postgresql:.*$\u0026lt;/gav\u0026gt; \u0026lt;cve\u0026gt;CVE-2016-7048\u0026lt;/cve\u0026gt; \u0026lt;/suppress\u0026gt; \u0026lt;/suppressions\u0026gt;  Resources  Dependency Check Gradle  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-10-3-openshift-cheatsheet/",
	"title": "OpenShift Cheatsheet",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-03-24-open-stack-cli/",
	"title": "OpenStack",
	"tags": [],
	"description": "",
	"content": " Getting Started with OpenStack What is OpenStack OpenStack is elastic cloud software that provides software developers with the ability to control the virtual infrastructure on which to deploy their applications. It is a set of software tools for building and managing cloud computing platforms for public and private clouds.\nIt accelerates time-to-market by dramatically reducing application provisioning times, giving companies full control of their software development lifecycle and ultimately giving them a significant competitive advantage. OpenStack also provides application portability by allowing enterprises to freely move between OpenStack clouds without vendor lock-in.\nIntroduction to OpenStack OpenStack lets users deploy virtual machines and other instances that handles different tasks for managing a cloud environment on the fly.\nKey Components of OpenStack  Nova - Primary Computing Engine behind OpenStack. It is used for deploying and managing large numbers of virtual machines and other instances to handle computing tasks.\n Swift - Storage System for Objects and Files.\n   Network Commands ### List floating IP Pools openstack ip floating pool list ### Get FloatingIP from the external Network openstack ip floating create \u0026lt;poolname\u0026gt; openstack ip floating create ext_vlan1767_net ### Create a Neutron Network neutron net-create ART-Land #### View list of Networks neutron net-list #### Create network for project neutron net-create \u0026lt;NetworkName\u0026gt; #### Creating Subnet neutron subnet-create --name ART-Subnet ART-Land 192.168.10.0/24 #### View Subnet neutron subnet show \u0026lt;subnetName\u0026gt; neutron subnet-show ART-Subnet ### Create Router neutron router-create \u0026lt;routername\u0026gt; neutron router-create ART-router ### Add Interface to router neutron router-interface-add \u0026lt;router\u0026gt; \u0026lt;subnet\u0026gt; neutron router-interface-add ART-router ART-Subnet ### Attach Router to External Network neuter router-gateway-set ART-router ext_vlan1767_net # Security Groups and Rules ### Creating Security Group openstack security group \u0026lt;groupname\u0026gt; --description 'Allow SSH and pings' openstack security group create BasicSG --description 'Allow SSH and Pings' ### Adding Rules to Security Groups #### Create Rule to allow SSH openstack security group rule create BasicSG --proto tcp --dst-port 22:22 #### Create Rule to allow Ping from any Source IP openstack security group rule create BasicSG --proto icmp --dst-port -1 ### Create Security Group to Open all TCP internal openstack security group create OpenSG --description 'All TCP internal' #### Add a new security rule to our OpenSG group that will allow all traffic from the private subnet in openstack security group rule create OpenSG --proto tcp --dst-port 1:65535 --src-ip 192.168.10.0/24  Creating Instances openstack server create --flavor smem-4vcpu --image ubuntu-latest --security-group BasicSG --nic net-id=ART-Land --key-name ArtKey ARTBastionHost  Working with Stacks ### Launching a Stacks openstack stack create --template \u0026lt;templateFile\u0026gt; --environment \u0026lt;envFile\u0026gt; \u0026lt;stackName\u0026gt; ### Listing Stacks openstack stack list openstack flavor list openstack image list  Neutron  Provides API to allow your users to create networks, subnets, routers etc.  Network - An isolated L2 segment, analogous to a VLAN in physical networking.\nSubnet - A block of v4 of v6 IP address and associated configuration state.\nPort - A connection point for attaching a single device (NIC) to Neutron network.\nopenstack network list # Create Network openstack network create mynetwork # Create Subnet openstack subnet create --network mynetwork \\ --subnet-range 10.0.0.0/29 --dns-nameserver 8.8.8.8 mynetwork-subnet  Nova Nova sits on top of Hypervisor Nova Flavors\n# List images openstack image list # List flavors openstack flavor list # List networks openstack network list # Boot an instance using flavor and image names (if names are unique) openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyFirstInstance # Boot another instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MySecondInstance # List instances, notice status of instance openstack server list # Show details of instance openstack server show MyFirstInstance # View console log of instance openstack console log show MyFirstInstance # Get the console url of the instance openstack console url show MyFirstInstance  Cinder Creates a volume on a Hypervisor. Allowing instances to attaching and detaching the volume without losing it\u0026rsquo;s persistence.\nCinder is powered by LVM and iSCSI. Cinder is Inspired by Elastic Block Storage.\nopenstack volume create --size 1 --type sata MyFirstVolume openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID  Use \u0026lsquo;myadmin\u0026rsquo; credentials source ~/credentials/myadmin\nVerify Cinder services are functioning and checking in :-) openstack volume service list  Use \u0026lsquo;myuser\u0026rsquo; credentials source ~/credentials/myuser  Create a new volume openstack volume create --size 1 --type sata MyFirstVolume  Boot an instance to attach volume to openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyVolumeInstance  List instances, notice status of instance openstack server list  List volumes, notice status of volume openstack volume list  Get volume and instance IDs MYFIRSTVOLUME_ID=`openstack volume show MyFirstVolume | awk '/ id / { print $4 }'` MYVOLUMEINSTANCE_ID=`openstack server show MyVolumeInstance | awk '/ id / { print $4 }'` # Attach volume to instance after instance is active, and volume is available openstack server add volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # Confirm the volume has been attached openstack volume list # Get the console url of the instance openstack console url show MyVolumeInstance # Login to the instance # username: cirros # password: cubswin:) # From inside the instance # List storage devices sudo fdisk -l # Make filesystem on volume sudo mkfs.ext3 /dev/vdb # Create a mountpoint sudo mkdir /extraspace # Mount volume at mountpoint sudo mount /dev/vdb /extraspace # Create a file on volume sudo touch /extraspace/helloworld.txt sudo ls /extraspace # Unmount volume sudo umount /extraspace # Log out of instance exit # Detach volume from instance openstack server remove volume $MYVOLUMEINSTANCE_ID $MYFIRSTVOLUME_ID # List volumes, notice status of volume openstack volume list # Delete instance openstack server delete MyVolumeInstance  # Use 'myuser' credentials source ~/credentials/myuser # Create a new instance openstack server create --flavor m1.tiny --image cirros --nic net-id=private MyLastInstance --wait # Get the console url of the instance openstack console url show MyLastInstance # Login to the instance # username: cirros # password: cubswin:) # Ping openstack.org ( should fail, as we have nothing routing between the tenant network (private) and provider network (public) ) ping openstack.org # Create a router openstack router create MyRouter # Connect the private-subnet to the router openstack router add subnet MyRouter private-subnet # List interfaces attached to router openstack port list --router MyRouter # Connect router to public network neutron router-gateway-set MyRouter public # Examine details of router openstack router show MyRouter # Get instance ID for MyLastInstance MYLASTINSTANCE_ID=`openstack server show MyLastInstance | awk '/ id / { print $4 }'` # Find port id for instance MYLASTINSTANCE_IP=`openstack server show MyLastInstance | awk '/ addresses / { print $4 }' | cut -d '=' -f 2` MYLASTINSTANCE_PORT_ID=`openstack port list --device-owner compute:None | awk ' /'$MYLASTINSTANCE_IP'/{print $2}'` # Create a floating IP and attach it to instance openstack floating ip create --port $MYLASTINSTANCE_PORT_ID public # Create a new security group openstack security group create remote # Add rules to security group to allow SSH and ping openstack security group rule create --proto icmp --src-ip 0.0.0.0/0 remote openstack security group rule create --proto tcp --dst-port 22 --src-ip 0.0.0.0/0 remote # Apply security group to instance with floating IP openstack server add security group MyLastInstance remote # Ping instance with floating IP ping 172.16.0.12 # Delete all your servers openstack server delete MyLastInstance  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-08-16-openshift/",
	"title": "Openshift",
	"tags": [],
	"description": "",
	"content": "  Architecture Containers and Image  Container Registries  Pods and Services  Pods Services Labels  Builds and Image Streams  Builds Image Stream Image stream tag Image stream image Image stream trigger Templates  References  Architecture OpenShift is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as acurately as possible, with a focus on easy composition of applications by a developer.\nOpenShift Container Platform (OCP) has a microservices based architecture of smaller, decoupled units that work together. It runs on top of Kubernetes cluster, with data about the objects stored in etcd, a realible key-value store.\n REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object.  Containers and Image The basic units of OpenShift Container Platform applications are called Containers. Linux container technologies are lightweight mechanism for isolating running processess so that they are limited to interacting with only their designated resources.\nContainers in OpenShift Container Platform are based on Docker formatted container Images. An image is a binary that includes all of the requirements for running a single container, as well as metadata describring its needs and capabilities.\nContainer Registries A container registry is a service for storing and retreving Docker formatted container images. A registry contains a collection of one or more image repositories. Each image repository contains one of more tagged images. Docker provides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a registry at registry.access.redhat.com for subscribers. OpenShift Container Platform can also supply its own internal registry for managing custom container images.\nPods and Services Pods OpenShift Enterprise leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.\nPods are the rough equivalent of a machine instance (physical or virtual) to a container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.\n# Get All Pods within a Namespace oc get pods # Get Pod information in YAML oc get pod hello-node-1-nz525 -o yaml # Get Pods Details using Label oc describe pods -l app=hello-node  Services A Kubernetes service serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Backing pods can be added to or removed from a service arbitrarily while the service remains consistently available, enabling anything that depends on the service to refer to it at a consistent internal address.\nServices are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. A service uses a label selector to find all the containers running that provide a certain network service on a certain port.\nLabels Labels are used to organize, group, or select API objects. For example, pods are \u0026ldquo;tagged\u0026rdquo; with labels, and then services use label selectors to identify the pods they proxy to. This makes it possible for services to reference groups of pods, even treating pods with potentially different containers as related entities.\nMost objects can include labels in their metadata. So labels can be used to group arbitrarily-related objects; for example, all of the pods, services, replication controllers, and deployment configurations of a particular application can be grouped.\nlabels: key1: value1 key2: value2  Builds and Image Streams Builds A build is the process of transforming input parameters into a resulting object. Most often, the process is used to transform input parameters or source code into a runnable image. A BuildConfig object is the definition of the entire build process.\nOpenShift Container Platform leverages Kubernetes by creating Docker-formatted containers from build images and pushing them to a container registry.\nThe OpenShift build system provides extensible support for build strategies that are based on selectable types specified in the build API. There are three build strategies available:\n Docker build Source-to-Image (S2I) build Custom build  By default, Docker builds and S2I builds are supported.\nImage Stream An OpenShift Container Platform object that contains pointers to any number of Docker-formatted container images identified by tags. You can think of an image stream as equivalent to a Docker repository.\nImage stream tag A named pointer to an image in an image stream. An image stream tag is similar to a Docker image tag. See Image Stream Tag below.\nImage stream image An image that allows you to retrieve a specific Docker image from a particular image stream where it is tagged. An image stream image is an API resource object that pulls together some metadata about a particular image SHA identifier. See Image Stream Images below.\nImage stream trigger A trigger that causes a specific action when an image stream tag changes. For example, importing can cause the value of the tag to change, which causes a trigger to fire when there are Deployments, Builds, or other resources listening for those. See Image Stream Triggers below.\n# Get OpenShift from Project Openshift oc get is -n openshift oc describe is jenkins -n openshift # Get Image Stream Definition in YAML oc get is jenkins -o yaml  Templates A template describes a set of objects that can be parameterized and processed to produce a list of objects for creation by OpenShift. A template can be processed to create anything you have permission to create within a project, for example services, build configurations, and deployment configurations. A template may also define a set of labels to apply to every object defined in the template.\nYou can create a list of objects from a template using the CLI or, if a template has been uploaded to your project or the global template library, using the web console.\n# Get All Templates from a project oc get templates -n openshift # Create Template from YAML file oc create -f \u0026lt;filename\u0026gt;  References  Core Concepts - Builds and Image Streams\n CLI Reference - Basic CLI Operation\n Core Concepts - Architecture\n Core Concepts - Builds and Image Streams\n  "
},
{
	"uri": "http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/",
	"title": "Orchestrating Cloud with Kubernetes",
	"tags": [],
	"description": "",
	"content": " # Creating Kubernetes Cluster gcloud container clusters create io  Quick Demo # Create Deployment kubectl create deployment nginx --image=nginx:1.10.0 # List Pods kubectl get pods # Expose Deployment via a Service using LoadBalancer kubectl expose deployment nginx --port 80 --type LoadBalancer # List Service kubectl get services  Pods Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods\n# Create Pod kubectl create -f pods/monolith.yaml # List Pods kubectl get pods # Describe Pod kubectl describe pods monolith # Port Forward Pod kubectl port-forward monolith 10080:80  Services An abstract way to expose an application running on a set of Pods as a network service.\nNo need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives pods their own IP addresses and a single DNS name for a set of pods, and can load-balance across them.\nPods aren\u0026rsquo;t meant to be persistent. They can be stopped or started for many reasons - like failed liveness or readiness checks - and this leads to a problem\n "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-11-26-setting-apache-virtual-host/",
	"title": "Setting Apache Virtual Host",
	"tags": [],
	"description": "",
	"content": " Install Apache WebServer (Pre-Req) sudo apt-get update sudo apt-get install apache2  Apache Virtual Host To run more than one site on a single machine, you need to setup virtual hosts for sites your plan to host on an apache server.\nName Based Virtual Hosts (Most Common)  The server relies on the client to report the hostname as part of the HTTP headers. Using this technique, many different hosts can share the same IP address.  IP Based Virtual Hosts  IP-based virtual hosting is a method to apply different directives based on the IP address and port a request is received on. Most commonly, this is used to serve different websites on different ports or interfaces.  Setting up Virtual Directories sudo mkdir -p /var/www/aayushtuladhar.com/public_html sudo chown -R $USER:$USER /var/www/aayushtuladhar.com/public_html  Creating Virtual Host Files sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/aayushtuladhar.com.conf \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin admin@aayushtuladhar.com ServerName aayushtuladhar.com ServerAlias aayushtuladhar.com *.aayushtuladhar.com DocumentRoot /var/www/aayshtuladhar.com/public_html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u0026lt;/VirtualHost\u0026gt;   ServerName - Base Domain that should match for the virtual host definition. ServerAlias - Lists names are other names which people can use to see that same web site.  Enable Virtual Host sudo a2ensite example.com.conf\nDisable Default Virtual Host sudo a2dissite 000-default.conf\nRestart Apache sudo service apache2 restart # Ubuntu 15.10 sudo systemctl restart apache2 # Ubuntu 14.10 and Earlier  Reference  http://www.unixmen.com/setup-apache-virtual-hosts-on-ubuntu-15-10/ https://httpd.apache.org/docs/2.4/vhosts/name-based.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-03-01-setting-fqdn/",
	"title": "Setting FQDN",
	"tags": [],
	"description": "",
	"content": "  FQDN  Finding FQDN  Hostname  Finding Hostname  Setting hostname and FQDN  FQDN FQDN stands for Fully Qualified Domain Name. It is a domain name that specifies its exact location in the tree hierarhcy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone.\nExample, somehost.example.com\nFinding FQDN hostname -f  Hostname A hostname is a label that is assigned to a device connected to a computer network and that is used to identify the device. Example, kafka-dev\nFinding Hostname  $ hostname AayushTuladhar-Mac.local  Setting hostname and FQDN  Update /etc/hosts\n127.0.0.1 packer-20140710152503.aayushtuladhar.com packer-20140710152503 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 \u0026lt;ip-addr\u0026gt; kafka-dev.aayushtuladhar.com kafka-dev  Update /etc/sysconfig/network\n[vagrant@kafka-dev ~]$ cat /etc/sysconfig/network NETWORKING=yes HOSTNAME=kafka-dev.aayushtuladhar.com NOZEROCONF=yes  Restart the Hostname Service\n/etc/init.d/network restart  Verify Hostname and FQDN\nhostname hostname -f   "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-02-05-setting-selenium-grid/",
	"title": "Setting Selenium Grid",
	"tags": [],
	"description": "",
	"content": " Selenium Grid enables you to spread your tests across multiple machines and multiple browsers, which allows your to run tests in parallel. Also having Hub as central point of communication handles all the driver configuration and runs them automatically.\nDownloading Selenium http://docs.seleniumhq.org/download/\nSetting up Hub Once you have the jarfile downloaded from the Selenium Website,\njava -jar selenium-server-standalone-2.x.x.jar –role hub\nThis starts up a jetty server on default port 4444. Grid Console can be viewed at http://localhost:4444/grid/console\nSetting up Nodes Nodes are the actual machine which performs the tests,\njava –jar selenium-server-standalone-2.x.x.jar –role node –hub http://hubIP:4444/grid/register\nIE, Chrome, Safari \u0026amp; firefox selenium NODE java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot; -browser \u0026quot;browserName=internet explorer, version=11, platform=WINDOWS, maxInstances=10\u0026quot; -browser \u0026quot;browserName=chrome,version=ANY,maxInstances=10,platform=WINDOWS\u0026quot;  IE Node Setup java -Dwebdriver.ie.driver=C:/eclipse/IEDriverServer/IEDriverServer.exe -jar selenium-server-standalone-2.48.2.jar -port 5555 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=internet explorer,version=11,platform=WINDOWS,maxInstances=10\u0026quot;  Chrome Node Setup java -Dwebdriver.chrome.driver=C:/eclipse/chromedriver/chromedriver.exe -jar selenium-server-standalone-2.48.2.jar -port 5556 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=chrome, version=ANY, maxInstances=10, platform=WINDOWS\u0026quot;  FireFox Node Setup java -jar selenium-server-standalone-2.48.2.jar -port 5557 -role node -hub http://localhost:4444/grid/register -browser \u0026quot;browserName=firefox, maxInstances=10, platform=ANY, seleniumProtocol=WebDriver\u0026quot;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/bigdata/2018-01-02-setting-up-storm-in-5-minutes/",
	"title": "Setting up Apache Storm",
	"tags": [],
	"description": "",
	"content": "Apache Storm is free and open source distributed real-time computation system. Storm provides reliable real-time data processing what Hadoop did for batch processing. It provides real-time, robust, user friendly, reliable data processing capability with operational Intelligence.\nThis post is more about setting up your Storm Environment from ground up in less than 5 Minutes. Yes, you heard it right less than 5 Minutes. Without any delay, let\u0026rsquo;s get it running.\n# Installing Zookeeper and Storm brew install zookeeper brew install storm  # Starting Zookeeper brew services start zookeeper # Starting Storm cd /usr/local/bin/ ./storm nimbus ./storm supervisor ./storm ui  Here you go, Storm Cluster is up and you are ready to deploy your Storm Topology\nhttp://localhost:8080/index.html  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-09-17-setting-up-jekyll-website/",
	"title": "Setting up Jekyll Website",
	"tags": [],
	"description": "",
	"content": " Pre-Requires  Ruby  # Verify Ruby is Installed ruby --version  Installation gem install jekyll bundler  Create New Site jekyll new myblog  Run Blog Locally bundle exec jekyll serve  Reference  https://github.com/arttuladhar/my-jekyll-blog  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/jenkins_setup_terraform/",
	"title": "Setting up Jenkins using Terraform",
	"tags": [],
	"description": "",
	"content": " Building a Custom Jenkins Image Create Dockerfile with contents:\nFROM jenkins/jenkins:lts USER root RUN apt-get update -y \u0026amp;\u0026amp; apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common RUN curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;)/gpg \u0026gt; /tmp/dkey; apt-key add /tmp/dkey RUN add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;) $(lsb_release -cs) stable\u0026quot; RUN apt-get update -y RUN apt-get install -y docker-ce docker-ce-cli containerd.io RUN curl -O https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip \u0026amp;\u0026amp; unzip terraform_0.11.13_linux_amd64.zip -d /usr/local/bin/ USER ${user}  Build the Image\ndocker build -t jenkins:terraform .  List the Docker Images:\ndocker image ls  Setting up Jenkins Create main module for Terraform, main.tf\n# Jenkins Volume resource \u0026quot;docker_volume\u0026quot; \u0026quot;jenkins_volume\u0026quot; { name = \u0026quot;jenkins_data\u0026quot; } # Start the Jenkins Container resource \u0026quot;docker_container\u0026quot; \u0026quot;jenkins_container\u0026quot; { name = \u0026quot;jenkins\u0026quot; image = \u0026quot;jenkins:terraform\u0026quot; ports { internal = \u0026quot;8080\u0026quot; external = \u0026quot;8080\u0026quot; } volumes { volume_name = \u0026quot;${docker_volume.jenkins_volume.name}\u0026quot; container_path = \u0026quot;/var/jenkins_home\u0026quot; } volumes { host_path = \u0026quot;/var/run/docker.sock\u0026quot; container_path = \u0026quot;/var/run/docker.sock\u0026quot; } }  Initialize Terraform\nterraform init  Plan the deployment\nterraform plan -out=tfplan  Deploy Jenkins\nterraform apply tfplan  Get the Admin password\ndocker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword  Login using Public IP with Port 8080\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_kubernetes/",
	"title": "Setting up Kubernetes and Terraform",
	"tags": [],
	"description": "",
	"content": "Create kube-config.yml\napiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration networking: podSubnet: 10.244.0.0/16 apiServer: extraArgs: service-node-port-range: 8000-31274  Initialize Kubernetes\nsudo kubeadm init --config kube-config.yml  Copy admin.conf to your home directory\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  Install Flannel:\nsudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  Untaint Kubernetes Master\nkubectl taint nodes --all node-role.kubernetes.io/master-  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_docker/",
	"title": "Setting up Terraform With Docker",
	"tags": [],
	"description": "",
	"content": " Installing Docker on the Swarm Manager and Worker # Update the Operating System sudo yum update -y # Uninstall Old Versions sudo yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine # Install Docker CE sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # Add Docker Repository sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce # Start Docker and Enable sudo systemctl start docker \u0026amp;\u0026amp; sudo systemctl enable docker # Add `cloud_user` to the `docker` group sudo usermod -aG docker cloud_user docker --version # Configure Swarm Manager Node docker swarm init --advertise-addr [PRIVATE_IP] On the worker node, add the worker to the cluster: docker swarm join --token [TOKEN] [PRIVATE_IP]:2377 # Verify Swarm cluster docker node ls  Installing Terraform # Install Terraform 0.11.13 on the Swarm manager sudo curl -O https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip sudo yum install -y unzip sudo unzip terraform_0.11.13_linux_amd64.zip -d /usr/local/bin/ # Test the Terraform installation terraform version  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2018-09-18-spring-cloud-slueth/",
	"title": "Spring Cloud Slueth",
	"tags": [],
	"description": "",
	"content": " A powerful tool for enhancing logs in any application, but especially in a system built up of multiple services. This is where spring-cloud-starter-sleuth comes into play to help you enhance your logging and traceability across multiple systems. Just including the spring-cloud-starter-sluth in your project.\nFew concepts you need to be familiar with when using Spring Cloud Slueth are concepts of Trace and Spans. Trace can though as single request or job that is triggered in an application. All the various steps in the request, even across application and thread boundaries will have the same traceId. Whereas, Spans can be though of a section of a job request. A single trace can be composed of multiple spans each correlating to a specific step or section of the request.\n[application name, traceId, spanId, export]\n         Application Name Name of the Application, we set in the properties file   traceId Request Id to Single Request   spanId Track Unit of Work   export Indicates whether or not the log was exported to an aggregator like Zipkin    Resources Spring Cloud Sleuth - Baeldung\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-04-10-setting-up-supervisord/",
	"title": "Supervisor",
	"tags": [],
	"description": "",
	"content": "  Installation Configuration Running Supervisor  Supervisor Configuration Structure  Controller Processes  Reread Configuration and Reload It Controlling Tool Start / Stop Processess   It\u0026rsquo;s been a while I have been using Supervisor to run my application. It\u0026rsquo;s a great little tool for running and monitoring processes on UNIX-like operating systems. It provides you simple simple, centralized interface to all your applications running on a box. Using Web Interface, you can see health and logs of the applications without even logging in in the box.\nInstallation sudo apt-get install -y supervisor  Configuration /etc/supervisor/supervisor.conf\n[include] files = /etc/supervisor/conf.d/*.conf # Enable Web Interface [inet_http_server] port = 9001 username = user # Basic auth username password = pass # Basic auth password  Syntax\n[program:nodehook] - Define the program to monitor. We'll call it \u0026quot;nodehook\u0026quot;. command - This is the command to run that kicks off the monitored process. We use \u0026quot;node\u0026quot; and run the \u0026quot;http.js\u0026quot; file. If you needed to pass any command line arguments or other data, you could do so here. directory - Set a directory for Supervisord to \u0026quot;cd\u0026quot; into for before running the process, useful for cases where the process assumes a directory structure relative to the location of the executed script. autostart - Setting this \u0026quot;true\u0026quot; means the process will start when Supervisord starts (essentially on system boot). autorestart - If this is \u0026quot;true\u0026quot;, the program will be restarted if it exits unexpectedly. startretries - The number of retries to do before the process is considered \u0026quot;failed\u0026quot; stderr_logfile - The file to write any errors output. stdout_logfile - The file to write any regular output. user - The user the process is run as. environment - Environment variables to pass to the process.  Example Configuration\n/etc/supervisor/conf.d/node-chada-chutkila.conf\n[program:node-chadachutkila] command=node /home/art/apps/chada-chutkila/app.js directory=/home/art autostart=true autorestart=true startretries=3 stderr_logfile=/var/log/art/chada-chutkila.err.log stdout_logfile=/var/log/art/chada-chutkila.out.log user=art environment=SECRET_PASSPHRASE='this is secret',SECRET_TWO='another secret'  Running Supervisor sudo service supervisor start\nSupervisor Configuration Structure supervisor/ ├── conf.d │ └── digi-marketplace-node.conf └── supervisord.conf  Controller Processes Reread Configuration and Reload It supervisorctl reread supervisorctl update  Controlling Tool supervisorctl\nStart / Stop Processess supervisorctl start \u0026lt;processName\u0026gt; supervisorctl stop \u0026lt;processName\u0026gt;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/javascript/template-literals/",
	"title": "Teplate Literals",
	"tags": [],
	"description": "",
	"content": "Template literals are string literals allowing embedded expressions. You can use multi-line strings and string interpolation features with them.\nconst someText = `string text ${expression} string text`  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": " Introduction Terraform is the infrastructure as code offering from HashiCorp. It is a tool for building, changing, and managing infrastructure in a safe, repeatable way.\nUsing HCL as High Level Language, Terraform support Infrastrucre as Code by automating creation of those resources. Terraform provides support for desired resources on almost any provider (AWS, GCP, GitHub, Docker etc)\nFeature of Terraform Infrastructue as Code  Idempotent Uses High Level Language (HCL) Code Reusability using Modules  Execution Plan  Show the intent of the deploy Can help ensure everything in the development is intentional  Resource Graph  Illustrates all changes and dependencies  Use Cases for Terraform  Hybrid Cloud Multi-tier Architecture Software Defined Networking  "
},
{
	"uri": "http://learn.aayushtuladhar.com/terraform/terraform_state/",
	"title": "Terraform State",
	"tags": [],
	"description": "",
	"content": "Create a S3 Bucket in AWS that we will be using to store the Remote State.\nSet the Environment Variables:\nexport AWS_ACCESS_KEY_ID=\u0026quot;[ACCESS_KEY]\u0026quot; export AWS_SECRET_ACCESS_KEY=\u0026quot;[SECRET_KEY]]\u0026quot; export AWS_DEFAULT_REGION=\u0026quot;us-east-1\u0026quot;  Add the Remote Backend Configuration\nterraform { backend \u0026quot;s3\u0026quot; { key = \u0026quot;terraform-aws/terraform.tfstate\u0026quot; } }  Initialize Terraform\nterraform init -backend-config \u0026quot;bucket=[BUCKET_NAME]\u0026quot;  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2019-01-23-nginx/",
	"title": "The Twelve Factor App",
	"tags": [],
	"description": "",
	"content": " 12factor.net - Link Introduction Methodology for building software-as-a-service app that:\n Use Declarative formats for setup automation, to minimize time and cost for new developers joining the project. Have a Clean Contract with the underlying operation system, offering Maximum Portability between execution environments Are suitable for Deployment on modern Cloud platforms, obviating the need for servers and system administrators Minimize divergence between deployment and production, enabling Continuous deployment for maximum agility And can Scale up without significant changes to tooling, architecture, or development practices.  The Twelve Factors 1. Codebase  One codebase tracked in revision control, many deploys\n  Multiple apps sharing the same code is a violation of twelve-factor. The solution here is to factor shared code into libraries which can be included through the dependency manager  2. Dependencies  Explicity declare and isolate dependencies\n  A twelve-factor app never relies on implicit existence of system-wide packages. It declares all dependencies, completely and exactly via a dependency declaration manifest It uses dependency isolation tool during execution to ensure that no implicit dependency specification is applied uniformly to both production and development Another benefits of explicitly declaring dependency is that it allows new developers to quickly run application locally.  3. Config  Store Config in the environment\n  Apps storing config as constants in the code is violation of twelve-factor, which requires strict separation of config from code.  4. Backing Services  Treat backing services as attached resources\n  A backing service is any service that app consumes over the network as part of its normal operation. Eg, Datastore (MySQL, MongoDB), Messaging (RabbitMQ), SMTP Services and Caching System The code for twelve-factor app makes no distinction between local and third party services. To the app, both are attached resources, accessed via a URL or other locator/credentials stored in the config Each distinct backing service is a resource. The twelve-factor app treats these databases as attached resources, which indicate their loose coupling to the deploy they are attached to.   \n5. Build, Release, Run  Strictly separate build and run stages\n  The Build Stage is a tranform which converts a code repo into an executable bundle known as build. Using a version of the code at a commit specified by the development process, the build stage fetches vendors dependencies and compiles binaries and assets The Release Stage takes the build produced by the build stage and combines it with the deploy\u0026rsquo;s current config. The resulting relrease contains both the build and the config and is ready for immediate execution in the execution environment. The Run Stage runs the application in the execution environment, by launching some set of the app\u0026rsquo;s process against a selected release.   Every release should always have a unique release ID Release cannot be mutated once its created. Any change must create a new release  6. Processes  Execute the app as one or more stateless Processes\n  Twelve-factor process are stateless and share-nothing Sticy sessions are a violation of twelve-factor; Session state data is a good candidate for a datastore that offers time-expiration, such as Memcached  7. Port Binding  Export services via port Binding\n  The twelve-factor app is completely self-contained and does not rely on runtime injection of a webserver into the execution environment to create a web-facing services. The web app exports HTTP as a service by binding to a port, and listening to requests coming in on that port. Note also that the port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app.  8. Concurrency  Scale out via the process model\n  In the twelve-factor app, processess are a first class citizen. The process model truly shines when it comes time to scale out. The share-nothing, horizontal partionable nature of twelve-factor app process means that adding more concurrency is a simple and reliable operation.  9. Disposability  Maximize robustness with fast startup and graceful shutdown\n  The twelve-factor app\u0026rsquo;s processess are disposable, meaning they can be started or stopped at a moment\u0026rsquo;s notice.  10. Dev / Prod Parity  Keep development, staging and production as similar as possible\n  The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small. The twelve-factor developers resists the urge to use different backing services between development and production, even when adapters theoritically abstract away differences in backing services.  11. Logs  Treat logs as event streams\n  Twelve-factor app never concerns itself with routing or storage of its output stream It should not attempt to write to or manage logfiles, instead each running process writes its event stream, unbuffered, to stout  12. Admin Process  Run admin / management tasks as one-off processes\n  Twelve-factor strongly favors languages which provide a REPL shell out of the box, and which make it easy to run one-off scripts. In a local deploy, developers invoke one-off admin processes by a direct shell command inside the app’s checkout directory. In a production deploy, developers can use ssh or other remote command execution mechanism provided by that deploy’s execution environment to run such a process.  "
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2016-07-07-ssh-keys/",
	"title": "Understanding SSH Keys",
	"tags": [],
	"description": "",
	"content": "  \nSSH is the most common way of connecting to remove Linux Server. It stands for Secure Shell. It provide a safe and secure way of executing commands, making changes and configuring service remotely. SSH connecting is implemented using client-server model. For a client machine to connect to a remote machine using SSH, SSH daemon must be running on the remote machine.\nClients generally authenticate either using passwords or by SSH Keys. SSH keys provides a more secure way of logging into a virtual private server using SSH. SSH keys are nearly impossible to decipher by brute force alone.\nSSH keys are a matching set of cryptographic keys. Each set contains a public and private key. To authenticate using SSH keys, you place the public key on any server and then unlock it by connecting with the private key.\nGenerating SSH Key Pair ssh-keygen -t rsa\nCopy Public Key ssh-copy-id user@123.45.67.89\nAlternatively,\ncat ~/.ssh/id_rsa.pub | ssh user@123.45.56.78 \u0026quot;mkdir -p ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot;\nNote - Generally when you spin up a server, you can embed your public key in your new server.\nGenerate Public Key from Private Key Option -y outputs the public key\nssh-keygen -y -f private-key \u0026gt; public-key\n# Example ssh-keygen -y -f pin-ost.pem \u0026gt; art.pub  Reference https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys\n"
},
{
	"uri": "http://learn.aayushtuladhar.com/devops/2017-03-15-using-apache-server-benchmarking/",
	"title": "Using Apache Server Benchmarking",
	"tags": [],
	"description": "",
	"content": " Apache Benchmark is a single-threaded command line tool for measuring the performance of a HTTP web server. It gives you an impression of how many requests per second your server is capable of serving.\nInstallation sudo apt-get install apache2-utils.  Usage -p POST Message -H Message Header -T Content Type -c Concurrent Clients -n Number of Requests to Run in the Test  GET REQUEST $ ab -n200 -c100 -H \u0026quot;APP-TOKEN: Q977quNeXjFsNjLNlmC9MK1HuRP+fFKmwDX9KSD6Y=\u0026quot; \\ http://test-api.aayushtuladhar.com:8080/test/start_page=0  POST REQUEST $ ab -p body.txt -n200 -c100 -T application/json \\ https://test-api.aayushtuladhar.com:8080/anotherTest  "
},
{
	"uri": "http://learn.aayushtuladhar.com/hugo-basics/",
	"title": "Using Hugo",
	"tags": [],
	"description": "",
	"content": " All the Hugo Basics "
},
{
	"uri": "http://learn.aayushtuladhar.com/data-analytics/2018-05-01-pandas/",
	"title": "Using Pandas",
	"tags": [],
	"description": "",
	"content": " Explore, Visualize, and Predict using Pandas \u0026amp; Jupyter  Explore, Visualize, and Predict using Pandas \u0026amp; Jupyter Pandas Intro Load Data Inspecting Data Summarize Data Reference  Setup\n%matplotlib inline import pandas as pd import matplotlib import numpy as np  pd.__version__, matplotlib.__version__, np.__version__  Pandas Intro The pandas library is very popular among data scientists, quants, Excel junkies, and Python developers because it allows you to perform data ingestion, exporting, transformation, and visualization with ease. But if you are only familiar with Python, pandas may present some challenges. Since pandas is inspired by Numpy, its syntax conventions can be confusing to Python developers.\nPandas has two main datatypes: a Series and a DataFrame\n A Series is like a column from a spreadsheet  s = pd.Series([0, 4, 6, 7]) temps = [30, 40, 60, 90] temp_series = pd.Series(temps) temp_series.sum() # Boolean Arrays hot = pd.Series([False, False, True, True]) temp_series[hot] # Masking mask1 = temp_series \u0026gt; 30 mask2 = temp_series \u0026lt; 60 result = temp_series[mask1 \u0026amp; mask2] dates = pd.date_range('20160101', periods=4) temp3 = pd.Series(temps, index=dates)  A DataFrame is like a spreadsheet\ndf = pd.DataFrame({'name': ['Fred', 'Johh', 'Joe', 'Abe'], 'age': s})  Load Data %ls data/ nyc = pd.read_csv('data/central-park-raw.csv', parse_dates=[0])  Inspecting Data # Columns for DataFrame nyc.columns # Get Datatypes for Columns nyc.dtypes # DataFrame Info nyc.info # Show Only First Bit nyc.head(10)  Summarize Data  df['w'].value_counts() - Count number of rows with unique value of variable df.describe()  Reference  https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf  "
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/bigdata/",
	"title": "bigdata",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/bigdata/",
	"title": "bigdata",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/categories/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://learn.aayushtuladhar.com/tags/hive/",
	"title": "hive",
	"tags": [],
	"description": "",
	"content": ""
}]