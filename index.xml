<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning Notebook on </title>
    <link>http://learn.aayushtuladhar.com/</link>
    <description>Recent content in Learning Notebook on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="http://learn.aayushtuladhar.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>00 - Containers and Orchestration</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_containers_and_orchestration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_containers_and_orchestration/</guid>
      <description>Containers Microservices Container Orchestration  Container Orchestrators   Containers Containers are application-centric methods to deliver high-performing, scalable applications on any infrastructure of your choice. Containers are best suited to deliver microservices by providing portable, isolated virtual environments for applications to run without interference from other running applications.
Microservices Microservices are lightweight applications written in various modern programming languages, with specific dependencies, libraries and environmental requirements. To ensure that an application has everything it needs to run successfully it is packaged together with its dependencies.</description>
    </item>
    
    <item>
      <title>00 - Terraform Introduction</title>
      <link>http://learn.aayushtuladhar.com/terraform/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/introduction/</guid>
      <description> Introduction Terraform is the infrastructure as code offering from HashiCorp. It is a tool for building, changing, and managing infrastructure in a safe, repeatable way.
Using HCL as High Level Language, Terraform support Infrastrucre as Code by automating creation of those resources. Terraform provides support for desired resources on almost any provider (AWS, GCP, GitHub, Docker etc)
Feature of Terraform Infrastructue as Code  Idempotent Uses High Level Language (HCL) Code Reusability using Modules  Execution Plan  Show the intent of the deploy Can help ensure everything in the development is intentional  Resource Graph  Illustrates all changes and dependencies  Use Cases for Terraform  Hybrid Cloud Multi-tier Architecture Software Defined Networking  </description>
    </item>
    
    <item>
      <title>01 - AWS Compute Services</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/01-compute/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/01-compute/</guid>
      <description>Elastic Cloud Compute (EC2)  EC2 is a Cloud Computing Service. It is responsible for providing long-running compute as service. Configures your EC2 by choosing your OS, Storage, Memory, Network Throughput EC2 comes in variety Instance types of specialization for different roles [EC2 Families]  General Purpose - Balance of Compute, Memory and Networking resources Compute Optimized - Ideal for compute based application that benefit from high performance processor Memory Optimized - Ideal for fast performance workloads processing large data sets in memory Storage Optimized - Ideal for high sequential, read and write access to very large dataset on local storage Accelerated Optimized - Hardware accelerators, or, co-processors  Instance sizes include Nano, Small, Medium, Large, X.</description>
    </item>
    
    <item>
      <title>01 - Architecture 101</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/01-architecture-101/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/01-architecture-101/</guid>
      <description>Access Management Basics Shared Reponsibility Model Service Models Availability and Fault Tolerance Scalling Tiered Application Design Encryption  Access Management Basics Principal - A person or application that can make an authenticated or anonymous request to perform an action on a system.
Authentication - The process of authenticating a principal against an identity. This could be via username and passsword or API Keys.
Identity - Objects that require authentication and are authorized to access resources.</description>
    </item>
    
    <item>
      <title>01 - HCL Basics</title>
      <link>http://learn.aayushtuladhar.com/terraform/hcl_basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/hcl_basics/</guid>
      <description>Terraform CLI Commands Terraform Syntax Resources Console and Outputs Variables  Passing Variable  DataSources Terraform Workspaces NullResources and Local-exec  Terraform CLI Commands    Command Description     init Initializes a new or existing Terraform configuration   validate Validates the Terraform files   plan Generates ans shows an execution plan   apply Builds or Change Infrastructure   output Reads an output from a state file   show Inspects Terraform State or Plan   providers Print a tree of the providers using the configuration   destory Destorys Terraform-managed infrastructure    Terraform Syntax  Single line comments start with # Multi line comments are wrapped with /* and */ Values are assigned with the syntax key=value Strings are double quoted Strings can interpolate other values using the syntax ${}  Resources Resosurces are the Objects manged by Terraform such as VM or S3 Buckets.</description>
    </item>
    
    <item>
      <title>01 - Kubernetes Architecture</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/01_kubernetes-architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/01_kubernetes-architecture/</guid>
      <description>Master Node  Master Node Components Master Node Components: API Server Master Node Components: Scheduler Master Node Componets: Controller Managers Master Node Components: etcd  Worker Node  Worker Node Components Worker Node Component: Container Runtime Worker Node Components: kubelet Worker Node Components: kube-proxy Worker Node Components: Addons  Networking Challenges  Container to Container Communication inside Pods Pod-to-Pod Communication Across Nodes Pod-to-External World Communication   At a very high level, Kubernetes has the following main components</description>
    </item>
    
    <item>
      <title>01 - Setting up Kubernetes Cluster</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/01_setting_up_k8_cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/01_setting_up_k8_cluster/</guid>
      <description>Using Ubuntu Distribution (Ubuntu Xenial LTS 16.04) as Base Image for the Virtual Machine. We will be building a Kubernetes Cluster
Setup Docker and Kubernetes Repositories curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository &amp;quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&amp;quot; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat &amp;lt;&amp;lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF  Install Docker, Kubelet, KubeAdm and KubeCtl sudo apt-get update sudo apt-get install -y docker-ce=18.</description>
    </item>
    
    <item>
      <title>02 - AWS Fundamentals</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/02-aws-fundamentals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/02-aws-fundamentals/</guid>
      <description>AWS Accounts  Authentication Authorization Billing  AWS Physical and Networking Layer Well-Architected Framework  Pillars of Well Architected Framework Security Reliability Performance Efficiency Operational Excellence Cost Optimization  Elasticity Introduction to S3 - (Simple Storage Service) Introduction to Cloud Formation  AWS Accounts AWS accounts are more than just a way to log in and access AWS services â€” they are a crucial AWS feature that AWS solutions architects can use to implement secure and high-performance systems.</description>
    </item>
    
    <item>
      <title>02 - Core Concepts</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/02_core_concepts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/02_core_concepts/</guid>
      <description>Kubernetes API Primitives Pods Namespaces Basic Container Configuration  Kubernetes API Primitives Kubernetes API Primitives are also called Kubernetes Objects. These are data objects that represent the state of the cluster. Example of Kubernetes Objects:
 Pod Node Service Service Account  The kubectl api-resource command will list the object types currently available to the cluster.
Every object has a spec and status:
 Spec - You provide the spec.</description>
    </item>
    
    <item>
      <title>02 - Installing Kubernetes</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/02_installing_kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/02_installing_kubernetes/</guid>
      <description>Local Installation On-Premise Installation Cloud Installation  All-in-One Single-Node Installation In this setup, all the master and worker components are installed and running on a single-node. While it is useful for learning, development, and testing, and it should not be used in production. Minikube is one such example, and we are going to explore it in future chapters.
Single-Node etcd, Single-Master and Multi-Worker Installation In this setup, we have a single-master node, which also runs a single-node etcd instance.</description>
    </item>
    
    <item>
      <title>02 - Terraform Modules</title>
      <link>http://learn.aayushtuladhar.com/terraform/modules/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/modules/</guid>
      <description>Module is a container for multiple resources that are going to be used together.
Main goal of module is logical grouping of resources to it&amp;rsquo;s cohesive unit that can be reused and shared across different systems. Modules can also be shared across multiple teams or via public registry such as GitHub or Terraform Cloud registry.
Using Terraform Modules # Download the image module &amp;quot;image&amp;quot; { source = &amp;quot;./image&amp;quot; image_name = &amp;quot;${var.</description>
    </item>
    
    <item>
      <title>02 - VPC Networking</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/02-networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/02-networking/</guid>
      <description>Virtual Private Cloud (VPC)  A private network within AWS. It lets you provision a logically isolated section of AWS cloud where you can launch AWS resources in a virtual network you define. Can be configured to be public/private or a mixture Regional Service (canâ€™t span regions), highly available, and can be connected to your data center and corporate networks Isolated from other VPCs by default VPC and subnet: Max /16 (65,536 IPs) and minimum /28 (16 IPs) VPC subnet cannot span AZs (1:1 Mapping) Certain IPs are reserved in subnets By default you can create up to 5 VPC per region Default VPC  Required for some AWS services Pre-configured with all required network / security configurations A /20 Public subnet in each AZ, allocating a public P by default Attached internet gateway with a &amp;ldquo;main&amp;rdquo; route table sending all IPv4 traffic to the internet gateway using a 0.</description>
    </item>
    
    <item>
      <title>03 - AWS Storage</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/03-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/03-storage/</guid>
      <description>Amazon S3  Simple Storage Service (S3) is object based Storage Service. Stores unlimited amount of data without worrying about underlying infrastructure S3 replicates data across at least 3 AZ&amp;rsquo;s to ensure 99.99% Availability and 99.99% (11 9&amp;rsquo;s Durability) Objects can be from 0 Bytes up to 5 TB (Multipart Upload). Single PUT upload support up to 5 GB For filesize greater than 100 MB, Multipart Uploads are recommended S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.</description>
    </item>
    
    <item>
      <title>03 - Configuration</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/03_configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/03_configuration/</guid>
      <description>ConfigMaps SecurityContexts Resource Requirements Secrets Service Accounts   ConfigMaps A ConfigMap is a Kubernetes Object that stores configuration data in a key-value format. This configuration data can then be used to configure software running in a container, by referencing the ConfigMap in the Pod spec.
myconfigmap.yml
apiVersion: v1 kind: ConfigMap metadata: name: my-config-map data: myKey: myValue anotherKey: anotherValue  Passing ConfigMap data to a pod&amp;rsquo;s container as an environment variable:</description>
    </item>
    
    <item>
      <title>03 - Identity and Access Control</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/03-identity_access_control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/03-identity_access_control/</guid>
      <description>Identity and Access Control  IAM Essentials IAM Policies IAM Users IAM Groups IAM Access Keys  Multi-Account Management and Organizations  AWS Organizations Role Switching Between Accounts   Identity and Access Control IAM Essentials Identity and Access Management, known as IAM, is one of the key services within AWS. It controls access to the AWS API endpoints that are used by the console UI, command line tools, and any applications wanting to utilize AWS.</description>
    </item>
    
    <item>
      <title>03 - Minikube</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/03_minikube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/03_minikube/</guid>
      <description>Installing Minikube  Command Line Interface (CLI) tools and scripts Web-based User Interface (Web UI) from a web browser APIs from CLI or programmatically Kubectl Proxy   Installing Minikube # Install Minikube brew install minikube # Starting Minikube minikube start minikube start --vm-driver=xhyve minikube start --vm-driver=hyperkit minikube status minikube stop  Any healthy running Kubernetes cluster can be accessed via any one of the following methods:
Command Line Interface (CLI) tools and scripts kubectl is the Kubernetes Command Line Interface (CLI) client to manage cluster resources and applications.</description>
    </item>
    
    <item>
      <title>04 (A) - Compute Services - Server Based</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_1-compute_server_based/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_1-compute_server_based/</guid>
      <description>Elastic Cloud Compute (EC2) Elastic Block Storage (EBS)  Exam Facts General Purpose (gp2): (SSD) Provisioned IOPS SSD (io1): (SSD) Throughput Optimized(st1): (HHD) Cold HDD (sc1): HDD  EBS Snapshots Security Groups Instance Metadata AMI Bootstrap Private Instance and Public Instance Advanced Topics  EC2 Instance Roles EBS Volume and Snapshot Encryption EBS Optimized, Enhanced Networking, and Placement Group EBS optimization Enhanced networking  Cluster, partition, and spread placement groups  EC2 Billing Models - Spot Instances EC2 Billing Models - Reserved Instances Dedicated Hosts   Elastic Cloud Compute (EC2) EC2 is one of the most widely used services within AWS.</description>
    </item>
    
    <item>
      <title>04 (B) - Compute Services - Serverless</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_2-compute_serverless/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/04_2-compute_serverless/</guid>
      <description>Serverless Compute  Lambda Essentials API Gateway Step Functions  Container Based Compute  ECS   Serverless Compute Serverless architecture consists of two main principles, including BaaS (Backend as a Service), which means using third party services where possible rather than running your own. Example include Auth0 or Cognito for authentication and Firebase or DynamoDb for data storage. Servless architecture uses event driven architecture using FaaS (Function as a Service) products to provide application logic.</description>
    </item>
    
    <item>
      <title>04 - Kubernetes Building Blocks</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/04_kubernetes_building_blocks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/04_kubernetes_building_blocks/</guid>
      <description>Kubernetes Object Model Pods  Labels  Replication Controller Replica Set Deployments Namespaces  Kubernetes Object Model With each object, we declare our intent or the desired state under the spec section. When creating an object, the object&amp;rsquo;s configuration data section from below the spec field has to be submitted to the Kubernetes API server.
Example of Deployment object configuration in YAML format.
apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.</description>
    </item>
    
    <item>
      <title>04 - Multi Container Pods</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/04_multi_container_pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/04_multi_container_pods/</guid>
      <description>Multi-container pods are simply pods with more than one container that all work together as a single unit.
It is often a good idea to keep containers separate by keeping them in their own separate pods, but there are several cases where multi-container pods can be beneficial.
You can create multi-container pods by listing multiple containers in the pod definition.
apiVersion: v1 kind: Pod metadata: name: multi-container-pod spec: containers: - name: nginx image: nginx:1.</description>
    </item>
    
    <item>
      <title>04 - Network and Content Delivery</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/04-network-content-delivery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/04-network-content-delivery/</guid>
      <description>Amazon CloudFront  CloudFront is a CDN (Content Delivery Network). It makes website load faster by serving cached content Benefits of using CloudFront includes  Lower Latency Higher Transfer Speed Reduced load on Content Server  Origin is the address of where the original copies of your files reside eg. S3, EC2, ELB Distribution defines a collection of Edge locations and behaviors on how it should handle your cached content Distribution has 2 types: Web Distribution (static website content) and RTMP (streaming media) Edge Locations are local infrastructure that hosts cache of data Origin Identity Access (OAI) is used to access private S3 buckets, restricting S3 bucket access only via Cloud Front Access to cached content can by protected via Signed Urls or Signed Cookies Lambda@Edge allows you to pass each request through a Lambda to change the behavior of the response  Amazon Route 53  Route53 is a DNS provider, register and manage domains, create record sets and health check of resources Simple Routing (Default) - A simple routing policy is a single record within a hosted zone that contains one or more values.</description>
    </item>
    
    <item>
      <title>05 - AWS Database Services</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/05-database/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/05-database/</guid>
      <description>Amazon RDS (Relational Database Service)  RDS is a Database as a Service (DBaaS) product. It can be used to provision a fully functional database without the admin overhead traditionally associated with DB platforms RDS supports a number of database engines - MySQL, MariaDB, PostgreSQL, Oracle, Microsoft SQL Server, Aurora RDS can be deployed in single AZ or Multi-AZ mode (for resilience) and supports General purpose and Memory Optimized instances RDS instances are managed by AWS, you cannot SSH into the VM instances Primary use case for RDS are Relational, Transactional databases.</description>
    </item>
    
    <item>
      <title>05 - Authentication, Authorization, and Admission Control</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/05_authorization_access_control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/05_authorization_access_control/</guid>
      <description>Authentication Authorization  Types of RoleBindings Admission Control  Demo - Authentication and Authorization  To access and manage any Kubernetes resource or object in the cluster, we need to access a specific API endpoint on the API server. Each access request goes through the following three stages:
 Authentication - Logs in a user Authorization - Authorizes the API requests added by the logged-in user. Admission Control - Software modules that can modify or reject the requests based on some additional checks, like a pre-set Quota.</description>
    </item>
    
    <item>
      <title>05 - Networking</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/05-networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/05-networking/</guid>
      <description>Networking Fundamentals  OSI Model IP Addressing Subnetting Firewall Proxy Servers  Virtual Private Cloud (VPC)  Virtual Private Cloud (VPC): VPC Routing Routes Bastion Hosts (Jump Boxes)  * Multifactor authentication, ID federation and/or IP blocks.  NAT Gateway Network ACLs VPC Peering VPC Endpoints  Global DNS (Route 53)  Hosted Zone Public Zones Private Zones Routing Policy Simple Routing Failover Routing Weighted Routing Policy Latency Based Routing GeoLocation based Routing   Networking Fundamentals OSI Model The Open Syste Interconnection (OSI) Model is a standard used by networking manufacturers globally.</description>
    </item>
    
    <item>
      <title>05 - Observability</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/05_observability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/05_observability/</guid>
      <description>Liveness and Readiness Probes  Liveness probe Readiness Probe  Container Logging Installing Metrics Server Monitoring Applications Debugging  Liveness and Readiness Probes Probes - Allow you to customize how Kubernetes determines the status of your containers
Liveness probe Indicates whether the container is running properly, and governs whether the cluster will automatically stop or restart the container. Liveness probes can be created by including them in the container spec.</description>
    </item>
    
    <item>
      <title>06 - Analytics</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/06-analytics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/06-analytics/</guid>
      <description>Amazon Athena  Amazon Athena is an interactive query service that utilizes Schema-On-Read, allowing you to run ad-hoc SQL like queries on data from a range of sources Athena is used to query large dataset (structured, semi-structured, and unstructured) stored in S3 with infrequent access pattern You are charged for compute time only. You donâ€™t need to maintain separate dataset for Athena, it can directly access S3 bucket  Amazon EMR  EMR is tool for large-scale parallel processing of big data and other large data workloads It is based on the Apache Hadoop framework and is delivered as a managed cluster using EC2 instances It is used for huge-scale log analysis, indexing, machine learning, financial analysis, simulations, bio-informatics and many other large-scale applications EMR cluster have zero or more core nodes, which are managed by the master node.</description>
    </item>
    
    <item>
      <title>06 - Databases</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/07-databases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/07-databases/</guid>
      <description>SQL - RDS  RDS Backups and Restore RDS Resilency RDS Multi-AZ RDS Read Replica  When would you use RDS Read Replica Deployment   SQL - Aurora  Aurora Serverless  Notes   NoSQL - DynamoDB  Query and Scan Operation Performance and Billing Streams and Triggers DynamoDB Indexes  In-Memory Caching  DynamoDB Accelerator (DAX) Use Case Elastic Cache   SQL - RDS RDS is a Database as a Service (DBaaS) product.</description>
    </item>
    
    <item>
      <title>06 - Pod Design</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/06_pod-design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/06_pod-design/</guid>
      <description>Labels, Selectors, and Annotations Deployments Rolling Updates and Rollbacks  Rolling Updates and Rollbacks  Jobs and CronJobs  Labels, Selectors, and Annotations Labels are key-value pairs attached to Kubernetes objects. They are used for identifying various attributes of objects which can in turn be used to select and group various subsets of those objects.
We can attach labels to objects by listing them in the metadata.labels section of an object descriptor.</description>
    </item>
    
    <item>
      <title>06 - Services</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/06_services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/06_services/</guid>
      <description>Services Service Object Example kube-proxy Service Discovery Servie Type  Cluster IP NodePort LoadBalancer ExternalIP ExternalName   Services  An abstract way to expose an application running on a set of Pods as a network service. With Kubernetes you donâ€™t need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.</description>
    </item>
    
    <item>
      <title>07 - Application Integration</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/07-application-integration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/07-application-integration/</guid>
      <description>Amazon Simple Notification Service (SNS)  SNS is a fully managed pub/sub messaging service Topics are logical access point and communication channel within SNS Topics can be encrypted via KMS Subscribers are endpoints that receive message for topic. When a topic received a message it automatically and immediately pushes messages to subscribers SNS supports notification over multiple protocols:  HTTP/HTTPS - Subscribers specify a URL as part of the subscription registration Email/Email Json - Messages are sent to registered address as email SQS - User can specify an SQS standard queue as the endpoint SMS - Messages are sent to registered phone number as SMS text messages   Amazon Simple Queue Service (SQS)  SQS provides fully managed, highly available message queuing service for inter-process /service /service messaging SQS is used for application integration, it lets decouple different systems SQS supports both Standard and FIFO Queues Standard queues are distributed and scalable to nearly unlimited message volume.</description>
    </item>
    
    <item>
      <title>07 - Deploying Standalone App</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/07_deploying_standalone_app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/07_deploying_standalone_app/</guid>
      <description>Creating Deployment using YAML File Exposing Application Liveness and Readiness Probes  Liveness Probe Readiness Probe   Creating Deployment using YAML File webserver.yaml
apiVersion: apps/v1 kind: Deployment metadata: name: webserver labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80  kubectl create -f webserver.yaml  This will also create ReplicaSet and Pods as defined in the YAML configuration.</description>
    </item>
    
    <item>
      <title>07 - Hybrid and Scaling</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/08-hybrid-scaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/08-hybrid-scaling/</guid>
      <description>Load Balancing and Auto Scalling  Elastic Load Balancing (ELB)  Launch Templates and Configurations Auto Scalling Groups  Scaling  VPN and Direct Connect  VPN VPC VPN Components Single Tunnel with Single Customer Gateway Full AWS Resilency with Two Tunnel Endpoint Full High Available VPN Connection Direct Connect (DX) VPN vs Direc Connect or Both VPN Direct Connect Both  Snowball and Snowmobile  Snowball Snowball Edge Snowmobile  Data and DB Migration  Storage Gateway Database Migration Service (DMS)  Identity Federation and SSO  Identity Federation (IDF) SAML 2.</description>
    </item>
    
    <item>
      <title>07 - Storage and Content Delivery</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/06-storage_and_content_delivery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/06-storage_and_content_delivery/</guid>
      <description>S3 Architecture and Features  Permissions Transferring Data to S3 Single PUT Upload (Default) Multi Part Upload Encryption Static Websites and CORS Object Versioning Presigned URL  S3 Performance and Resilience  Storage Tiers / Classes Standard Standard Infrequent Access (Standard - IA) One Zone - IA Glacier Glacier Deep Archive Intellgent Tiering Lifecycle Rules S3 Cross Region Replication  Cloud Front  CloudFront Components Origin Access Identity (OAI)  Network File System  Amazon EFS Throughput modes Performance Mode   S3 Architecture and Features Permissions Bucket authorization within S3 is controlled using Identity Policies on AWS identities, as well as Bucket policies in the form of resource policies on the bucket and bucket or object ACLs.</description>
    </item>
    
    <item>
      <title>08 - Hybrid and Scaling</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/08-hybrid-scaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/08-hybrid-scaling/</guid>
      <description> Snowball and Snowball Edge  Snowball and Snowball Edge is a rugged container which contains a storage device It is used to move large amount of data quickly in and out of AWS. You can both export or import data using Snowball or Snowmobile Snowball and Snowball Edge is peta-scale migration; Snowmobile is for exabyte-scale migration Snowball comes in two sizes:  50 TB (42 TB of usable space) 80 TB (72 TB of usable space)  Snowball Edge comes in two sizes:  100 TB (83 TB of usable space) 100 TB Clustered (45 TB per node)  Snowball Edge includes both Storage and edge-computing workloads Snowball Edge provides three options  Edge Storage Optimized (24 vCPU) Edge Compute Optimized (52 vCPU)) Edge Compute Optimized with GPU  Snowball and Snowball Edge are idea for data transfer from 10 TB to 10 PB of data transfer  Snowmobile  It is 45 foot long ruggedize shipping container, pulled by a semi-trailer truck Snowmobile comes in one size: 100 PB Available in certain areas via special order from AWS Ideal for greater than 10PB Data Transfer for a single location Situated on side and connected into your data center for the duration of the transfer  </description>
    </item>
    
    <item>
      <title>08 - Kubernetes Volume Management</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/08_kubernetes_volume_management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/08_kubernetes_volume_management/</guid>
      <description>Volumes Volume Types PersistentVolumeClaims Using a Shared hostPath Volume Type  Volumes As we know, containers running in Pods are ephemeral in nature. All data stored inside a container is deleted if the container crashes. However, the kubelet will restart it with a clean slate, which means that it will not have any of the old data.
To overcome this problem, Kubernetes uses Volumes. A Volume is essentially a directory backed by a storage medium.</description>
    </item>
    
    <item>
      <title>09 - Application, Analytics, and Operations</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/09-applicationanalyticsoperations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/09-applicationanalyticsoperations/</guid>
      <description>Application Integration  Simple Notification Service (SNS) SNS Components  Topic Subscribers Publisher  Simple Queue Service (SQS) Polling Types Elastic Transcoder  Analytics  Athena Elastic MapReduce (EMR) Kinesis and Firehose Kinesis Stream Kinesis Shard Kinesis Data Record Kinesis Firehose Redshift  Logging and Monitoring  CloudWatch CloudWatch Metrics and Alarms CloudWatch Logs CloudTrail VPC Flow Logs  Operations  CloudWatch Events KMS  Deployment  Elastic Beanstalk Deployment Options OpsWorks OpsWorks Components   Application Integration Simple Notification Service (SNS)  SNS is a Publisher / Subscriber based fully managed Regional Service.</description>
    </item>
    
    <item>
      <title>09 - ConfigMaps and Secrets</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/09_configmaps_secrets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/09_configmaps_secrets/</guid>
      <description>ConfigMaps  Creating ConfigMaps Using CommandLine Using Configuration File Using Properties File Using ConfigMaps Inside Pods As Environment Variable As Volume  Secrets  Creating Secret Create a Secret from Literal and Display Its Details Create a Secret from YAML File Use Secrets Inside Pods   ConfigMaps ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs, which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes.</description>
    </item>
    
    <item>
      <title>10 - Ingress</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/10_ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/10_ingress/</guid>
      <description>With Services, routing rules are associated with a given Service. They exist for as long as the Service exists, and there are many rules because there are many Services in the cluster. If we can somehow decouple the routing rules from the application and centralize the rules management, we can then update our application without worrying about its external access. This can be done using the Ingress resource.
 An Ingress is a collection of rules that allow inbound connections to reach the cluster Services.</description>
    </item>
    
    <item>
      <title>11 - Advanced Topics</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/11_advanced_topics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/11_advanced_topics/</guid>
      <description>Annotations With Annotations, we can attach arbitrary non-identifying metadata to any objects, in a key-value format:
&amp;quot;annotations&amp;quot;: { &amp;quot;key1&amp;quot; : &amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot; : &amp;quot;value2&amp;quot; }  Unlike Labels, annotations are not used to identify and select objects. Annotations can be used to:
 Store build/release IDs, PR numbers, git branch, etc. Phone/pager numbers of people responsible, or directory entries specifying where such information can be found Pointers to logging, monitoring, analytics, audit repositories, debugging tools, etc.</description>
    </item>
    
    <item>
      <title>12 - Resources</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/12_resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/12_resources/</guid>
      <description> Labs CKA Curriculum   </description>
    </item>
    
    <item>
      <title>Arrow Functions</title>
      <link>http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/es-6/arrowfunctions/</guid>
      <description>Arrow Function Express allows you to write shorter syntax than it&amp;rsquo;s predecessor Function expression. In addition and more exciting is how the new Arrow function bind their context.
(param1, param2, param3) =&amp;gt; { statements } singleParam =&amp;gt; { statements } () =&amp;gt; { statements }  Example var materials = [ &amp;#39;Iron&amp;#39;, &amp;#39;Calcium&amp;#39;, &amp;#39;Sodium&amp;#39;, &amp;#39;Magnanese&amp;#39; ] materials.map(material =&amp;gt; material.length)  An arrow function does not newly define its own this when it&amp;rsquo;s being executed.</description>
    </item>
    
    <item>
      <title>Basic Essentials</title>
      <link>http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/google-cloud-platform/basic-essentials/</guid>
      <description>Creting Instance (Directly) # Create Compute Instance gcloud compute instances create gcelab2 --machine-type n1-standard-2 \ --zone us-central1-c  Using Instance Templates / Instance Groups # Create Instance Template gcloud compute instance-templates create nginx-template \ --metadata-from-file startup-script=startup.sh # Create Target Pool gcloud compute target-pools create nginx-pool # Create Instance Group gcloud compute instance-groups managed create nginx-group \ --base-instance-name nginx \ --size 2 \ --template nginx-template \ --target-pool nginx-pool # List Instances gcloud compute instances list  Create Filewall # Create Filewall rule to Allow 80 gcloud compute firewall-rules create www-firewall --allow tcp:80  SSH Instance gcloud compute ssh gcelab2 --zone us-central1-c   gcloud is a command-line tool for Google Cloud Platform</description>
    </item>
    
    <item>
      <title>Berkshelf</title>
      <link>http://learn.aayushtuladhar.com/devops/chef/2016-02-29-berks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/chef/2016-02-29-berks/</guid>
      <description>Berkshelf Reference External Dependent Cookbooks so that it can download those cookbooks rather than manually downloading via knife cookbook command.
knife cookbook site
Berkshelf lets you treat your cookbooks the way you treat gem in a Ruby project. When external cookbooks are used, Berkshelf doesn&amp;rsquo;t requite knife cookbook site to install community cookbooks.
Implementing Berkshelf gem install berkshelf
Install Cookbooks via Berks berks install
Upload berks to Chef Server berks upload &amp;lt;cookbook&amp;gt;</description>
    </item>
    
    <item>
      <title>Chef Databags</title>
      <link>http://learn.aayushtuladhar.com/devops/chef/2018-01-30-chef-databags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/chef/2018-01-30-chef-databags/</guid>
      <description>Databags are global variables that is stored as JSON Data and is accessible from Chef Server. A data bag is indexed for searching and can be loaded by recipe or accessed during a search.
Creating Data Bag (Using Knife) $ knife data bag create DATA_BAG_NAME (DATA_BAG_ITEM) knife data bag create TEST_BAG  Adding File to Data Bag knife data bag from file TEST_BAG test.json  Data Bag Items A data bag is container of related data bag items, where each individual data bag item is a JSON file.</description>
    </item>
    
    <item>
      <title>Circle CI</title>
      <link>http://learn.aayushtuladhar.com/devops/ci-cd/circle-ci/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/ci-cd/circle-ci/</guid>
      <description>Concepts Workflows Jobs Steps Image Example  Concepts Workflows Workflows define a list of jobs and their run order. It is possible to run jobs concurrently, sequentially, on a schedule, or with a manual gate using an approval job.
Jobs Jobs are a collection of Steps. All of the steps in the job are executed in a single unit which consumes a CircleCI container from your plan while itâ€™s running.</description>
    </item>
    
    <item>
      <title>Cloud Storage</title>
      <link>http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/google-cloud-platform/cloud-storage/</guid>
      <description> Create a Stoage Bucket gsutil mb gs://unique-name  </description>
    </item>
    
    <item>
      <title>Continuous Delivery with Jenkins in Kubernetes Engine</title>
      <link>http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/google-cloud-platform/continuous-delivery-with-jenkins-in-kubernetes-engine/</guid>
      <description># Create Kubernetes Cluster gcloud container clusters create jenkins-cd \ --num-nodes 2 \ --machine-type n1-standard-2 \ --scopes &amp;quot;https://www.googleapis.com/auth/projecthosting,cloud-platform&amp;quot; # Update KubeConfig with Cluster credentials gcloud container clusters get-credentials jenkins-cd # Verify Kubernetes can connect to GCP Kubernetes Cluster kubectl cluster-info  </description>
    </item>
    
    <item>
      <title>Creating a Static Website Using Amazon S3</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/creating_static_website_using_s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/creating_static_website_using_s3/</guid>
      <description>Create S3 Bucket  Navigate to the S3 portion of the AWS Management Console. Create a bucket, choosing a globally unique name. Select US East (N. Virginia) region. Click Next. Leave options as defaults; click Next. Under permissions, uncheck all four permissions restrictions. Click Next. Click Create bucket. Select bucket name. Click Upload. Add files (use your own or those from the sample website). Click Upload.  Enable Static Website Hosting  Click the bucket name.</description>
    </item>
    
    <item>
      <title>DNS Basics</title>
      <link>http://learn.aayushtuladhar.com/devops/network/2017-01-29-dns-basics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/network/2017-01-29-dns-basics/</guid>
      <description>DNS (Domain Name System) is essential component of modern internet communication. It allows us to reference computers by human friendly names instead of IP Addresses.
Terminologies  Domain Name IP Address  DNS Lookup using Dig Dig is a flexible tool for interrogating DNS name servers. It performs DNS lookup and is very helpful to troubleshoot DNS problems.
dig &amp;lt;serverName&amp;gt; +nostats +nocomments +nocmd  $ dig google.com +nostats ; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.</description>
    </item>
    
    <item>
      <title>Designing and Building a Custom VPC from Scratch</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/vpc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/vpc/</guid>
      <description>This hands-on lab provides you with some experience building and connecting the following services inside AWS: VPC, subnets, internet gateway, NAT gateways, Bastion host, route tables, security groups, and network access control lists (NACLs). These services are the foundation of networking architecture inside of AWS and cover concepts such as infrastructure, design, routing, and security.
Create a VPC  SelectÂ Your VPCs. ClickÂ Create VPC, and set the following values:  labVPC 10.</description>
    </item>
    
    <item>
      <title>Destructuring JavaScript Objects</title>
      <link>http://learn.aayushtuladhar.com/javascript/es-6/destructuring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/es-6/destructuring/</guid>
      <description>const person = { firstName: &amp;#39;Aayush&amp;#39;, lastName: &amp;#39;Tuladhar&amp;#39;, country: &amp;#39;Nepal&amp;#39;, twitter: &amp;#39;@aayushtuladhar&amp;#39; } /* Problem */ const first = person.firstName; const last = person.lastName; console.log(`Hello ${first}${last}`); /* Solution */ const { firstName, lastName } = person; console.log(`Hello ${firstName}${lastName}`); /* ------------------ */ const art = { first: &amp;#39;ART&amp;#39;, last: &amp;#39;Ratna&amp;#39;, links: { social: { twitter: &amp;#39;https://twitter.com/aayushtuladhar&amp;#39;, facebook: &amp;#39;https://facebook.com/aayush.tuladhar&amp;#39;, }, web: { blog: &amp;#39;https://aayushtuladhar.com&amp;#39; } } }; const { twitter, facebook } = art.</description>
    </item>
    
    <item>
      <title>Docker Compose</title>
      <link>http://learn.aayushtuladhar.com/devops/docker/2019-09-17-docker-compose/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/docker/2019-09-17-docker-compose/</guid>
      <description> Docker Compose is used to run multiple containers as a single service.
# docker-compose.yml version: &#39;2&#39; services: web: build: . # build from Dockerfile context: ./Path dockerfile: Dockerfile ports: - &amp;quot;5000:5000&amp;quot; volumes: - .:/code redis: image: redis  Command Line # Start Service docker-compose start # Stop Service docker-compose stop # Pause Service docker-compose pause # UnPause Service docker-compose unpause # List containers docker-compose ps # Create and start containers docker-compose up # Stop and remove containers, networks, images, and volumes docker-compose down  </description>
    </item>
    
    <item>
      <title>Drone</title>
      <link>http://learn.aayushtuladhar.com/devops/ci-cd/2018-04-19-drone/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/ci-cd/2018-04-19-drone/</guid>
      <description>Creating Pipeline  Images Cloning Commands Services Plugins  Running Drone Locally Drone Secrets  Repo Level Secrets Org Level Secrets Using Secrets in Pipeline   Drone is a CI/CD platform built on Docker and written in Go.
Creating Pipeline Drone pipeline are written in .drone.yml file in the root of the repository.
 Pipelines are event based, which can be triggered via push, pull_request, tag and deployment events</description>
    </item>
    
    <item>
      <title>Getting Started</title>
      <link>http://learn.aayushtuladhar.com/hugo-basics/basic_1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/hugo-basics/basic_1/</guid>
      <description>Learning the Basics Stuff Starting Server hugo server -D  Documentation Learn</description>
    </item>
    
    <item>
      <title>Getting Started with Cloud Formation</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/cloud_formation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/cloud_formation/</guid>
      <description>AWS CloudFormation provides users with a simple way to create and manage a collection of Amazon Web Services (AWS) resources by provisioning and updating them in a predictable way. AWS CloudFormation enables you to manage your complete infrastructure or AWS resources in a text file.</description>
    </item>
    
    <item>
      <title>Getting Started with Redux</title>
      <link>http://learn.aayushtuladhar.com/javascript/reactjs/redux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/reactjs/redux/</guid>
      <description>Redux gives you a store, and lets you keep state in it, and get state out, and respond when the state changes. But thatâ€™s all it does.
Itâ€™s actually react-redux that lets you connect pieces of the state to React components.
 The state is the data, and the store is where itâ€™s kept
 Redux Store Redux Reducer Reducer&amp;rsquo;s job is to take the current state and action and return the new state.</description>
    </item>
    
    <item>
      <title>Gradle Wrapper</title>
      <link>http://learn.aayushtuladhar.com/devops/gradle/2016-07-06-gradle-wrapper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/gradle/2016-07-06-gradle-wrapper/</guid>
      <description>Gradle Wrapper The Gradle wrapper allows you to run a Gradle task without requiring that Gradle is installed on your system.
Creating Gradle Wrapper task wrapper(type: Wrapper) { gradleVersion = &#39;2.10&#39; //we want gradle 2.10 to run this project }  Running Gradle Wrapper gradle wrapper
Following files will be created:
|-gradle |--- wrapper |--- gradle-wrapper.jar |--- gradle-wrapper.properties |-gradlew |-gradlew.bat  Gradle wrapper are useful when you want to run gradle command without installing gradle</description>
    </item>
    
    <item>
      <title>Hive Query Optimizations</title>
      <link>http://learn.aayushtuladhar.com/data/hive/2017-05-18-hive-query-optimizations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/data/hive/2017-05-18-hive-query-optimizations/</guid>
      <description>Changing Engine for Hive Queries In general, Tez provides increased performance over Map Reduce specially where JOINS are required resulting in intermediate output being written to disk.
set hive.execution.engine=tez  Using Partitions Partitions separate date in Hive tables by HDFS directories. If data is distributed based on partitions on certain fields, Using those fields to access data allows speeding up Hive queries. In other words, querying without partitions equates to full table scan.</description>
    </item>
    
    <item>
      <title>Hive Tables [Best Practices]</title>
      <link>http://learn.aayushtuladhar.com/data/hive/2018-01-02-hive-tables-best-practices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/data/hive/2018-01-02-hive-tables-best-practices/</guid>
      <description>Hive Table Storage Formats Avoid using TEXT format, Sequence file format or complex storage format such as JSON. Ideally, RCFile (Row Columnar File) or Parquet files are best suited. If you are building data warehouse on Hive, for better performance use Parquet file format.
CREATE TABLE IF NOT EXISTS test_table ( col1 int, col2 string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS PARQUET;  Compression Techniques Try to split compression algorithms provided by Hadoop &amp;amp; Hive like Snappy.</description>
    </item>
    
    <item>
      <title>Implementing VPC Peering</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/implementing_vpc_peering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/implementing_vpc_peering/</guid>
      <description>Create a VPC Peer Ensure you are logged in to the AWS account, INSTANCE1, and INSTANCE2 using the cloud_user credentials provided.
 Change the NACL for Public2 Subnet - change ICMP from 0.0.0.0/0 to 10.0.0.0/13 Create a VPC peer from VPC1 to VPC2 Accept the VPC peer between VPC1 and VPC2  Configure Routing Ensure the VPC peer is created and active from Task 1:
 Locate the route tables associated with PublicSubnet1 and PrivateSubnet1 In each - Add a route for the CIDR of VPC2 and the target of the VPC Peer created in Task 1 Locate the route tables associated with PublicSubnet2 and PrivateSubnet2 In each - Add a route for the CIDR of VPC1 and the target of the VPC Peer created in Task 1 Obtain the privateIP for Instance2 and ping it from Instance1  Create VPC Peer Mesh Ensure the VPC Peer from Task 1 is created and active:</description>
    </item>
    
    <item>
      <title>Implementing an Auto Scaling Group and Application Load Balancer</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/autoscaling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/autoscaling/</guid>
      <description>In this AWS hands-on lab, we will integrate two powerful AWS services: Elastic Load Balancers and Auto Scaling groups.
Specifically, we will create an Auto Scaling group of EC2 instances operating as web servers, and we&amp;rsquo;ll configure an Application Load Balancer to load balance between the instances inside that Auto Scaling group.
After everything is set up, we will simulate stress tests on the EC2 instances to confirm the Auto Scaling group works as expected.</description>
    </item>
    
    <item>
      <title>Installing Docker CE on Centos 7</title>
      <link>http://learn.aayushtuladhar.com/devops/docker/2017-09-22-install-docker-centos-7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/docker/2017-09-22-install-docker-centos-7/</guid>
      <description>  Install Docker Community Edition Verify  Install Docker Community Edition sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum makecache fast sudo yum install docker-ce sudo systemctl start docker  Verify sudo docker run hello-world  </description>
    </item>
    
    <item>
      <title>JavaScript Revisited</title>
      <link>http://learn.aayushtuladhar.com/javascript/eloquent_js/revisited/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/eloquent_js/revisited/</guid>
      <description>JavaScript  Language of the Web Introduced in 1995 as a way to add programs to web pages ECMAScript Standard  Values, Types and Operators  Inside the computer&amp;rsquo;s world, there is only data. You can read data, modify data, create new data but anything that isn&amp;rsquo;t data simple does not exist.
 DataTypes  Numbers Strings  Single Quotes or Double Quotes Escaping using \n String Concatenation (+)  Boolean Undefined Values  Null Undefined   Operators  Binary -&amp;gt; Takes Two Values Unary -&amp;gt; Takes One Value  Arithmetic Operators  Addition (+) Substraction (-) Multiplication (*) Division (/) Modulus (%)  Comparison Operator  Greater Than (&amp;gt;) Less Than (&amp;lt;) Greater Than or Equal To (&amp;gt;=) Less Than or Equal To (&amp;lt;=) Equal to (==) Not Equal to (!</description>
    </item>
    
    <item>
      <title>Jenkins</title>
      <link>http://learn.aayushtuladhar.com/devops/ci-cd/2019-08-20-jenkins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/ci-cd/2019-08-20-jenkins/</guid>
      <description>Installing Plugins  Pipeline Script   Installing Jenkins docker pull jenkins/jenkins # Persist Jenkins Data within the Docker Container docker volume create jenkins-data docker run --name jenkins-production \ --detach \ -p 50000:50000 \ -p 8080:8080 \ -v jenkins-data:/var/jenkins_home \ jenkins/jenkins:2.164.2  Access Jenkins at http://localhost:8080/
Installing Plugins  Blueocean plugin  Manage Jenkins &amp;gt; Manage Plugins
Pipeline Script Hello World Pipeline Script
pipeline { agent none environment { APPLICATION_NAME = &#39;hello-jenkins-pipeline&#39; } stages { stage(&#39;build&#39;) { steps { echo &amp;quot;Hello World&amp;quot; } } } }  Pipeline script to Build Another Jenkins Job</description>
    </item>
    
    <item>
      <title>Jenkins Shared Library</title>
      <link>http://learn.aayushtuladhar.com/devops/ci-cd/2019-09-13-jenkins-shared-libs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/ci-cd/2019-09-13-jenkins-shared-libs/</guid>
      <description>Jenkins Shared library is the concept of having a common pipeline code in the version control system that can be used by any number of pipeline just by referencing it. In fact, multiple teams can use the same library for their pipelines. Pipeline has support for creating &amp;ldquo;Shared Libraries&amp;rdquo; which can be defined in external source control repositories and loaded into existing Pipelines.
 A shared library is a collection of independent Groovy scripts which you pull into your Jenkinsfile at runtime.</description>
    </item>
    
    <item>
      <title>Jupyter NoteBook - Shortcuts</title>
      <link>http://learn.aayushtuladhar.com/data/2018-05-10-jupyter-notebooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/data/2018-05-10-jupyter-notebooks/</guid>
      <description> Jupyter Shortcuts Command Mode gives to the ability to create, copy, paste, move, and execute cells. A few keys to know: To enter Command Mode (control + m)
   Keywork Description     h Bring up help (ESC to dismiss)   b Create cell below   a Create cell above   c Copy cell   v Paste cell below   Enter Go into Edit Mode   m Change cell type to Markdown   y Change cell type to code   ii Interrupt kernel   oo Restart kernel    </description>
    </item>
    
    <item>
      <title>Knife Commands</title>
      <link>http://learn.aayushtuladhar.com/devops/chef/2016-04-05-chef-knife-commands/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/chef/2016-04-05-chef-knife-commands/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/introduction-to-kubernetes-lfs158/00_intro/</guid>
      <description>Kubernetes is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure. It is an open source system which helps in creating and managing containerization of application. This tutorial provides an overview of different kind of features and functionalities of Kubernetes and teaches how to manage the containerized infrastructure and application deployment.
Features Automatic bin packing</description>
    </item>
    
    <item>
      <title>Lab 6 - Rolling Updates with Kubernetes Deployments</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab6-rolling-updates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab6-rolling-updates/</guid>
      <description> candy-deployment Deployment descriptor apiVersion: extensions/v1beta1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: &amp;quot;1&amp;quot; kubectl.kubernetes.io/last-applied-configuration: | {&amp;quot;apiVersion&amp;quot;:&amp;quot;apps/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Deployment&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;candy-deployment&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:2,&amp;quot;selector&amp;quot;:{&amp;quot;matchLabels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;candy-ws&amp;quot;}},&amp;quot;template&amp;quot;:{&amp;quot;metadata&amp;quot;:{&amp;quot;labels&amp;quot;:{&amp;quot;app&amp;quot;:&amp;quot;candy-ws&amp;quot;}},&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;image&amp;quot;:&amp;quot;linuxacademycontent/candy-service:2&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;candy-ws&amp;quot;}]}}}} creationTimestamp: null generation: 1 name: candy-deployment selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/candy-deployment spec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: app: candy-ws strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: creationTimestamp: null labels: app: candy-ws spec: containers: - image: linuxacademycontent/candy-service:2 imagePullPolicy: IfNotPresent name: candy-ws resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30  Updating to New Image # Setting New Image for Deployment kubectl set image deployment/candy-deployment candy-ws=linuxacademycontent/candy-service:3 --record # Performing Rollout kubectl rollout status deployment/candy-deployment  Performing RollBack kubectl rollout history deployment/candy-deployment kubectl rollout undo deployment/candy-deployment  </description>
    </item>
    
    <item>
      <title>Lab 7 - Configuring Cron Jobs in Kubernetes</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab7-cronjobs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab7-cronjobs/</guid>
      <description>cleanup-cronjob.yml
apiVersion: batch/v1beta1 kind: CronJob metadata: name: cleanup-cronjob spec: schedule: &amp;quot;*/1 * * * *&amp;quot; jobTemplate: spec: template: spec: containers: - name: hello image: linuxacademycontent/data-cleanup:1 restartPolicy: OnFailure  </description>
    </item>
    
    <item>
      <title>Lab1 - Creating Kubernetes Pod</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab1-creating-kubernetes-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab1-creating-kubernetes-pods/</guid>
      <description>nginx.yml
apiVersion: v1 kind: Pod metadata: name: nginx namespace: web spec: containers: - name: nginx image: nginx command: [&#39;nginx&#39;] args: [&#39;-g&#39;, &#39;daemon off;&#39;, &#39;-q&#39;] ports: - containerPort: 80  ## Applying Spec Definition for Creating Pod kubectl -n web apply -f nginx.yml ## Verifying Pod running in web namespace kubectl get pods -n web  </description>
    </item>
    
    <item>
      <title>Lab2 - Configuring Kubernetes Pod</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab2-configuring-kubernetes-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab2-configuring-kubernetes-pods/</guid>
      <description> Create a ConfigMap called candy-service-config to store the container&amp;rsquo;s configuration data. candy-service-config.yml
apiVersion: v1 kind: ConfigMap metadata: name: candy-service-config data: candy.cfg: |- candy.peppermint.power: 100000000 candy.nougat-armor.strength: 10  Create a Kubernetes secret called db-password to store the database password. candy-service-secret.yml
apiVersion: v1 kind: Secret metadata: name: db-password stringData: db-password: Kub3rn3t3sRul3s!  Create the pod for the candy-service application according to the provided specification. candy-service-pod.yml
apiVersion: v1 kind: Pod metadata: name: candy-service spec: securityContext: fsGroup: 2000 containers: - name: candy-service image: linuxacademycontent/candy-service:1 volumeMounts: - name: candy-service-volume mountPath: /etc/candy-service env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-password key: db-password resources: requests: memory: &amp;quot;64Mi&amp;quot; cpu: &amp;quot;250m&amp;quot; limits: memory: &amp;quot;128Mi&amp;quot; cpu: &amp;quot;500m&amp;quot; volumes: - name: candy-service-volume configMap: name: candy-service-config  </description>
    </item>
    
    <item>
      <title>Lab3 - Forwarding Port Traffic with an Ambassador Container</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab3-forwarding-port-traffic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab3-forwarding-port-traffic/</guid>
      <description>fruit-service-ambassador-config.yml
# Config Map for HA Proxy apiVersion: v1 kind: ConfigMap metadata: name: fruit-service-ambassador-config data: haproxy.cfg: |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8775 maxconn 32  fruit-service-pod.yml
# Pod Definition for fruit-service apiVersion: v1 kind: Pod metadata: name: fruit-service spec: containers: - name: fruit-service-container image: linuxacademycontent/legacy-fruit-service:1 - name: fruit-service-ambassador image: haproxy:1.7 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /usr/local/etc/haproxy volumes: - name: config-volume configMap: name: fruit-service-ambassador-config  busybox.</description>
    </item>
    
    <item>
      <title>Lab4 - Configuring Probes for a Kubernetes Pod</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab4-configuring-probes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab4-configuring-probes/</guid>
      <description>candy-service-pod.yml
apiVersion: v1 kind: Pod metadata: name: candy-service spec: containers: - name: candy-service image: linuxacademycontent/candy-service:2 livenessProbe: httpGet: path: /healthz port: 8081 readinessProbe: httpGet: path: / port: 80  </description>
    </item>
    
    <item>
      <title>Lab5 - Debugging Kubernetes</title>
      <link>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab5-debugging-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/kubernetes/ckad/labs/lab5-debugging-kubernetes/</guid>
      <description> Identify the Problem kubectl get pods --all-namespaces  Get the Broken Pods&amp;rsquo;s Container Logs kubectl logs cart-ws -n candy-store  Fix the Problem # Save the Pod Descriptor of Broken Pod kubectl get pod &amp;lt;pod name&amp;gt; -n &amp;lt;namespace&amp;gt; -o yaml --export &amp;gt; broken-pod.yml # Delete Broken Pod kubectl delete pod &amp;lt;pod name&amp;gt; -n &amp;lt;namespace&amp;gt; # Recreate Broken Pod kubectl apply -f broken-pod.yml -n &amp;lt;namespace&amp;gt; # Verify Fix kubectl get pod &amp;lt;pod name&amp;gt; -n &amp;lt;namespace&amp;gt;  </description>
    </item>
    
    <item>
      <title>Lambda Expressions</title>
      <link>http://learn.aayushtuladhar.com/java/the-basics/lambda_expressions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/java/the-basics/lambda_expressions/</guid>
      <description>Lambda expressions are not unknown to many of us who have worked on other popular programming languages like Scala. In Java programming language, a Lambda expression (or function) is just an anonymous function, i.e., a function with no name and without being bounded to an identifier. They are written exactly in the place where itâ€™s needed, typically as a parameter to some other function.
The most important features of Lambda Expressions is that they execute in the context of their appearance.</description>
    </item>
    
    <item>
      <title>Layouts</title>
      <link>http://learn.aayushtuladhar.com/hugo-basics/basic_2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/hugo-basics/basic_2/</guid>
      <description>Headings # h1 Heading ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading  Renders to:
h1 Heading h2 Heading h3 Heading h4 Heading h5 Heading h6 Heading Typography I am just being Bold
I Love my Italics Style
Strike Through
 I love to give a quotation
 Images ![Minion](https://octodex.github.com/images/minion.png)  Resizing Images ![Minion](https://octodex.github.com/images/minion.png?width=20pc)  Buttons Get Grav  Note / Info/ Tip / Warning A notice disclaimer</description>
    </item>
    
    <item>
      <title>Linux Command Line Hacks</title>
      <link>http://learn.aayushtuladhar.com/devops/linux/2016-11-27-issue-remote-commands/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/linux/2016-11-27-issue-remote-commands/</guid>
      <description> </description>
    </item>
    
    <item>
      <title>Mqtt</title>
      <link>http://learn.aayushtuladhar.com/devops/2019-02-26-mqtt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2019-02-26-mqtt/</guid>
      <description>Introduction  MQTT is a featherweight, ISO complaint PUB-SUB messaging protocol. Designed for low powered devices PRAM consistent: Guaranteed in-order delivery per-publisher Multiple Transport: TCP, TLS, Websockets Flexible: Arbitrary message up to 256 MB Topics can also be used for Key-Value storage  Topic based Pub/Sub  Decouples Publisher and Subscribers  Quality of Service  QoS 0 - &amp;ldquo;Fire and Forget&amp;rdquo; Q0S 1 - &amp;ldquo;At least once&amp;rdquo; QoS 2 - &amp;ldquo;Exactly once; 2 phase commit&amp;rdquo;  Ideal for intermittent connectivity; Sessions may last weeks or months Supports Disconnect &amp;amp; Last Will &amp;amp; Testament message</description>
    </item>
    
    <item>
      <title>Network Tools / Command Essentials</title>
      <link>http://learn.aayushtuladhar.com/devops/network/2016-10-01-network-command-essentials/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/network/2016-10-01-network-command-essentials/</guid>
      <description> Must Know Network Tools / Commands NetStat netstat stands for network statistics. This command displays incoming and outgoing network connections as well as other network information. The netstat utility can show you the open connections on your computer, which programs are making which connections, how much data is being transmitted, and other information.
## List all connections netstat -a ## List TCP or UDP Connections netstat -at //TCP Connections netstat -au //UDP Connections ## List all Ports being Listened to netstat -an | grep &amp;quot;LISTEN &amp;quot;  IpTables ## Open 9001 Port sudo iptables -A INPUT -p tcp --dport 9001 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 9001 -m conntrack --ctstate ESTABLISHED -j ACCEPT sudo iptables -A INPUT -p tcp --dport 3306 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT sudo iptables -A OUTPUT -p tcp --sport 3306 -m conntrack --ctstate ESTABLISHED -j ACCEPT  </description>
    </item>
    
    <item>
      <title>OWASP Dependency Check</title>
      <link>http://learn.aayushtuladhar.com/devops/2019-10-8-owasp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2019-10-8-owasp/</guid>
      <description>OWASP dependency-check is an open source solution that can be used to scan Java and .NET applications to identify the use of known vulnerable components.
Link
Adding OWASP Check to Gradle Projects Adding following fragments to build.gradle
buildscript { repositories { mavenCentral() } dependencies { classpath &#39;org.owasp:dependency-check-gradle:5.2.2&#39; } } plugins { id &#39;org.owasp.dependencycheck&#39; version &#39;5.2.2&#39; }  Gradle Task
./gradlew dependencyCheckAggregate  Configuring DependencyCheck dependencyCheck { format=&#39;ALL&#39; cveValidForHours=1 outputDirectory = file(&amp;quot;$project.</description>
    </item>
    
    <item>
      <title>OpenStack</title>
      <link>http://learn.aayushtuladhar.com/devops/2016-03-24-open-stack/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2016-03-24-open-stack/</guid>
      <description>Getting Started with OpenStack What is OpenStack OpenStack is elastic cloud software that provides software developers with the ability to control the virtual infrastructure on which to deploy their applications. It is a set of software tools for building and managing cloud computing platforms for public and private clouds.
It accelerates time-to-market by dramatically reducing application provisioning times, giving companies full control of their software development lifecycle and ultimately giving them a significant competitive advantage.</description>
    </item>
    
    <item>
      <title>Openshift</title>
      <link>http://learn.aayushtuladhar.com/devops/2019-08-16-openshift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2019-08-16-openshift/</guid>
      <description>Architecture Containers and Image  Container Registries  Pods and Services  Pods Services Labels  Builds and Image Streams  Builds Image Stream Image stream tag Image stream image Image stream trigger Templates  References Cheatsheet  Architecture OpenShift is a layered system designed to expose underlying Docker-formatted container image and Kubernetes concepts as acurately as possible, with a focus on easy composition of applications by a developer.</description>
    </item>
    
    <item>
      <title>Orchestrating Cloud with Kubernetes</title>
      <link>http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/google-cloud-platform/orchestrating-cloud-with-kubernetes/</guid>
      <description># Creating Kubernetes Cluster gcloud container clusters create io  Quick Demo # Create Deployment kubectl create deployment nginx --image=nginx:1.10.0 # List Pods kubectl get pods # Expose Deployment via a Service using LoadBalancer kubectl expose deployment nginx --port 80 --type LoadBalancer # List Service kubectl get services  Pods Pods are the smallest deployable units of computing that can be created and managed in Kubernetes. Pods represent and hold a collection of one or more containers.</description>
    </item>
    
    <item>
      <title>REST API with</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/rest_api_with_api_gateway/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/rest_api_with_api_gateway/</guid>
      <description>IAM Setup Create Policy
Create Policy with name lambda_execute with following JSON, which will allow to perform lambda:InvokeFunction on all resources
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Action&amp;quot;: &amp;quot;lambda:InvokeFunction&amp;quot;, &amp;quot;Resource&amp;quot;: &amp;quot;*&amp;quot; } ] }  Create an IAM role lambda_invoke_function_assume_apigw_role
Edit Trust Policy
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Sid&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Principal&amp;quot;: { &amp;quot;Service&amp;quot;: [ &amp;quot;lambda.amazonaws.com&amp;quot;, &amp;quot;apigateway.amazonaws.com&amp;quot; ] }, &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot; } ] }  Creating Lambda Function</description>
    </item>
    
    <item>
      <title>Resources</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/study-guide/resources/</guid>
      <description> AWS Well-Architected Amazon Elastic Compute Cloud Documentation Introduction to AWS Lambda &amp;amp; Serverless Applications  </description>
    </item>
    
    <item>
      <title>Setting Apache Virtual Host</title>
      <link>http://learn.aayushtuladhar.com/devops/2016-11-26-setting-apache-virtual-host/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2016-11-26-setting-apache-virtual-host/</guid>
      <description>Install Apache WebServer (Pre-Req) sudo apt-get update sudo apt-get install apache2  Apache Virtual Host To run more than one site on a single machine, you need to setup virtual hosts for sites your plan to host on an apache server.
Name Based Virtual Hosts (Most Common)  The server relies on the client to report the hostname as part of the HTTP headers. Using this technique, many different hosts can share the same IP address.</description>
    </item>
    
    <item>
      <title>Setting FQDN</title>
      <link>http://learn.aayushtuladhar.com/devops/network/2016-03-01-setting-fqdn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/network/2016-03-01-setting-fqdn/</guid>
      <description>FQDN  Finding FQDN  Hostname  Finding Hostname  Setting hostname and FQDN  FQDN FQDN stands for Fully Qualified Domain Name. It is a domain name that specifies its exact location in the tree hierarhcy of the Domain Name System (DNS). It specifies all domain levels, including the top-level domain and the root zone.
Example, somehost.example.com
Finding FQDN hostname -f  Hostname A hostname is a label that is assigned to a device connected to a computer network and that is used to identify the device.</description>
    </item>
    
    <item>
      <title>Setting Selenium Grid</title>
      <link>http://learn.aayushtuladhar.com/devops/2017-02-05-setting-selenium-grid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2017-02-05-setting-selenium-grid/</guid>
      <description>Selenium Grid enables you to spread your tests across multiple machines and multiple browsers, which allows your to run tests in parallel. Also having Hub as central point of communication handles all the driver configuration and runs them automatically.
Downloading Selenium http://docs.seleniumhq.org/download/
Setting up Hub Once you have the jarfile downloaded from the Selenium Website,
java -jar selenium-server-standalone-2.x.x.jar â€“role hub
This starts up a jetty server on default port 4444.</description>
    </item>
    
    <item>
      <title>Setting up Apache Storm</title>
      <link>http://learn.aayushtuladhar.com/data/2018-01-02-setting-up-storm-in-5-minutes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/data/2018-01-02-setting-up-storm-in-5-minutes/</guid>
      <description>Apache Storm is free and open source distributed real-time computation system. Storm provides reliable real-time data processing what Hadoop did for batch processing. It provides real-time, robust, user friendly, reliable data processing capability with operational Intelligence.
This post is more about setting up your Storm Environment from ground up in less than 5 Minutes. Yes, you heard it right less than 5 Minutes. Without any delay, let&amp;rsquo;s get it running.</description>
    </item>
    
    <item>
      <title>Setting up Jekyll Website</title>
      <link>http://learn.aayushtuladhar.com/devops/2019-09-17-setting-up-jekyll-website/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2019-09-17-setting-up-jekyll-website/</guid>
      <description> Pre-Requires  Ruby  # Verify Ruby is Installed ruby --version  Installation gem install jekyll bundler  Create New Site jekyll new myblog  Run Blog Locally bundle exec jekyll serve  Reference  https://github.com/arttuladhar/my-jekyll-blog  </description>
    </item>
    
    <item>
      <title>Setting up Jenkins using Terraform</title>
      <link>http://learn.aayushtuladhar.com/terraform/jenkins_setup_terraform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/jenkins_setup_terraform/</guid>
      <description>Building a Custom Jenkins Image Create Dockerfile with contents:
FROM jenkins/jenkins:lts USER root RUN apt-get update -y &amp;amp;&amp;amp; apt-get -y install apt-transport-https ca-certificates curl gnupg-agent software-properties-common RUN curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo &amp;quot;$ID&amp;quot;)/gpg &amp;gt; /tmp/dkey; apt-key add /tmp/dkey RUN add-apt-repository &amp;quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo &amp;quot;$ID&amp;quot;) $(lsb_release -cs) stable&amp;quot; RUN apt-get update -y RUN apt-get install -y docker-ce docker-ce-cli containerd.io RUN curl -O https://releases.hashicorp.com/terraform/0.11.13/terraform_0.11.13_linux_amd64.zip &amp;amp;&amp;amp; unzip terraform_0.11.13_linux_amd64.zip -d /usr/local/bin/ USER ${user}  Build the Image</description>
    </item>
    
    <item>
      <title>Setting up Kubernetes and Terraform</title>
      <link>http://learn.aayushtuladhar.com/terraform/terraform_kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/terraform_kubernetes/</guid>
      <description>Create kube-config.yml
apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration networking: podSubnet: 10.244.0.0/16 apiServer: extraArgs: service-node-port-range: 8000-31274  Initialize Kubernetes
sudo kubeadm init --config kube-config.yml  Copy admin.conf to your home directory
mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  Install Flannel:
sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml  Untaint Kubernetes Master
kubectl taint nodes --all node-role.kubernetes.io/master-  </description>
    </item>
    
    <item>
      <title>Setting up Terraform With Docker</title>
      <link>http://learn.aayushtuladhar.com/terraform/terraform_docker/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/terraform_docker/</guid>
      <description>Installing Docker on the Swarm Manager and Worker # Update the Operating System sudo yum update -y # Uninstall Old Versions sudo yum remove -y docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine # Install Docker CE sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 # Add Docker Repository sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo sudo yum -y install docker-ce # Start Docker and Enable sudo systemctl start docker &amp;amp;&amp;amp; sudo systemctl enable docker # Add `cloud_user` to the `docker` group sudo usermod -aG docker cloud_user docker --version # Configure Swarm Manager Node docker swarm init --advertise-addr [PRIVATE_IP] On the worker node, add the worker to the cluster: docker swarm join --token [TOKEN] [PRIVATE_IP]:2377 # Verify Swarm cluster docker node ls  Installing Terraform # Install Terraform 0.</description>
    </item>
    
    <item>
      <title>Spread Syntax</title>
      <link>http://learn.aayushtuladhar.com/javascript/es-6/spread/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/es-6/spread/</guid>
      <description>Spread Syntax represented by ... expands an iterable to its individual elements.
Common use cases:  Spread in array literal  const head = [&#39;eyes&#39;,&#39;nose&#39;,&#39;mouth&#39;] const body = [&#39;stomach&#39;,&#39;hip&#39;,&#39;legs&#39;] const headBody = [...head, ...body] console.log(headBody) // [&amp;quot;eyes&amp;quot;, &amp;quot;nose&amp;quot;, &amp;quot;mouth&amp;quot;, &amp;quot;stomach&amp;quot;, &amp;quot;hip&amp;quot;, &amp;quot;legs&amp;quot;]   Spread in Object Literals  const obj1 = { foo: &#39;bar&#39;, x: 42 }; const obj2 = { foo: &#39;baz&#39;, y: 13 }; const clonedObj = { .</description>
    </item>
    
    <item>
      <title>Spring Cloud Slueth</title>
      <link>http://learn.aayushtuladhar.com/java/spring/2018-09-18-spring-cloud-slueth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/java/spring/2018-09-18-spring-cloud-slueth/</guid>
      <description>A powerful tool for enhancing logs in any application, but especially in a system built up of multiple services. This is where spring-cloud-starter-sleuth comes into play to help you enhance your logging and traceability across multiple systems. Just including the spring-cloud-starter-sluth in your project.
Few concepts you need to be familiar with when using Spring Cloud Slueth are concepts of Trace and Spans. Trace can though as single request or job that is triggered in an application.</description>
    </item>
    
    <item>
      <title>SupervisorD</title>
      <link>http://learn.aayushtuladhar.com/devops/2016-04-10-setting-up-supervisord/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2016-04-10-setting-up-supervisord/</guid>
      <description>Installation Configuration Running Supervisor  Supervisor Configuration Structure  Controller Processes  Reread Configuration and Reload It Controlling Tool Start / Stop Processess   It&amp;rsquo;s been a while I have been using Supervisor to run my application. It&amp;rsquo;s a great little tool for running and monitoring processes on UNIX-like operating systems. It provides you simple simple, centralized interface to all your applications running on a box. Using Web Interface, you can see health and logs of the applications without even logging in in the box.</description>
    </item>
    
    <item>
      <title>Teplate Literals</title>
      <link>http://learn.aayushtuladhar.com/javascript/template-literals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/javascript/template-literals/</guid>
      <description>Template literals are string literals allowing embedded expressions. You can use multi-line strings and string interpolation features with them.
const someText = `string text ${expression} string text`  </description>
    </item>
    
    <item>
      <title>Terraform State</title>
      <link>http://learn.aayushtuladhar.com/terraform/terraform_state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/terraform/terraform_state/</guid>
      <description>Create a S3 Bucket in AWS that we will be using to store the Remote State.
Set the Environment Variables:
export AWS_ACCESS_KEY_ID=&amp;quot;[ACCESS_KEY]&amp;quot; export AWS_SECRET_ACCESS_KEY=&amp;quot;[SECRET_KEY]]&amp;quot; export AWS_DEFAULT_REGION=&amp;quot;us-east-1&amp;quot;  Add the Remote Backend Configuration
terraform { backend &amp;quot;s3&amp;quot; { key = &amp;quot;terraform-aws/terraform.tfstate&amp;quot; } }  Initialize Terraform
terraform init -backend-config &amp;quot;bucket=[BUCKET_NAME]&amp;quot;  </description>
    </item>
    
    <item>
      <title>The Twelve Factor App</title>
      <link>http://learn.aayushtuladhar.com/architecture/twelve-factor-app/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/architecture/twelve-factor-app/</guid>
      <description>12factor.net - Link Introduction Methodology for building software-as-a-service app that:
 Use Declarative formats for setup automation, to minimize time and cost for new developers joining the project. Have a Clean Contract with the underlying operation system, offering Maximum Portability between execution environments Are suitable for Deployment on modern Cloud platforms, obviating the need for servers and system administrators Minimize divergence between deployment and production, enabling Continuous deployment for maximum agility And can Scale up without significant changes to tooling, architecture, or development practices.</description>
    </item>
    
    <item>
      <title>Ubuntu Packages</title>
      <link>http://learn.aayushtuladhar.com/devops/linux/2016-11-28-ubuntu-packages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/linux/2016-11-28-ubuntu-packages/</guid>
      <description> List Packages Installed dpkg -l
Create Backup of What Packages Installed dpkg --get-selection &amp;gt; list.txt
Restore dpkg --clear-selections sudo dpkg --set-selections &amp;lt; list.txt sudo apt-get autoremove sudo apt-get dselect-upgrade  </description>
    </item>
    
    <item>
      <title>Understanding SSH Keys</title>
      <link>http://learn.aayushtuladhar.com/devops/security/2016-07-07-ssh-keys/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/security/2016-07-07-ssh-keys/</guid>
      <description>SSH is the most common way of connecting to remove Linux Server. It stands for Secure Shell. It provide a safe and secure way of executing commands, making changes and configuring service remotely. SSH connecting is implemented using client-server model. For a client machine to connect to a remote machine using SSH, SSH daemon must be running on the remote machine.
Clients generally authenticate either using passwords or by SSH Keys.</description>
    </item>
    
    <item>
      <title>Using Apache Server Benchmarking</title>
      <link>http://learn.aayushtuladhar.com/devops/2017-03-15-using-apache-server-benchmarking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/devops/2017-03-15-using-apache-server-benchmarking/</guid>
      <description>Apache Benchmark is a single-threaded command line tool for measuring the performance of a HTTP web server. It gives you an impression of how many requests per second your server is capable of serving.
Installation sudo apt-get install apache2-utils.  Usage -p POST Message -H Message Header -T Content Type -c Concurrent Clients -n Number of Requests to Run in the Test  GET REQUEST $ ab -n200 -c100 -H &amp;quot;APP-TOKEN: Q977quNeXjFsNjLNlmC9MK1HuRP+fFKmwDX9KSD6Y=&amp;quot; \ http://test-api.</description>
    </item>
    
    <item>
      <title>Using EC2 Roles and Instance Profiles</title>
      <link>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/ec2_roles_instance_profiles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/amazon-web-services/aws-solutions-architect/labs/ec2_roles_instance_profiles/</guid>
      <description>AWS Identity and Access Management (IAM) roles for Amazon Elastic Compute Cloud (EC2) provide the ability to grant instances temporary credentials. These temporary credentials can then be used by hosted applications to access permissions configured within the role. IAM roles eliminate the need for managing credentials, help mitigate long-term security risks, and simplify permissions management.</description>
    </item>
    
    <item>
      <title>Using Pandas</title>
      <link>http://learn.aayushtuladhar.com/data/data-analytics/2018-05-01-pandas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://learn.aayushtuladhar.com/data/data-analytics/2018-05-01-pandas/</guid>
      <description>Explore, Visualize, and Predict using Pandas &amp;amp; Jupyter  Explore, Visualize, and Predict using Pandas &amp;amp; Jupyter Pandas Intro Load Data Inspecting Data Summarize Data Reference  Setup
%matplotlib inline import pandas as pd import matplotlib import numpy as np  pd.__version__, matplotlib.__version__, np.__version__  Pandas Intro The pandas library is very popular among data scientists, quants, Excel junkies, and Python developers because it allows you to perform data ingestion, exporting, transformation, and visualization with ease.</description>
    </item>
    
  </channel>
</rss>